# 📊 Ridge 回归正则化强度 (α) 调优实验报告

---

# 0. 元信息（Meta Information）
- **实验名称：** Ridge Regression Alpha Sweep Experiment  
- **作者：** TODO  
- **日期：** 2025-11-27  
- **数据版本：** HDF5 光谱数据（4096 像素合成光谱）  
- **模型版本：** Ridge Regression with α ∈ [0.001, 1000]

---

# 1. 目标

## 1.1 大目标

> 系统性研究 Ridge 回归正则化强度 α 对恒星物理参数（log_g）预测性能的影响，以期：

- 理解高维光谱数据（4096 维）中正则化的必要性
- 建立不同噪声水平下的最优 α 选择策略
- 为后续神经网络设计提供线性基线和正则化参考

**最终目标：** 确定 Ridge 回归在不同噪声条件下的最优超参数，为评估非线性模型（NN、LightGBM）提供可靠的线性基准。

## 1.2 实验目标

本次实验属于以下中层方向：

- **探索正则化强度 α 与模型性能的关系曲线**
- 对比 Ridge 回归与 OLS（无正则化）在不同噪声水平下的表现
- 验证偏差-方差权衡在高维光谱回归中的表现

## 1.3 子目标

本次实验要验证的具体问题：

1. **最优 α 如何随噪声水平变化？**
2. **Ridge 回归相比 OLS 能提供多大改进？**
3. **不同评估指标（R², MAE, RMSE）是否指向相同的最优 α？**
4. **高噪声下线性模型的性能上限在哪里？**

---

# 2. 实验设计（Experiment Design）

## 2.1 数据（Data）

| 配置项 | 值 |
|--------|-----|
| 样本数 | ~52 unique (α, noise_level) 组合 |
| 特征维度 | 4,096 |
| 标签参数 | log_g |
| 噪声水平 | 0.0, 0.1, 0.2, 0.5, 1.0, 2.0 |

**噪声模型：**
$$
\text{noisy\_flux} = \text{flux} + \mathcal{N}(0, 1) \times \text{error} \times \text{noise\_level}
$$

其中：
- **flux**: 光谱流量值（每个波长点的辐射强度）
- **error**: 每个波长点的已知高斯噪声标准差 σ
- **noise_level**: 噪声缩放因子 (0.0 - 2.0)

## 2.2 模型与算法（Model & Algorithm）

### Ridge 回归（L2 正则化线性回归）
$$
\hat{y} = X w + b
$$
$$
w = (X^\top X + \alpha I)^{-1} X^\top y
$$

### OLS（Ordinary Least Squares，无正则化）
$$
w = (X^\top X)^{-1} X^\top y
$$

## 2.3 超参数搜索空间

| 参数 | 搜索范围 |
|------|----------|
| Ridge α | 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0 |
| noise_level | 0.0, 0.1, 0.2, 0.5, 1.0, 2.0 |
| 特征维度 | 4096 |

## 2.4 数据来源

| 来源 | 文件路径 | 噪声范围 | Alpha 范围 |
|------|----------|----------|------------|
| Alpha Sweep | `results/linear_alpha_search/linear_alpha_sweep.csv` | 0.0 - 2.0 | 0.001 - 1000 |
| Linear Experiments | `results/all_linear_experiments.csv` | 0.0, 0.1, 1.0 | 1 - 100 |
| Sonnet Collection | `collections/sonnet/results/LNREG_MASTER_RESULTS_TABLE.csv` | 0.0 - 2.0 | 100 |

---

# 3. 实验结果表（Results）

## 3.1 Ridge 回归完整结果（按噪声水平分组）

### Noise = 0.0（无噪声）

| Alpha | Test R² | Test MAE | Test RMSE |
|-------|---------|----------|-----------|
| 0.001 | **0.9990** | **0.0060** | **0.0092** |
| 0.01 | 0.9978 | 0.0093 | 0.0138 |
| 0.1 | 0.9934 | 0.0162 | 0.0237 |
| 1.0 | 0.9784 | 0.0294 | 0.0429 |
| 10.0 | 0.9301 | 0.0546 | 0.0772 |
| 100.0 | 0.7943 | 0.1014 | 0.1324 |
| 1000.0 | 0.4720 | 0.1798 | 0.2122 |
| **OLS** | 0.9694 | 0.0380 | 0.0511 |

### Noise = 0.1

| Alpha | Test R² | Test MAE | Test RMSE |
|-------|---------|----------|-----------|
| 0.001 | 0.9012 | 0.0674 | 0.0918 |
| 0.1 | 0.9029 | 0.0669 | 0.0910 |
| 1.0 | **0.9090** | **0.0650** | **0.0881** |
| 10.0 | 0.8907 | 0.0724 | 0.0965 |
| 100.0 | 0.7802 | 0.1051 | 0.1369 |
| **OLS** | 0.9007 | 0.0673 | 0.0920 |

### Noise = 0.5

| Alpha | Test R² | Test MAE | Test RMSE |
|-------|---------|----------|-----------|
| 0.001 | 0.6075 | 0.1397 | 0.1829 |
| 10.0 | 0.6325 | 0.1359 | 0.1770 |
| 100.0 | **0.6493** | **0.1357** | **0.1729** |
| 1000.0 | 0.4365 | 0.1860 | 0.2192 |
| **OLS** | 0.6075 | 0.1397 | 0.1829 |

### Noise = 1.0

| Alpha | Test R² | Test MAE | Test RMSE |
|-------|---------|----------|-----------|
| 0.001 | 0.3851 | 0.1776 | 0.2290 |
| 50.0 | 0.4325 | 0.1734 | 0.2199 |
| 100.0 | **0.4507** | **0.1730** | **0.2164** |
| 1000.0 | 0.3684 | 0.1963 | 0.2320 |
| **OLS** | 0.3851 | 0.1776 | 0.2290 |

### Noise = 2.0

| Alpha | Test R² | Test MAE | Test RMSE |
|-------|---------|----------|-----------|
| 0.001 | 0.1312 | 0.2214 | 0.2722 |
| 100.0 | 0.1709 | 0.2184 | 0.2659 |
| 1000.0 | **0.2210** | **0.2177** | **0.2577** |
| **OLS** | 0.1312 | 0.2214 | 0.2722 |

## 3.2 最优 α 与 OLS 对比总结

| Noise | Best α | Ridge R² | OLS R² | Δ R² | 相对改进 |
|-------|--------|----------|--------|------|----------|
| 0.0 | 0.001 | **0.9990** | 0.9694 | +0.030 | +3.1% |
| 0.1 | 1.0 | **0.9090** | 0.9007 | +0.008 | +0.9% |
| 0.2 | 10.0 | **0.8264** | 0.8108 | +0.016 | +1.9% |
| 0.5 | 100.0 | **0.6493** | 0.6075 | +0.042 | +6.9% |
| 1.0 | 100.0 | **0.4507** | 0.3851 | +0.066 | +17.0% |
| 2.0 | 1000.0 | **0.2210** | 0.1312 | +0.090 | +68.4% |

---

# 4. 关键洞见（Key Insights）

## 4.1 宏观层洞见（用于指导 Neural Network 架构设计）

### 核心发现：最优 α 随噪声系统性增大

$$
\text{noise} \uparrow \quad \Longrightarrow \quad \alpha_{\text{opt}} \uparrow
$$

| 噪声水平 | 最优 α | 规律 |
|----------|--------|------|
| 0.0 | 0.001 | 几乎无需正则化 |
| 0.1 | 1.0 | 轻度正则化 |
| 0.5 | 100 | 强正则化 |
| 2.0 | 1000 | 极强正则化 |

**物理解释：** 噪声增加导致 OLS 更容易过拟合到噪声模式，需要更强的 L2 约束来抑制权重膨胀。

### 对 NN 设计的启示

1. **"线性基线 + 非线性微调"架构**
   - 光谱→log_g 映射几乎完全线性（noise=0 时 R²≈1）
   - NN 应设计为学习残差/微调，而非从头学习整个映射

2. **正则化策略需自适应噪声**
   - 低噪声：轻 weight decay
   - 高噪声：强 weight decay / dropout / denoising

3. **4096 维特征空间已足够冗余**
   - 即使 noise=0，极小正则化（α=0.001）就能显著超越 OLS
   - 说明存在多余自由度，NN 同样需要正则化

## 4.2 模型层洞见（用于优化模型）

### Ridge 一致性优于 OLS

在所有噪声水平下，最优调参的 Ridge 都优于 OLS：
- 最小改进：noise=0.1 时 +0.9%
- 最大改进：noise=2.0 时 +68.4%

**结论：** 即使是简单的 L2 正则化，在高维回归中也是必要的。

### 过度正则化的代价

| Noise | α=100 vs α_opt |
|-------|----------------|
| 0.0 | R² 从 0.999 降至 0.794 (−20%) |
| 0.1 | R² 从 0.909 降至 0.780 (−14%) |

**警示：** 固定 α 策略在跨噪声场景下不可行。

### 评估指标一致性

R², MAE, RMSE 三个指标在所有噪声水平下指向相同的最优 α，表明：
- 无需针对不同指标选择不同超参数
- 可以使用任一指标进行超参数搜索

## 4.3 物理层洞见

### 光谱-log_g 映射的线性本质

$$
\text{noise} = 0 \Rightarrow R^2 = 0.999
$$

这表明：
1. **log_g 信息分布在多个像素上**，而非稀疏谱线
2. **映射本质是线性的**，非线性模型可能只带来边际改进
3. **特征工程重点应在去噪**，而非复杂变换

### 噪声对信息的破坏

| Noise | Best R² | 信息损失 |
|-------|---------|----------|
| 0.0 | 0.999 | 0% |
| 0.1 | 0.909 | ~9% |
| 0.5 | 0.649 | ~35% |
| 1.0 | 0.451 | ~55% |
| 2.0 | 0.221 | ~78% |

**结论：** 噪声对性能的影响是剧烈的，再多的正则化也无法恢复丢失的信息。

---

# 5. 实验结果图

## 5.1 主图：Ridge 性能 vs log₁₀(α)（按噪声水平分组）

![Ridge Metrics vs Alpha by Noise](ridge_metrics_vs_alpha_by_noise.png)

**图表结构：** 3×1 布局
- **上图**：Test R² vs log₁₀(α)（越高越好）
- **中图**：Test MAE vs log₁₀(α)（越低越好）
- **下图**：Test RMSE vs log₁₀(α)（越低越好）

**颜色方案：** Viridis colormap
- 🟡 黄色：noise=0（干净数据）
- 🟣 紫色：noise=2（高噪声）

**标记说明：**
- 折线 + 圆点：Ridge 回归结果
- ★ 星号 (x=-3.5)：OLS 基线

---

# 6. 结论（Conclusion）

## 6.1 本实验验证了什么？

| 结论 | 证据 |
|------|------|
| ✅ 最优 α 随噪声单调增大 | α_opt: 0.001 → 1000 (noise: 0 → 2) |
| ✅ Ridge 在所有噪声下优于 OLS | Δ R²: +0.8% ~ +68.4% |
| ✅ 光谱-log_g 映射本质线性 | noise=0 时 R²=0.999 |
| ✅ 三个评估指标一致指向相同最优 α | R², MAE, RMSE 峰值/谷值重合 |
| ⚠️ 高噪声下线性模型能力有限 | noise=2 时 best R²=0.22 |

## 6.2 对模型设计的启示

1. **线性模型足以作为强基线**
   - noise ≤ 0.5 时，Ridge R² ≥ 0.65
   - NN 需要明显超越此基线才有价值

2. **正则化策略**
   - NN 的 weight decay 应与数据噪声水平挂钩
   - 考虑自适应正则化机制

3. **架构设计**
   - 推荐 "Linear + Residual" 结构
   - 先用 Ridge 获得初始预测，再用 NN 学习残差

## 6.3 对光谱物理的贡献

- 证实了 **4096 像素光谱对 log_g 的线性映射几乎完美**
- 信息冗余性高，支撑使用压缩/降维技术
- 噪声鲁棒性需要依靠正则化而非模型复杂度

## 6.4 仍不确定的问题

- 非线性模型（NN、LightGBM）在高噪声下能否超越 Ridge？
- 其他恒星参数（T_eff, [Fe/H]）是否呈现相似的线性特性？
- α_opt 是否可以从噪声统计量直接估计？

---

# 7. 下一步（Next Steps）

## 7.1 非线性模型对比

| 实验 | 目的 |
|------|------|
| LightGBM vs Ridge (same noise levels) | 量化非线性增益 |
| NN vs Ridge (same noise levels) | 评估神经网络必要性 |

## 7.2 其他恒星参数

```python
# 对 T_eff, [Fe/H] 重复相同实验
for param in ['teff', 'feh']:
    run_alpha_sweep(target=param, noise_levels=[0, 0.1, 0.5, 1.0, 2.0])
```

## 7.3 α 自动选择

- 交叉验证自动选择 α
- 探索 α 与噪声估计的函数关系

## 7.4 神经网络架构验证

```python
# 验证 "Linear + Residual" 设计
class LinearResidualNN(nn.Module):
    def __init__(self):
        self.ridge_layer = nn.Linear(4096, 1)  # 用 Ridge 权重初始化
        self.residual_net = MLP(4096, 1)
    
    def forward(self, x):
        return self.ridge_layer(x) + self.residual_net(x)
```

---

# 8. 相关文件索引

| 类型 | 路径 |
|------|------|
| 图表 | `figs/ridge_metrics_vs_alpha_by_noise.png` |
| 绘图脚本 | `scripts/plot_ridge_r2_vs_alpha.py` |
| Alpha Sweep 脚本 | `scripts/alpha_sweep.sh` |
| Ridge 配置 | `configs/exp/logg/linear_ridge.yaml` |
| 结果汇总 | `results/LINEAR_ALPHA_SWEEP_RESULTS.md` |

---

# 9. 复现命令

```bash
cd /home/swei20/VIT
python scripts/plot_ridge_r2_vs_alpha.py
# Output: figs/ridge_metrics_vs_alpha_by_noise.png
```

---

*Generated: 2025-11-27*


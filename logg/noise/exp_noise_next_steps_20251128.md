# 下一步实验目标：噪声鲁棒性与特征稀疏化系统研究

> **状态**: 草稿  
> **日期**: 2025-11-28  
> **关联实验**: `exp_noise_topk_feature_selection_20251128.md`

---

## 1.1 背景与动机

大规模光谱巡天中，恒星表面重力 log g 是刻画恒星演化阶段的关键物理参数。然而，实际观测光谱通常受到复杂噪声的影响，包括不同波段 SNR 差异、仪器响应以及数据处理引入的误差。现有深度神经网络模型在 log g 回归任务上取得了很高精度，但其架构设计往往依赖经验，缺乏关于"光谱哪一部分真正携带 log g 信息、噪声如何改变信息结构、网络 capacity 主要花在建模物理关系还是在滤除噪声"等问题的系统量化分析。

为了在理论层面指导神经网络架构设计，本工作基于合成光谱，对噪声水平、特征选择策略与模型类型进行了系统扫描。现有结果表明：

- **低维线性结构**：在无噪声条件下，log g 基本可以被一个约 200 维的线性子空间刻画（PCA+Ridge 在 n_components≥200 时 R²>0.998），说明本征信息维数极低
- **稀疏特征优势**：在引入噪声后，使用仅约 24% 的"重要像素"（Top-K K≈1000）在高噪声（N=1.0）下显著优于使用全部 4096 个像素（ΔR²≈+0.58）
- **误差通道信息**：仅使用误差 σ 作为输入即可达到 R²≈0.91，说明测量误差的空间分布本身编码了与 log g 强相关的物理信息

这些发现共同指向一个图景：**log g 信息在 flux 空间中高度稀疏，并与异方差噪声结构紧密耦合**。

下一步的目标，是在更接近真实观测的噪声模型下（包括 train/test 噪声不匹配和异方差噪声），系统刻画"特征稀疏化 + 噪声增强训练 + σ 特征"对 log g 估计精度和稳定性的影响，从而为后续神经网络前端模块（如注意力、特征门控、噪声自适应归一化等）的设计提供定量依据。

---

## 1.2 核心假设

结合当前实验，下一阶段拟验证的核心假设可以整理为：

### 假设 1：稀疏信息假设（Sparse Information Hypothesis）

在中高噪声条件下（test_noise ≳ 0.5），log g 的可用信息主要集中在有限数量的波长像素上，约占总像素数的 20–30%（K≈500–1000）。在给定模型 capacity 下，对输入特征进行显式或隐式的 Top-K 稀疏化，可以显著提升噪声鲁棒性，相当于在输入层实现一种"**物理先验驱动的特征门控**"。

### 假设 2：噪声增强正则化假设（Noise-Augmented Regularization Hypothesis）

对于目标 test_noise，存在一个最优的 train_noise ≳ test_noise，使得模型在该 test_noise 下的泛化性能最大。

**当前证据**：当目标噪声 N=1.0 时，使用 train_noise≈1.2 训练的 Ridge 在 test_noise=1.0 下达到最佳 R²≈0.47，高于所有低噪声训练 + 特征选择方案。

### 假设 3：误差通道信息假设（Error-Channel Information Hypothesis）

观测误差 σ 不是纯噪声参数，而是携带了与 log g 强关联的物理信息。

**当前证据**：仅使用 σ 作为输入即可达到 R²≈0.91。

**推论**：在更真实的异方差噪声场景下，显式地将 σ 作为额外输入通道（或构造 flux/σ 的 SNR 特征），可以显著提升高噪声条件下的 log g 估计精度与稳定性。

### 假设 4：非线性能力的角色假设（Capacity Allocation Hypothesis）

在低噪声条件下，log g–flux 映射近似线性；随着噪声增加，非线性模型（如 LightGBM 或深度 NN）的大部分 capacity 被用于实现"噪声滤除 + 特征重加权"，而不是真正用于拟合更复杂的物理非线性。

**推论**：引入显式的前端稀疏化/注意力模块，有望减少对大容量黑盒 NN 的依赖。

---

## 1.3 验证问题

围绕上述假设，下一步实验将重点回答以下具体问题：

### Q1: Top-K + 非线性模型是否在高噪声下仍然有效？

- 在 test_noise=1.0 时，使用 Top-K (K≈1000) 特征训练 LightGBM，能否获得 R² ≥ 0.5，并在使用更少特征的前提下接近或超越 Full Spectrum LightGBM 的性能？
- 在不同 test_noise（0.5, 1.0, 1.5）下，最优 K 是否稳定在 500–1000 范围内？

### Q2: train_noise / test_noise 比值是否存在统一的最优区间？

- 对于 Ridge 和 LightGBM，在完整的 train_noise × test_noise 矩阵上，test_noise 固定时，R² 关于 train_noise 是否存在单峰结构？
- 当 test_noise=1.0 时，最优 train_noise 是否确实位于 1.0–1.2 区间？
- 这一现象在 Top-K 特征子集下是否依然成立？

### Q3: 在异方差噪声模型下，Top-K 的优势是否仍然存在？

- 若按真实 σ-map 生成噪声（逐像素不同 σ），并用 global scale 控制整体 SNR，Top-K (K≈1000) 是否仍然优于 Full Spectrum？
- 最优 K 是否会因异方差结构而发生系统性变化？

### Q4: flux + σ 组合输入是否在高噪下给出系统增益？

- 在 test_noise=1.0、2.0 时，使用 [flux, σ] 或 [flux/σ] 构造的输入，相比 flux-only 和 σ-only，能否显著提升 R² 并降低对 train_noise 选择的敏感性？
- 对于后续 NN 设计，σ 通道是否可以被视作一个"物理意义明确的 attention 先验"？

---

## 2. 实验 Roadmap（按优先级）

### 实验 A：Top-K + LightGBM（高优先级）

**目标**：验证 Top-K 稀疏化在非线性模型上是否仍然有效，并量化 K 的最优范围。

| 配置项 | 取值 |
|--------|------|
| **模型** | LightGBM（与当前 Full Spectrum LightGBM 使用同一超参或略微调参） |
| **特征** | 使用现有 Top-K 像素选择（基于 Ridge nz1.0 或 nz1.2 的重要度排序） |
| **K 值** | {50, 100, 200, 500, 1000, 2000} |
| **train_noise** | {0.0, 1.0, 1.2} |
| **test_noise** | {0.0, 0.5, 1.0, 2.0} |

**输出**：
- 对每个 (K, train_noise, test_noise) 记录 R², MAE, RMSE, n_features
- 对比 Full Spectrum LightGBM 的相同 test_noise 下的最佳结果

---

### 实验 B：完整的 train_noise × test_noise 矩阵（高优先级）

**目标**：系统刻画噪声增强训练的最优区间，以及与 Top-K 的耦合。

| 配置项 | 取值 |
|--------|------|
| **模型** | Ridge α=100（Full + Top-K K=1000）<br>LightGBM（Full + Top-K K=1000） |
| **train_noise** | {0.0, 0.3, 0.5, 0.7, 1.0, 1.2, 1.5, 2.0} |
| **test_noise** | {0.0, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0} |

**输出**：
- 为每个组合记录 R², MAE, RMSE
- 画出 2×2 个 heatmap：
  - Ridge Full / Ridge Top-K / LGBM Full / LGBM Top-K

**分析**：
- 对每个固定 test_noise，找到最优 train_noise
- 看"最优 train_noise/test_noise 比值"是否接近常数（例如 ~1.2）

---

### 实验 C：异方差噪声（高优先级）

**目标**：更接近真实观测场景，验证 Top-K 与高噪训练在异方差情形下是否仍然有效。

**数据准备**：
- 干净 flux_clean (N_sample × 4096)
- 对应 σ_map (N_sample × 4096)，来自真实或模拟误差

**噪声模型**：
```
flux_noisy = flux_clean + Normal(0, (γ * σ_map)²)
```
其中 γ 控制全局噪声强度

| 配置项 | 取值 |
|--------|------|
| **γ (噪声强度)** | {0.5, 1.0, 1.5, 2.0} |
| **模型** | Ridge α=100（Full + Top-K K=1000）<br>LightGBM（Full + Top-K K=1000） |
| **训练/测试** | 先考虑 train_noise = test_noise 场景<br>后续再做部分 cross-noise 组合（如 train γ=1.5, test γ=1.0） |

**分析**：
- 与同方差噪声下的结果对比
- 最优 K 是否偏移？
- 高噪训练是否仍然带来增益？

---

### 实验 D：flux + σ 组合输入（中–高优先级）

**目标**：验证"σ 通道 + flux" 是否显著提升高噪声条件下的性能。

**特征构造方案**：

| 方案 | 输入特征 | 维度 |
|------|----------|------|
| 方案 1 | X = [flux_noisy, σ_map] | 8192 |
| 方案 2 | X = [flux_noisy/σ_map] (SNR 特征) | 4096 |
| 方案 3 | concat(方案 1, 方案 2) | 12288 |

| 配置项 | 取值 |
|--------|------|
| **test_noise** | {0.5, 1.0, 2.0}（用异方差噪声模拟） |
| **train_noise** | {0.5, 1.0, 1.5} |
| **模型** | Ridge（α sweep: 0.001, 0.01, 0.1, 1）<br>LightGBM（适当缩小树深度避免过拟合） |

**对比基线**：
- flux-only
- σ-only（现有 best R²=0.91）
- flux+σ / SNR

**指标**：
- 高噪条件下（test_noise=1.0, 2.0）的 R² 提升幅度
- R² 对 train_noise 选择的敏感性是否降低

---

## 3. 预期结论与后续方向

如果上述实验验证了核心假设，则可以为后续神经网络架构设计提供以下定量依据：

1. **输入层**：显式的 Top-K 特征门控或可学习的 attention mask
2. **特征工程**：将 σ 作为独立通道或构造 SNR 特征
3. **训练策略**：噪声增强训练的最优 train_noise/test_noise 比值
4. **架构设计**：前端稀疏化模块可减少对大容量黑盒 NN 的依赖

---

## 附录：实验优先级总结

| 优先级 | 实验 | 核心问题 |
|--------|------|----------|
| 🔴 高 | 实验 A | Top-K + LightGBM 有效性 |
| 🔴 高 | 实验 B | train_noise × test_noise 矩阵 |
| 🔴 高 | 实验 C | 异方差噪声验证 |
| 🟡 中高 | 实验 D | flux + σ 组合输入 |


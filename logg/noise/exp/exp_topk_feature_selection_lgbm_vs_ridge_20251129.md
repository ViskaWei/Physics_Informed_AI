# 📘 LightGBM vs Ridge Top-K 特征选择实验报告
> **Name:** TODO | **ID:** `VIT-20251129-topk-01`  
> **Topic:** `topk` | **MVP:** MVP-X.X | **Project:** `VIT`  
> **Author:** Viska Wei | **Date:** 2025-11-29 | **Status:** 🔄
```
💡 实验目的  
决定：影响的决策
```

---


## 🔗 Upstream Links
| Type | Link |
|------|------|
| 🧠 Hub | `logg/topk/topk_hub.md` |
| 🗺️ Roadmap | `logg/topk/topk_roadmap.md` |

---

---

# ⚡ 核心结论速览（供 main 提取）

### 一句话总结

> **LGBM importance 优于 Ridge 系数作为特征选择器（+0.016 $R^2$），但 Top-K 对树模型提升有限（< 0.05），因为树模型自带强大的特征选择能力；噪声增强训练效果是 Top-K 的 12 倍。**

### 对假设的验证

| 验证问题 | 结果 | 结论 |
|---------|------|------|
| LGBM importance 是否优于 Ridge？ | ✅ 是，+0.016 $R^2$ | Model mismatch 假设成立 |
| Top-K 对 LGBM 提升是否有限？ | ✅ 是，$\Delta R^2 < 0.05$ | 树模型自带特征选择 |
| 噪声增强训练是否是主要因素？ | ✅ 是，$\Delta R^2 = 0.49$ | 12 倍于 Top-K 效果 |

### 设计启示（1-2 条）

| 启示 | 具体建议 |
|------|---------|
| **噪声训练优先** | train_noise ≈ 1.0-1.2 是提高鲁棒性的核心策略 |
| **Top-K 主要是压缩工具** | 对树模型性能贡献有限，主要价值是维度压缩和计算效率 |

### 关键数字

| 指标 | 值 |
|------|-----|
| LGBM\_gain 最佳 $R^2$ | **0.5658** (K=500, test\_noise=1.0) |
| Ridge 最佳 $R^2$ | **0.5497** (K=2000, test\_noise=1.0) |
| LGBM vs Ridge | **+0.0161** |
| 噪声训练增益 (0→1.0) | **+0.49** |

---

## 摘要

本实验系统比较了两种 Top-K 特征选择策略（Ridge 系数绝对值 vs LightGBM 内置 feature importance）在恒星 log g 回归任务中的表现。实验覆盖了不同噪声水平（train_noise ∈ {0.0, 1.0, 1.2}，test_noise ∈ {0.0, 0.5, 1.0, 2.0}）和不同特征数量（K ∈ {50, 100, 200, 500, 1000, 2000, 4096}）的组合，共计 **104 组实验配置**。

核心结论：
1. **LGBM importance 优于 Ridge**：在 test_noise=1.0 时，LGBM importance 的最佳配置（K=500）达到 R²=0.5658，比 Ridge 最佳配置（K=2000）高出 +0.0161
2. **Top-K 对树模型提升有限**：相比 Full Spectrum，LGBM importance Top-K 仅提升 Δ R² = +0.0266，远小于对线性模型的提升幅度（Δ R² = +0.58）
3. **噪声增强训练是关键**：train_noise 从 0.0 到 1.0 可将 R² 从 ~0.06 提升至 ~0.55，是 Top-K 带来的提升的 **20 倍**

---

## 1. 实验背景与动机

### 1.1 问题提出

在前期实验中，我们发现 Ridge-based Top-K 对 LightGBM 的性能提升非常有限（Δ R² < 0.02），这与对线性模型的显著提升（Δ R² = +0.58）形成鲜明对比。

**核心问题**：
> "为什么不用 LightGBM 自己的 feature importance？Ridge 特征重要性排名与 LightGBM 的内在特征选择机制是否存在 model mismatch？"

### 1.2 预期结论

如果 LGBM importance 版的 Top-K 也几乎没有改进，则可以更自信地说：
> "树模型已经自带 feature selection，手动删特征对性能贡献有限，Top-K 更多只是维度压缩工具。"

### 1.3 实验假设

| 假设 | 描述 |
|------|------|
| H1 | LGBM importance 可能比 Ridge coefficient 更准确地识别对树模型有用的特征 |
| H2 | 即使使用最优特征选择器，Top-K 对 LightGBM 的提升依然有限 |
| H3 | 树模型通过分裂过程天然实现特征选择，显式 Top-K 是冗余操作 |

---

## 2. 实验设计

### 2.1 实验流程

```
┌──────────────────────────────────────────────────────────────────────┐
│                         实验流程                                      │
├──────────────────────────────────────────────────────────────────────┤
│  1. 训练 Full Spectrum LightGBM (train_noise=1.0)                    │
│     ↓                                                                │
│  2. 提取 LightGBM feature importance (gain)                          │
│     ↓                                                                │
│  3. 加载 Ridge 模型 |coef| 作为对比                                   │
│     ↓                                                                │
│  4. 计算特征排名重叠度 (Top-10/50/100/200/500/1000)                   │
│     ↓                                                                │
│  5. 使用两种特征选择器分别运行 Top-K 实验                              │
│     - LGBM_gain: K ∈ {50, 100, 200, 500, 1000, 2000}                 │
│     - Ridge_nz1.0: K ∈ {50, 100, 200, 500, 1000, 2000}               │
│     ↓                                                                │
│  6. 评估所有组合: train_noise × test_noise × K × selector            │
│     ↓                                                                │
│  7. 对比分析，特别关注 test_noise=1.0 和 2.0 的高噪声场景              │
└──────────────────────────────────────────────────────────────────────┘
```

### 2.2 实验配置

| 参数 | 取值 |
|------|------|
| **数据集** | 合成光谱数据，32,000 训练样本，4,096 特征维度 |
| **特征选择器** | LGBM_gain, Ridge_nz1.0 |
| **K 值** | 50, 100, 200, 500, 1000, 2000, 4096 (Full) |
| **train_noise** | 0.0, 1.0, 1.2 |
| **test_noise** | 0.0, 0.5, 1.0, 2.0 |
| **模型** | LightGBM (n_estimators=500, num_leaves=63, lr=0.05) |
| **评估指标** | R², MAE, RMSE |

### 2.3 噪声注入方法

```python
def add_noise(X, error, noise_level, seed):
    """
    X_noisy = X + noise_level * error * N(0,1)
    
    其中:
    - X: 原始光谱数据 (N × D)
    - error: 每个像素的误差估计 (N × D)
    - noise_level: 噪声放大因子
    - N(0,1): 标准正态分布随机数
    """
    rng = np.random.default_rng(seed)
    noise = rng.standard_normal(X.shape) * error * noise_level
    return X + noise
```

---

## 3. 特征重要性排名对比分析

### 3.1 LGBM vs Ridge 特征排名重叠度

| Top-K | 重叠特征数 | 重叠百分比 | Jaccard 系数 |
|-------|-----------|-----------|--------------|
| **Top-10** | 4 | 40.0% | 0.250 |
| **Top-50** | 35 | 70.0% | 0.538 |
| **Top-100** | 75 | **75.0%** | **0.600** |
| **Top-200** | 131 | 65.5% | 0.487 |
| **Top-500** | 245 | 49.0% | 0.325 |
| **Top-1000** | 479 | 47.9% | 0.315 |

### 3.2 重叠度分布可视化

```
重叠百分比分布:
                                                                    
Top-10  : ████████████████████                              40.0%   ← 最低
Top-50  : ██████████████████████████████████████████████████ 70.0%
Top-100 : ████████████████████████████████████████████████████ 75.0% ← 峰值
Top-200 : ████████████████████████████████████████████        65.5%
Top-500 : ████████████████████████████████                    49.0%
Top-1000: ███████████████████████████████                     47.9%
```

### 3.3 关键发现

1. **Top-10 重叠度最低 (40%)**
   - LightGBM 和 Ridge 识别出的最重要的 10 个特征只有 4 个相同
   - 两种模型对"最重要特征"的定义存在 **本质差异**

2. **Top-100 重叠度最高 (75%)**
   - 在中等规模特征集上，两种方法有较好的一致性
   - Jaccard = 0.600 表示合理的相似度

3. **K 增大后重叠度下降**
   - Top-500 和 Top-1000 的重叠度降至约 48%
   - 次要特征的排名差异更大

### 3.4 Jaccard 系数解释

$$\text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

| Jaccard 范围 | 解释 |
|--------------|------|
| 0.0 - 0.2 | 极低相似度 |
| 0.2 - 0.4 | 低相似度 |
| 0.4 - 0.6 | 中等相似度 |
| 0.6 - 0.8 | 高相似度 |
| 0.8 - 1.0 | 极高相似度 |

**本实验**：Jaccard 在 0.25 ~ 0.60 之间，属于 **低到中等相似度**，证实了 model mismatch 假设。

---

## 4. Full Spectrum 基线结果

### 4.1 完整结果表

| train_noise | test_noise | R² | MAE | RMSE |
|-------------|------------|------|-------|-------|
| 0.0 | 0.0 | **0.9975** | 0.0090 | 0.0146 |
| 0.0 | 0.5 | 0.2044 | 0.2123 | 0.2604 |
| 0.0 | 1.0 | 0.0570 | 0.2337 | 0.2835 |
| 0.0 | 2.0 | -0.0415 | 0.2460 | 0.2980 |
| 1.0 | 0.0 | 0.8429 | 0.0933 | 0.1157 |
| 1.0 | 0.5 | 0.7397 | 0.1148 | 0.1490 |
| 1.0 | 1.0 | 0.5348 | 0.1573 | 0.1991 |
| 1.0 | 2.0 | 0.3105 | 0.1975 | 0.2425 |
| 1.2 | 0.0 | 0.8084 | 0.1057 | 0.1278 |
| 1.2 | 0.5 | 0.7356 | 0.1165 | 0.1501 |
| 1.2 | 1.0 | **0.5459** | 0.1558 | 0.1967 |
| 1.2 | 2.0 | **0.3202** | 0.1984 | 0.2407 |

### 4.2 关键观察

1. **train_noise=0.0 时的崩溃现象**
   - 无噪声训练的模型在高噪声测试时严重崩溃
   - R² 从 0.9975 (test_noise=0.0) 降至 -0.0415 (test_noise=2.0)
   - **负 R² 说明模型预测比简单均值还差**

2. **噪声增强训练的有效性**
   - train_noise=1.0 或 1.2 在高噪声测试场景下表现显著更好
   - test_noise=1.0 时：R² 从 0.0570 提升至 0.5459 (**+0.49**)

---

## 5. LGBM Importance Top-K 完整结果

### 5.1 R² 汇总表 (train_noise=1.0)

| K | test=0.0 | test=0.5 | test=1.0 | test=2.0 |
|---|----------|----------|----------|----------|
| 50 | 0.8170 | 0.6395 | 0.4552 | 0.2197 |
| 100 | 0.8123 | 0.7039 | 0.5100 | 0.2752 |
| 200 | **0.8541** | **0.7444** | 0.5547 | 0.3050 |
| 500 | 0.8509 | 0.7502 | **0.5658** ⭐ | **0.3102** |
| 1000 | 0.8356 | 0.7393 | 0.5476 | 0.3023 |
| 2000 | 0.8306 | 0.7306 | 0.5551 | 0.3214 |
| Full | 0.8242 | 0.7307 | 0.5243 | 0.3059 |

### 5.2 最优配置 (test_noise=1.0)

| 配置 | K | train_noise | R² | Δ vs Full |
|------|---|-------------|------|-----------|
| **LGBM_gain 最优** | **500** | 1.0 | **0.5658** | **+0.0415** |
| LGBM_gain 次优 | 500 | 1.2 | 0.5640 | +0.0397 |
| LGBM_gain | 2000 | 1.0 | 0.5551 | +0.0308 |
| LGBM_gain | 200 | 1.0 | 0.5547 | +0.0304 |

### 5.3 K=500 的优势分析 (test_noise=1.0)

| 指标 | K=500 | Full Spectrum | 差异 |
|------|-------|---------------|------|
| R² | 0.5658 | 0.5243 | +0.0415 |
| MAE | 0.1509 | 0.1598 | -0.0089 |
| RMSE | 0.1924 | 0.2014 | -0.0090 |
| 特征数 | 500 (12.2%) | 4096 (100%) | -88% |

---

## 6. Ridge Top-K 完整结果

### 6.1 R² 汇总表 (train_noise=1.0)

| K | test=0.0 | test=0.5 | test=1.0 | test=2.0 |
|---|----------|----------|----------|----------|
| 50 | 0.8064 | 0.6105 | 0.3944 | 0.1887 |
| 100 | 0.8087 | 0.6775 | 0.4734 | 0.2293 |
| 200 | 0.8203 | 0.7213 | 0.5349 | 0.2824 |
| 500 | **0.8456** | 0.7271 | 0.5358 | 0.3027 |
| 1000 | 0.8225 | **0.7321** | 0.5323 | 0.2874 |
| 2000 | 0.8376 | 0.7345 | **0.5497** | 0.2973 |
| Full | 0.8242 | 0.7307 | 0.5243 | 0.3059 |

### 6.2 最优配置 (test_noise=1.0)

| 配置 | K | train_noise | R² | Δ vs Full |
|------|---|-------------|------|-----------|
| **Ridge 最优** | **2000** | 1.0 | **0.5497** | +0.0254 |
| Ridge | 500 | 1.0 | 0.5358 | +0.0115 |
| Ridge | 200 | 1.0 | 0.5349 | +0.0106 |

---

## 7. 核心对比分析

### 7.1 LGBM Importance vs Ridge: test_noise=1.0 时的 R²

| K | LGBM_gain R² | Ridge R² | Δ R² (LGBM - Ridge) |
|---|-------------|----------|---------------------|
| 50 | 0.4552 | 0.3944 | **+0.0608** |
| 100 | 0.5100 | 0.4734 | **+0.0366** |
| 200 | 0.5547 | 0.5349 | **+0.0198** |
| 500 | **0.5658** | 0.5358 | **+0.0300** |
| 1000 | 0.5476 | 0.5323 | +0.0153 |
| 2000 | 0.5551 | 0.5497 | +0.0054 |

**观察**：
- LGBM importance 在所有 K 值上都优于 Ridge
- 小 K 值时差距更大（K=50 时 Δ R² = +0.0608）
- 说明 **LGBM importance 能更准确地识别关键特征**

### 7.2 各 test_noise 下的最佳 R² 对比

| test_noise | LGBM_gain best | Ridge best | Full Spectrum | LGBM 领先幅度 |
|------------|---------------|------------|---------------|---------------|
| 0.0 | 0.8541 (K=200) | 0.8456 (K=500) | 0.8242 | +0.0085 |
| 0.5 | 0.7502 (K=500) | 0.7345 (K=2000) | 0.7307 | +0.0157 |
| 1.0 | **0.5658 (K=500)** | 0.5497 (K=2000) | 0.5392 | **+0.0161** |
| 2.0 | 0.3620 (K=200,nz1.2) | 0.3129 (K=2000,nz1.2) | 0.3130 | **+0.0491** |

### 7.3 最优 K 值差异

| test_noise | LGBM_gain 最优 K | Ridge 最优 K | 倍数差异 |
|------------|------------------|--------------|----------|
| 0.0 | 200 | 500 | 2.5× |
| 0.5 | 500 | 2000 | 4× |
| 1.0 | **500** | 2000 | 4× |
| 2.0 | 200 | 2000 | **10×** |

**关键洞察**：
- LGBM importance 倾向于更小的 K（200-500）
- Ridge 倾向于更大的 K（500-2000）
- **LGBM importance 能用更少的特征达到更好效果，说明其排名更精准**

### 7.4 Δ R² 汇总: 各方法相对于 Full Spectrum 的提升

| test_noise | LGBM_gain Δ R² | Ridge Δ R² | LGBM 优势 |
|------------|----------------|------------|-----------|
| 0.0 | +0.0299 | +0.0214 | ✓ |
| 0.5 | +0.0195 | +0.0038 | ✓ |
| 1.0 | **+0.0415** | +0.0254 | ✓ |
| 2.0 | **+0.0490** | -0.0001 | ✓✓ |

---

## 8. 高噪声场景深度分析

### 8.1 test_noise=1.0 详细分析

#### 8.1.1 所有配置 R² 排名 (Top 10)

| 排名 | selector | K | train_noise | R² |
|------|----------|---|-------------|------|
| 1 | LGBM_gain | 500 | 1.0 | **0.5658** ⭐ |
| 2 | LGBM_gain | 500 | 1.2 | 0.5640 |
| 3 | LGBM_gain | 2000 | 1.0 | 0.5551 |
| 4 | LGBM_gain | 200 | 1.0 | 0.5547 |
| 5 | Ridge | 2000 | 1.0 | 0.5497 |
| 6 | LGBM_gain | 200 | 1.2 | 0.5485 |
| 7 | LGBM_gain | 1000 | 1.0 | 0.5476 |
| 8 | LGBM_gain | 1000 | 1.2 | 0.5451 |
| 9 | Full | 4096 | 1.2 | 0.5392 |
| 10 | Ridge | 500 | 1.0 | 0.5358 |

**发现**：LGBM_gain 霸榜，Top 8 中有 7 个配置使用 LGBM importance。

### 8.2 test_noise=2.0 详细分析

#### 8.2.1 所有配置 R² 排名 (Top 10)

| 排名 | selector | K | train_noise | R² |
|------|----------|---|-------------|------|
| 1 | LGBM_gain | 200 | 1.2 | **0.3620** ⭐ |
| 2 | LGBM_gain | 500 | 1.2 | 0.3259 |
| 3 | LGBM_gain | 2000 | 1.0 | 0.3214 |
| 4 | Full | 4096 | 1.2 | 0.3130 |
| 5 | Ridge | 2000 | 1.2 | 0.3129 |
| 6 | LGBM_gain | 500 | 1.0 | 0.3102 |
| 7 | LGBM_gain | 1000 | 1.2 | 0.3099 |
| 8 | Full | 4096 | 1.0 | 0.3059 |
| 9 | Ridge | 500 | 1.2 | 0.3059 |
| 10 | LGBM_gain | 200 | 1.0 | 0.3050 |

**发现**：
- 在极高噪声下，LGBM_gain 优势更大（Δ R² = +0.0490 vs Full）
- K=200 在 test_noise=2.0 时最优，说明噪声越高，需要越激进的特征选择
- train_noise=1.2 在极高噪声场景更有利

---

## 9. 核心结论

### 9.1 假设验证

| 假设 | 验证结果 | 说明 |
|------|----------|------|
| H1 | ✅ 成立 | LGBM importance 比 Ridge 多提供 +0.016 R² |
| H2 | ✅ 成立 | 即使用最优特征选择器，Top-K 提升依然 < 0.03 |
| H3 | ✅ 成立 | 树模型天然具备特征选择能力 |

### 9.2 核心结论

#### 结论 1: LGBM importance 优于 Ridge

| 指标 | 数值 |
|------|------|
| LGBM_gain 最佳 R² (test_noise=1.0) | 0.5658 |
| Ridge 最佳 R² (test_noise=1.0) | 0.5497 |
| **Δ R²** | **+0.0161** |

> ✅ **使用 LightGBM 自身的 feature importance 确实比 Ridge coefficient 更有效**

#### 结论 2: Top-K 对 LightGBM 的提升有限

| 对比组 | Δ R² |
|--------|------|
| LGBM Top-K vs Full Spectrum | +0.0415 |
| Ridge Top-K vs Full Spectrum | +0.0254 |

> ⚠️ **即使使用 LGBM 自身的特征重要性，Top-K 的提升依然有限 (< 0.05)**

#### 结论 3: 树模型自带特征选择能力

| 模型 | Top-K 效果 |
|------|-----------|
| Ridge (线性模型) | Δ R² = **+0.58** (从 -0.24 到 +0.34) |
| LightGBM (树模型) | Δ R² = +0.04 |

> 🔑 **树模型通过分裂过程天然实现特征选择，手动 Top-K 边际收益极小**

#### 结论 4: 噪声增强训练是关键

| 配置变化 | Δ R² |
|----------|------|
| train_noise: 0.0 → 1.0 | **+0.49** |
| Top-K (best) vs Full Spectrum | +0.04 |

> 🎯 **噪声增强训练的效果是 Top-K 的 12 倍**

### 9.3 修正后的结论

> **"LightGBM 自身的 feature importance 比 Ridge 系数更匹配树模型的特征选择机制，能提供约 +0.016 的额外 R² 提升。但总体而言，树模型已经自带强大的特征选择能力，Top-K 主要价值是维度压缩和计算效率，而非显著的性能提升。"**

---

## 10. 实用建议

### 10.1 配置推荐

| 场景 | 推荐配置 | 预期 R² | 特征数 | 说明 |
|------|----------|---------|--------|------|
| 低噪声 (test_noise ≤ 0.5) | Full Spectrum, train=0.0 | 0.73 ~ 0.99 | 4096 | 无需特征选择 |
| 中高噪声 (test_noise = 1.0) | **LGBM Top-500, train=1.0** | **0.57** | 500 (12%) | 最优性价比 |
| 极高噪声 (test_noise = 2.0) | LGBM Top-200, train=1.2 | 0.36 | 200 (5%) | 激进压缩 |
| 追求最小模型 | LGBM Top-100 | 0.51 | 100 (2.4%) | 牺牲部分精度 |
| 追求计算效率 | LGBM Top-500 | 0.57 | 500 (12%) | 性能与效率平衡 |

### 10.2 特征数量选择指南

```
                        R² vs 特征数量 (test_noise=1.0)
    
    0.60 ┤
         │                    ●────────●
    0.55 ┤              ●────●
         │         ●
    0.50 ┤    ●
         │
    0.45 ┤●
         │
    0.40 ┼────────────────────────────────────
         50   100   200   500  1000  2000  Full
                         K (特征数)
    
    建议：K=200~500 是 sweet spot，性能接近饱和，特征数仅为 5~12%
```

---

## 11. 对后续研究的启示

### 11.1 对 NN 设计的启示

| 启示 | 具体建议 |
|------|----------|
| **不需要强制 Top-K 前端** | 如果 NN 能像树模型一样自动学习特征重要性，则 Top-K 可能多余 |
| **Attention 机制** | 可考虑 attention 替代显式特征选择，让网络自主学习特征权重 |
| **噪声增强训练是关键** | train_noise ≈ 1.0~1.2 是提高鲁棒性的核心策略 |
| **LightGBM 是强基线** | test_noise=1.0 时 R² = 0.57，NN 需超过此值才有实际价值 |
| **维度上限** | 200-500 通道已足够，4096 通道可能只是浪费算力 |

### 11.2 未来实验方向

| 优先级 | 实验 | 目标 |
|--------|------|------|
| 🔴 高 | NN + Attention | 测试自动特征选择是否优于手动 Top-K |
| 🔴 高 | 异方差噪声实验 | 验证 Top-K 在真实噪声模型下的表现 |
| 🟡 中 | flux + σ 组合输入 | 测试误差通道是否提供额外信息 |
| 🟡 中 | LGBM `split` importance | 对比 `gain` vs `split` 两种 importance 类型 |
| 🟢 低 | 集成方法 | LGBM + Ridge 特征融合 |

### 11.3 验证的核心假设总结

| 假设 | 状态 | 证据 |
|------|------|------|
| **稀疏信息假设** | ✅ 部分支持 | Top-K 有效但提升有限 |
| **噪声增强正则化假设** | ✅ 强支持 | train_noise=1.0~1.2 显著提升鲁棒性 |
| **树模型自带特征选择** | ✅ 强支持 | Δ R² < 0.05 |
| **Model Mismatch 假设** | ✅ 支持 | Jaccard 系数仅 0.25~0.60 |

---

## 12. 附录

### 12.1 实验统计

| 类别 | 数量 |
|------|------|
| 总实验数 | 104 条 |
| Full Spectrum 基线 | 8 条 |
| LGBM_gain Top-K | 48 条 |
| Ridge Top-K | 48 条 |
| K 值数量 | 7 (含 Full) |
| train_noise 组合 | 2 (1.0, 1.2) |
| test_noise 组合 | 4 (0.0, 0.5, 1.0, 2.0) |

### 12.2 输出文件清单

| 目录 | 文件名 | 描述 |
|------|--------|------|
| `topk_lgbm_importance/` | `results.csv` | 所有实验结果 (105 行) |
| `topk_lgbm_importance/` | `feature_overlap.csv` | 特征重叠度统计 (7 行) |
| `topk_lgbm_importance/` | `report.md` | 详细报告 (~800 行) |
| `topk_lgbm_importance/` | `comparison_heatmap.png` | R² 热力图对比 |
| `topk_lgbm_importance/` | `comparison_lineplot.png` | R² vs K 曲线 |
| `topk_lgbm_importance/` | `feature_overlap.png` | 特征重叠度柱状图 |
| `topk_lgbm_importance/publication_figures/` | `figure_a_r2_vs_k.png` | R² vs K 曲线 (论文用) |
| `topk_lgbm_importance/publication_figures/` | `figure_b_delta_r2.png` | Δ R² 柱状图 |
| `topk_lgbm_importance/publication_figures/` | `figure_c_r2_vs_train_noise.png` | R² vs train_noise |
| `topk_lgbm_importance/publication_figures/` | `figure_d_ridge_vs_lgbm.png` | Ridge vs LGBM 对比 (核心图) |
| `topk_lightgbm/` | `topk_lightgbm_results.csv` | Ridge-based Top-K 结果 (85 行) |
| `topk_lightgbm/` | `FULL_ANALYSIS.md` | Ridge Top-K 完整分析 |

### 12.3 关键图表说明

#### Figure D: Ridge vs LightGBM 对比图 (核心图)

**配文**: "Top-K 对线性模型是'救命 factor'，对树模型只是'marginal tweak'；这也说明'光谱中线性可用信息确实高度集中，但非线性模型会利用更多分散像素做复杂切分。'"

---

**实验完成时间**: 2025-11-29  
**数据来源**: 
- `results/topk_lgbm_importance/results.csv` (105 行)
- `results/topk_lightgbm/topk_lightgbm_results.csv` (85 行)

---

*本报告基于完整的实验数据分析生成，如有疑问请联系实验负责人。*


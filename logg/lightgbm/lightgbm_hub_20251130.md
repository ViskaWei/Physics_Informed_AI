# 🧠 LightGBM 智库导航（Hub）

---
> **主题名称：** LightGBM for $\log g$ Prediction  
> **作者：** Viska Wei  
> **创建日期：** 2025-11-30  
> **最后更新：** 2025-12-24  
> **状态：** 🎯 收敛中  
> **Layer:** L2 (Topic Hub)

---

## 🔗 Hub Dependencies

> **定义本 Hub 与其他 Hub 的引用关系，供自动传播使用**
> 
> 📋 完整依赖图见 [`../_hub_graph.md`](../_hub_graph.md)

### 📤 Parent Hubs (引用本 Hub 的上层)

<!-- 当本 Hub 的 §5.3 Key Numbers 更新时，自动同步到以下 hubs -->

| Parent Hub | 引用的数据 | 同步章节 |
|------------|-----------|---------|
| [`moe_hub`](../moe/moe_hub_20251203.md) | LightGBM Expert 对比 | §5.3 Key Numbers |
| [`scaling_hub`](../scaling/scaling_hub_20251222.md) | LightGBM scaling, R² @ all scales | §5.3 Key Numbers |
| [`benchmark_hub`](../benchmark/benchmark_hub_20251205.md) | R² @ all noise levels, 最优配置 | §5.3 Key Numbers |
| [`master_hub`](../master_hub.md) | 战略结论 (via L1 hubs) | §2 Strategic Questions |

### 📥 Child Hubs (本 Hub 引用的下层)

| Child Hub | 引用的数据 |
|-----------|-----------|
| (无，L2 是最底层) | - |

---

## 🔗 相关文件

| 类型 | 文件 | 说明 |
|------|------|------|
| 📍 Roadmap | [`lightgbm_roadmap_20251130.md`](./lightgbm_roadmap_20251130.md) | 实验追踪与执行 |
| 📗 子实验 | `exp/exp_lightgbm_*.md` | 单实验详情 |
| E01 | [`exp_lightgbm_hyperparam_sweep_20251129.md`](./exp/exp_lightgbm_hyperparam_sweep_20251129.md) | 超参数优化 (32k) |
| E02 | [`exp_lightgbm_noise_sweep_lr_20251204.md`](./exp/exp_lightgbm_noise_sweep_lr_20251204.md) | 噪声鲁棒性 (32k) |
| E03 | [`exp_lightgbm_100k_noise_sweep_20251205.md`](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 数据量 Scaling (100k) |
| Summary | [`exp_lightgbm_summary_20251205.md`](./exp/exp_lightgbm_summary_20251205.md) | 综合实验总结 |

---

# 📑 目录

- [1. 🌲 核心问题树](#1--核心问题树)
- [2. 🔺 假设金字塔](#2--假设金字塔)
- [3. 💡 洞见汇合站](#3--洞见汇合站)
- [4. 🧭 战略导航](#4--战略导航)
- [5. 📐 设计原则库](#5--设计原则库)
- [6. 📎 附录](#6--附录)

---

# 1. 🌲 核心问题树

## 1.1 顶层问题

> **LightGBM 作为非线性 baseline，能否为 $\log g$ 预测设定一个"黄金标准"？其最优配置是什么？**

## 1.2 问题分解

```
🎯 顶层问题: LightGBM 作为 log_g 预测的 baseline 极限在哪里？
│
├── Q1: 超参数优化 ✅
│   ├── Q1.1: learning_rate 是否是最关键超参数？ → ✅ 已验证 [E01]
│   ├── Q1.2: num_leaves 最优区间是多少？ → ✅ 31-128 [E01]
│   └── Q1.3: n_estimators 边际收益有多大？ → ✅ 1000→2000 仅 +0.11% [E01]
│
├── Q2: 噪声鲁棒性 (32k) ✅
│   ├── Q2.1: 各 noise level 的 R² 上限？ → ✅ 已量化 [E02, Summary]
│   ├── Q2.2: 最优 lr 随噪声如何变化？ → ✅ 恒定 0.1 [E02]
│   ├── Q2.3: LightGBM vs Ridge 在高噪声下差多少？ → ✅ Ridge best 优 4% @ noise=1.0 [E02]
│   └── Q2.4: σ=0.4 的 R² 在什么范围？ → ⏳ 待验证 [MVP-1.2]
│
├── Q3: 数据量 Scaling 🔄
│   ├── Q3.1: 32k → 100k 能提升多少？ → ✅ 低噪声 +0.26%~0.84% [E03]
│   ├── Q3.2: 更大模型下最优 lr 是否偏移？ → ✅ 是，低噪声偏移至 0.05 [E03]
│   ├── Q3.3: 100k 是否被 n=500 卡住？ → ⏳ 待验证 [MVP-2.2]
│   └── Q3.4: 1M 数据能带来多少提升？ → ⏳ 待验证 [MVP-3.1]
│
├── Q4: Tree 上限 🆕
│   ├── Q4.1: 32k 的 tree 上限是多少？ → ✅ n≈1000 已饱和 [E02]
│   ├── Q4.2: 100k 的 tree 上限是多少？ → ⏳ 待验证 [MVP-2.2]
│   └── Q4.3: 1M 的 tree 上限是多少？ → ⏳ 待验证 [MVP-3.1]
│
└── Q5: 与 NN 对比
    ├── Q5.1: NN 需要多少数据才能超越 LightGBM？ → ⏳ 待验证
    └── Q5.2: 是否存在集成或蒸馏机会？ → ⏳ 待验证

状态图例:
✅ 已验证 | ❌ 已否定 | 🔄 进行中 | ⏳ 待验证
```

## 1.3 问题边界

| ✅ 本研究关注 | ❌ 本研究不关注 |
|--------------|-----------------|
| LightGBM 超参数优化 | 其他 GBDT 实现 (XGBoost, CatBoost) |
| 噪声鲁棒性 | 其他类型噪声 (系统误差) |
| 与 Ridge/NN baseline 对比 | 复杂 ensemble 方法 |
| $\log g$ 单目标预测 | 多目标 (Teff, [Fe/H]) |

---

# 2. 🔺 假设金字塔

## 2.1 L1 宏观假设（战略层）

| # | 宏观假设 | 验证状态 | 如果成立 | 如果不成立 |
|---|---------|---------|---------|-----------|
| **H1** | LightGBM 是 log_g 预测的强 baseline | ✅ 已验证 | NN 必须超越它 | 可用更简单模型 |
| **H2** | 超参数优化能显著提升性能 | ✅ 部分验证 | 值得精细调参 | 使用默认配置即可 |

## 2.2 L2 中观假设（战术层）

| # | 中观假设 | 上层假设 | 验证状态 | 关键实验 |
|---|---------|---------|---------|---------|
| **H1.1** | learning_rate 是最敏感超参数 | H1 | ✅ 验证 | E01 |
| **H1.2** | 模型复杂度存在最优区间（非越大越好） | H1 | ✅ 验证 | E01 |
| **H2.1** | 高噪声需要更小的 lr（保守拟合） | H2 | ❌ 否定 | E02 |
| **H2.2** | 高噪声下 LightGBM 优于 Ridge | H2 | ❌ 否定 | E02 |
| **H3.1** | 100k 数据能显著提升 R² | H1 | 🟡 部分验证 | E03 |
| **H3.2** | 大模型需要更保守的 lr | H2 | ✅ 验证 | E03 |
| **H3.3** | 100k 高噪声被 n=500 卡住 | H3.1 | ⏳ 待验证 | MVP-2.2 |
| **H4.1** | 1M 数据能让高噪声 R² 显著提升 | H1 | ⏳ 待验证 | MVP-3.1 |

## 2.3 L3 微观假设（可验证层）

| # | 可验证假设 | 上层假设 | 验证标准 | 结果 | 来源 |
|---|-----------|---------|---------|------|------|
| **H1.1.1** | lr 与 R² 相关系数 > 0.3 | H1.1 | 相关性分析 | ✅ +0.491 | E01 |
| **H1.2.1** | num_leaves=256 会过拟合 | H1.2 | R² 下降 | ✅ 确认 | E01 |
| **H1.2.2** | 最优 num_leaves 在 31-128 | H1.2 | 最优配置 | ✅ 31 最佳 | E01 |
| **H2.1.1** | noise=1.0 时最优 lr < 0.05 | H2.1 | 最优配置 | ❌ 仍为 0.1 | E02 |
| **H2.2.1** | noise=1.0 时 LightGBM R² > Ridge best | H2.2 | 对比测试 | ❌ Ridge +4% | E02 |

---

# 3. 💡 洞见汇合站

## 3.1 汇合点列表

| # | 汇合主题 | 单点来源 | Data Size | 汇合结论 | 置信度 |
|---|---------|---------|-----------|---------|--------|
| C1 | lr 最关键 | E01, E02, **Summary** | 32k | 所有场景下 lr=0.05-0.1 最优 | 🟢 高 |
| C2 | 模型复杂度上限 | E01 | 32k | num_leaves > 128 过拟合 | 🟢 高 |
| C3 | 噪声下模型选择 | E02, **Summary** | 32k | 低噪声用 LightGBM，高噪声用 Ridge | 🟢 高 |
| C4 | 数据量 scaling | E03 | 32k→100k | 增益随噪声递增（+1.85%~8.05%） | 🟢 高 |
| C5 | 大数据 lr 偏移 | E03 | 100k | 低噪声 lr 从 0.1 偏移至 0.05 | 🟢 高 |
| **C6** | **性能崩溃点** | **Summary** | 32k | **noise≥1.0 时 R²<0.54，需换模型** | 🟢 高 |

## 3.2 汇合详情

### 汇合点 C1: Learning Rate 是最关键超参数 (32k)

**单点发现汇总**：

| 来源实验 | Data Size | 单点发现 | 关键数据 |
|---------|-----------|---------|---------|
| [E01](./exp/exp_lightgbm_hyperparam_sweep_20251129.md) | 32k | lr 与 R² 相关系数最高 | +0.491 vs 其他 <0.15 |
| [E02](./exp/exp_lightgbm_noise_sweep_lr_20251204.md) | 32k | 所有 noise level 下 lr=0.1 最优 | 恒定不漂移 |

**汇合结论**：
> **learning_rate 是 LightGBM 最关键的超参数，推荐值为 0.05（noiseless）或 0.1（noisy）**

**设计启示**：
- 调参时优先关注 lr
- 其他参数可用默认值

---

### 汇合点 C3: 噪声场景下的模型选择 (32k)

**单点发现汇总**：

| 来源实验 | Data Size | 单点发现 | 关键数据 |
|---------|-----------|---------|---------|
| [E02](./exp/exp_lightgbm_noise_sweep_lr_20251204.md) | 32k | 低噪声 LightGBM 领先 | noise=0.1: +4% vs Ridge best |
| [E02](./exp/exp_lightgbm_noise_sweep_lr_20251204.md) | 32k | 高噪声 Ridge 反超 | noise=1.0: Ridge +4% |

**汇合结论**：
> **noise < 1.0 时用 LightGBM，noise ≥ 1.0 时用 Ridge（最优 α 调参后）**

**设计启示**：
- 低噪声：LightGBM 的树分裂能捕捉非线性结构
- 高噪声：Ridge L2 正则化比 boosting 更稳健

---

### 汇合点 C4: 数据量 Scaling (32k→100k)

**单点发现汇总**：

| 来源实验 | Noise | 32k R² | 100k R² | 增益 |
|---------|-------|--------|---------|------|
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 0.1 | 0.9456 | 0.9641 | +1.96% |
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 0.5 | 0.6740 | 0.7370 | +9.35% |
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 1.0 | 0.4505 | 0.5310 | +17.9% |

**汇合结论**：
> **噪声越大，数据量增益越显著；低噪声下数据量增益有限**

**设计启示**：
- 如果数据获取成本高，优先在高噪声场景增加数据
- 低噪声场景（noise≤0.1），32k 已接近饱和

---

### 汇合点 C5: 大数据量下 lr 偏移 (100k)

**单点发现汇总**：

| 来源实验 | Data Size | Noise | 32k 最优 lr | 100k 最优 lr |
|---------|-----------|-------|------------|-------------|
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 100k | 0.1 | 0.1 | **0.05** |
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 100k | 0.5 | 0.1 | 0.1 |
| [E03](./exp/exp_lightgbm_100k_noise_sweep_20251205.md) | 100k | 1.0 | 0.1 | 0.1 |

**汇合结论**：
> **100k 数据在低噪声（noise≤0.1）下，最优 lr 从 0.1 偏移至 0.05**

**设计启示**：
- 大数据量 + 低噪声：需要更保守的 lr 避免过拟合
- 大数据量 + 高噪声：lr 仍保持 0.1

---

# 4. 🧭 战略导航

## 4.1 方向状态总览

```
┌───────────────────────────────────────────────────────────────────────────┐
│                           研究方向状态图 (2025-12-07)                       │
├───────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│   🟢 高信心方向（多实验支持）                                               │
│   ├── lr=0.05-0.1 最优 ← E01,E02,E03                                      │
│   ├── num_leaves=31 最佳 ← E01                                            │
│   └── 32k tree 上限 n≈1000 ← E02, Summary                                 │
│                                                                           │
│   🟡 待验证方向                                                            │
│   ├── 100k tree 上限 (n=5000?) ← MVP-2.2 🔴 P0                            │
│   ├── 1M data scaling ← MVP-3.1 🔴 P0                                     │
│   ├── σ=0.4 noise 补齐 ← MVP-1.2 🔴 P0                                    │
│   └── 与 NN 对比 ← MVP-4.1 🟢 P2                                          │
│                                                                           │
│   🔴 风险方向（有反例）                                                     │
│   └── 100k 高噪声增益 ← E03 发现 n=500 可能不够                             │
│                                                                           │
│   ⚫ 已关闭方向                                                            │
│   ├── ~~高噪声用小 lr~~ ← E02 否定                                         │
│   └── ~~高噪声 LightGBM 更强~~ ← E02 否定                                  │
│                                                                           │
└───────────────────────────────────────────────────────────────────────────┘
```

## 4.2 高信心方向（🟢 多实验支持）

| 方向 | 支持证据 | 下一步行动 | 优先级 |
|------|---------|-----------|--------|
| lr=0.05-0.1 默认推荐 | E01, E02, E03 | 无需进一步验证 | - |
| num_leaves=31 默认值 | E01 | 无需进一步验证 | - |
| 32k tree 上限 n≈1000 | E02, Summary | 无需进一步验证 | - |

## 4.3 待验证方向（🟡 假设未验证）

| 方向 | 依赖假设 | 需要实验 | 预计收益 | 优先级 |
|------|---------|---------|---------|--------|
| **100k tree 上限** | H3.3 | MVP-2.2 (n=5000+ES) | 确认 100k 真正上限 | 🔴 P0 |
| **1M data scaling** | H4.1 | MVP-3.1 | 画出完整 scaling 曲线 | 🔴 P0 |
| **σ=0.4 补齐** | - | MVP-1.2 | 完成 noise grid | 🔴 P0 |
| 与 NN 对比 | H5.1 | MVP-4.1 | 确定 NN 竞争力 | 🟢 P2 |

## 4.4 风险方向（🔴 有反例）

| 方向 | 反例证据 | 风险说明 | 缓解措施 |
|------|---------|---------|---------|
| 100k 高噪声增益 | E03: σ=0.5/1.0 时 100k < 32k | 可能被 n=500 卡住 | MVP-2.2 验证 n=5000 |

## 4.5 已关闭方向（⚫ 已否定）

| 方向 | 否定证据 | 关闭原因 | 教训 |
|------|---------|---------|------|
| ~~高噪声用小 lr~~ | E02 | 实验证明 lr=0.1 仍最优 | Boosting 动力学不受噪声影响 |
| ~~高噪声 LightGBM 更强~~ | E02 | Ridge best 反超 4% | L2 正则化对噪声更鲁棒 |

---

# 5. 📐 设计原则库

## 5.1 已确认原则

| # | 原则名称 | 具体建议 | Data Size | 证据来源 | 适用范围 |
|---|---------|---------|-----------|---------|---------|
| P1 | **默认 lr=0.05** | noiseless 场景使用 lr=0.05 | 32k/100k | E01, E03 | noise≤0.1 |
| P2 | **噪声下 lr=0.1** | noisy 场景使用 lr=0.1 | 32k/100k | E02, E03 | noise>0.1 |
| P3 | **num_leaves=31** | 不要超过 128 | 32k | E01 | 所有场景 |
| P4 | **n_estimators** | 32k: n=1000, 100k: n=500 | 32k/100k | E01, E03 | 配合 early stopping |
| P5 | **高噪声用 Ridge** | noise≥1.0 时 Ridge best 更优 | 32k | E02 | noise≥1.0 |
| P6 | **大数据量增益** | 100k 在高噪声下增益更大 | 100k | E03 | noise≥0.5 优先 |

## 5.2 推荐配置代码

```python
# ═══════════════════════════════════════════════════════════════
# 32k 数据配置 (E01, E02 验证)
# ═══════════════════════════════════════════════════════════════

# ⭐ 32k 默认配置（noiseless 或 low noise）
LIGHTGBM_32K_DEFAULT = {
    'n_estimators': 1000,
    'num_leaves': 31,
    'max_depth': 7,
    'learning_rate': 0.05,  # noiseless
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': 'mae',
    'verbose': -1,
    'n_jobs': -1,
    'random_state': 42
}
# 预期 (32k, noise=0): R² ≈ 0.9976, MAE ≈ 0.0099, Time ≈ 37s

# ═══════════════════════════════════════════════════════════════
# 100k 数据配置 (E03 验证)
# ═══════════════════════════════════════════════════════════════

# ⭐ 100k 默认配置（noise≤0.1）
LIGHTGBM_100K_LOW_NOISE = {
    'n_estimators': 500,
    'num_leaves': 31,
    'max_depth': 7,
    'learning_rate': 0.05,  # ⚠️ 100k 低噪声下 lr 偏移至 0.05
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': 'mae',
    'verbose': -1,
    'n_jobs': -1,
    'random_state': 42
}
# 预期 (100k, noise=0.1): R² ≈ 0.9641

# ⚡ 100k 高噪声配置（noise≥0.5）
LIGHTGBM_100K_HIGH_NOISE = {
    'n_estimators': 500,
    'num_leaves': 31,
    'max_depth': 7,
    'learning_rate': 0.1,  # 高噪声保持 0.1
    'verbose': -1,
    'n_jobs': -1
}
# 预期 (100k, noise=0.5): R² ≈ 0.7370
# 预期 (100k, noise=1.0): R² ≈ 0.5310
```

## 5.3 关键数字速查

### 📊 Noiseless 场景

| 指标 | R² | Data Size | 最优配置 | 来源 |
|------|-----|-----------|---------|------|
| 最优 R² | 0.9982 | 32k | n=2000, leaves=31, depth=7, lr=0.05 | E01 |
| 性价比最优 | 0.9976 | 32k | n=1000, leaves=31, lr=0.05 (37s) | E01 |

### 📊 Noisy 场景对比（核心表格）

| Noise | 32k R² (n=1000) | 100k R² | 增益 | 100k 最优配置 | 来源 |
|-------|-----------------|---------|------|--------------|------|
| 0.0 | **0.9981** | - | - | - | Summary |
| 0.1 | **0.9616** | 0.9641 | **+0.26%** | lr=0.05, n=500, leaves=31 | Summary, E03 |
| 0.2 | **0.9045** | - | - | - | Summary |
| 0.5 | **0.7393** | 0.7370 | **-0.31%** | lr=0.10, n=500, leaves=31 | Summary, E03 |
| 1.0 | **0.5361** | 0.5310 | **-0.95%** | lr=0.10, n=500, leaves=31 | Summary, E03 |

> **结论**: 32k n=1000 配置已接近最优；100k 增益主要来自低噪声场景

### 📊 32k 性能退化规律 (Summary)

```
σ=0.0  → R²=0.998 (baseline)
σ=0.1  → R²=0.962 (-3.6%)
σ=0.2  → R²=0.905 (-9.3%)
σ=0.5  → R²=0.739 (-26%)
σ=1.0  → R²=0.536 (-46%) ← 性能崩溃点
σ=2.0  → R²=0.268 (-73%)
```

### 📊 关键发现

| 指标 | 值 | Data Size | 说明 | 来源 |
|------|-----|-----------|------|------|
| lr 与 R² 相关系数 | +0.491 | 32k | lr 是最敏感超参数 | E01 |
| 32k→100k 平均增益 | +4.2% | - | 低噪声增益小，高噪声增益大 | E03 |
| 最优 lr 偏移 | 0.1→0.05 | 100k | 仅在 noise≤0.1 时发生 | E03 |

---

# 6. 📎 附录

## 6.1 术语表

| 术语 | 定义 | 备注 |
|------|------|------|
| learning_rate | Boosting 收缩因子 | 控制每棵树的贡献权重 |
| num_leaves | 每棵树最大叶子数 | 控制单棵树复杂度 |
| n_estimators | 树的总数量 | 影响训练时间 |
| max_depth | 树的最大深度 | -1 表示无限制 |

## 6.2 变更日志

| 日期 | 变更内容 | 影响章节 |
|------|---------|---------|
| 2025-11-30 | 创建 Hub（从 main.md 重构） | 全部 |
| 2025-12-04 | 添加噪声实验洞见 | §2, §3, §5 |
| 2025-12-05 | 更新 100k 实验状态 | §1.2 |
| 2025-12-05 | **重构：添加 Data Size 列，区分 32k vs 100k 实验** | §3.1, §5.1, §5.2, §5.3 |
| 2025-12-05 | 添加 Summary 综合实验报告 | §相关文件 |
| **2025-12-07** | **Gap Analysis: 添加 Q4 Tree 上限问题；更新战略导航** | §1.2, §2.2, §4 |
| 2025-12-07 | 识别 100k n=500 卡瓶颈风险 | §4.4 |

---

*最后更新: 2025-12-07*

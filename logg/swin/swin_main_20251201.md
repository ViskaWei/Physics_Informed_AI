# 📘 Swin-1D 架构实验主框架

---
> **主题名称：** Swin-1D for Stellar Parameter Estimation  
> **作者：** Viska Wei  
> **创建日期：** 2025-12-01  
> **最后更新：** 2025-12-01  
> **状态：** 🔄 进行中

---

# 📑 目录

- [0. 📋 问题重述与意义澄清](#0--问题重述与意义澄清)
  - [0.1 核心研究问题](#01-核心研究问题)
  - [0.2 多维度分析](#02-多维度分析)
  - [0.3 实验定位](#03-实验定位)
- [1. 🎯 目标](#1--目标)
  - [1.1 背景与动机](#11-背景与动机)
  - [1.2 核心假设](#12-核心假设)
  - [1.3 验证问题](#13-验证问题)
  - [1.4 结论摘要](#14-结论摘要实验后填写)
- [2. 🗺️ MVP 实验路线图](#2-️-mvp-实验路线图)
- [3. 📋 子实验追踪](#3--子实验追踪)
- [4. 🔗 跨实验分析](#4--跨实验分析)
- [5. 📝 总结与展望](#5--总结与展望)
- [6. 📎 附录](#6--附录)

---

# 0. 📋 问题重述与意义澄清

> **本章是整个实验系列的"灵魂"**：在动手做任何实验之前，先厘清核心问题、分析其多维度意义、明确实验的价值与局限。

## 0.1 核心研究问题

> **为什么当前 NN 跑不过 LightGBM？Swin-1D 能否通过 hierarchical local attention 突破这一瓶颈？**

据此推断：

> **在什么条件下（数据量、架构设计、训练策略），基于 Attention 的架构能够超越 tree-based 模型？**

### 0.1.1 为什么 NN 目前跑不过 LightGBM？（问题诊断）

**这件事本身一点都不丢人，反而很正常**。当前设定本质上是「高维 tabular + 比较有限的数据量」：

| 特点 | 对 NN 的影响 | 对 LightGBM 的影响 |
|------|-------------|-------------------|
| 4096 维输入，各维度含义清晰 | 过参数化，易过拟合 | 自动做非线性切分、特征交互 |
| 相关性很强（PCA 提示主信息 ~100 维） | 需要学习降维 | 对 feature scaling 不敏感 |
| 数据量 32k | 对 NN 偏小 | 绰绰有余 |
| 非均匀 feature 重要性 | 需要 attention/selection | 自动 top-K 重要像素 |

**NN 同时踩了几脚刹车**：
1. **架构偏"通用"**：没有明确 encode 已知的物理结构（Balmer wings、Ca II、global feature tower…）
2. **数据量对 NN 不算大**：3.2w 对 LightGBM 已绰绰有余，对 NN 是瓶颈
3. **噪声/loss/正则未对齐最优状态**：还没完全对齐"线性部分 + 小非线性残差"的最优状态

**结论**：不是 NN "不行"，而是 NN 没有利用已知的"信息结构"，还要在 LightGBM 最舒服的 regime 硬碰。

### 0.1.2 统计学层面

用统计学语言重新表述问题：

把 $\log g$ 看成随机变量 $Y$，把光谱看成高维输入 $X \in \mathbb{R}^{4096}$。

- **LightGBM**：自动做 $X \to \text{splits} \to Y$，对高维 tabular 有天然优势
- **NN**：需要学习 $f_\theta: X \to Y$，在数据有限时容易过拟合

$$
\text{NN 的挑战} = \underbrace{\text{过参数化}}_{\text{参数 >> 样本}} + \underbrace{\text{无物理先验}}_{\text{架构通用}} + \underbrace{\text{数据有限}}_{\text{32k}}
$$

### 0.1.3 物理/领域层面

从光谱学角度理解为什么 Swin-1D 可能有效：

- **Swin 的 hierarchical 结构**：低层看局部 line profile，高层看更大尺度的 pattern
- **窗口注意力 + shift**：相当于"可学习的局部 kernel"，比固定 kernel 的 CNN 更灵活
- **对应物理结构**：
  - 局部感受野 → 单条谱线的 profile（如 Balmer wings）
  - 层级聚合 → 谱线组之间的关系（Ca II K/H 与 Mg b triplet）

### 0.1.4 意义与启示

| 结果情况 | 含义 | 对设计的启示 |
|---------|------|-------------|
| Swin-1D > LightGBM (32k) | Attention 归纳偏置有效 | 优先投入 Attention 架构开发 |
| Swin-1D ≈ LightGBM (32k) | 数据量是主要瓶颈 | 先扩大数据集再优化架构 |
| Swin-1D > LightGBM (100k+) | 大数据场景 NN 有优势 | 合成数据 + Swin 是可行路线 |
| Swin-1D 无法收敛 | 实现/超参数问题 | 先在 noise=0 做 sanity check |

---

## 0.2 多维度分析

### 0.2.1 信息论角度

- **信息瓶颈**：PCA + Ridge 已提示 $\log g$ 的主要信息可压到 ~100 维
- **Swin 的优势**：通过 hierarchical attention 自适应发现这 ~100 维的最优组合
- **与 CNN 的区别**：CNN 用固定 kernel，Swin 用可学习的 attention weights

### 0.2.2 什么时候"大数据 + 大模型"路线值得走？

**适合走大模型的前提**（需同时满足）：

| 条件 | 当前状态 | 是否满足 |
|------|---------|---------|
| 能廉价生成更多"物理自洽"的模拟数据 | ✅ 可从 3.2w → 30w → 300w 合成谱 | ✅ |
| 有明确假设：大模型在哪个维度更强 | 学习 local line shape + deep nonlinear interactions | ✅ |
| 接受前几轮实验可能不超车 LightGBM | 第一波 Swin 实验是"验证可行性" | ⚠️ 需心理准备 |

**关键洞察**：Swin 实验应该是 **"信息上界分析 → 结构化小模型 → 大模型验证"** 的第三步，而不是"前面都不行 → 换个更大的锤子乱砸"。

### 0.2.3 模型设计角度

| 场景 | Swin-1D 策略 | 备选策略 |
|------|-------------|---------|
| 32k 数据，noise=0 | Tiny Swin (1-5M params)，sanity check | 先修复 CNN 问题 |
| 32k 数据，noise=1.0 | Tiny Swin + regularization | Residual MLP baseline |
| 100k+ 数据 | Small/Base Swin | LightGBM 对比实验 |
| 需要 interpretability | 可视化 attention weights | TopK Ridge 权重分析 |

---

## 0.3 实验定位

### 价值

1. **验证 Attention 归纳偏置**：Swin 的 hierarchical local attention 是否适合光谱这种"局部结构重要"的数据
2. **探索数据规模效应**：NN 能否在大数据场景超越 LightGBM
3. **为后续架构奠基**：如果 Swin-1D 有效，可扩展到多任务（$T_{\text{eff}}$, $[\text{M/H}]$）和 physics-informed loss

### 局限

1. **计算成本**：Swin 比 MLP/LightGBM 更耗资源
2. **调参复杂度**：窗口大小、层数、patch size 等超参数更多
3. **可解释性挑战**：Attention 权重的物理含义需要额外分析

**定位声明**：
> **本实验系列是架构验证实验，目的是探索 Swin-1D 在光谱参数估计任务上的可行性和优势边界，而非追求 SOTA。结论应理解为"Swin 是否值得深入投入"的判断依据。**

---

# 1. 🎯 目标

## 1.1 背景与动机

**研究背景**：
- 当前 MLP 在 32k 数据上达到 $R^2 \approx 0.50$，弱于 LightGBM ($R^2 \approx 0.54$)
- CNN 实验全面失败（$R^2 \approx 0$），需要新的局部结构建模方案
- PCA 分析提示信息主要在 ~100 维，但具体分布未知

**核心目标**（勾选适用项）：
- [x] 指导神经网络架构设计
- [x] 理解光谱中 $\log g$ 的信息结构
- [ ] 分离线性 / 非线性成分
- [ ] 评估噪声、特征选择对天文参数估计的影响
- [x] 量化 Attention 机制对光谱建模的信息增益
- [ ] 其他

**一句话概括**：
> 通过 Swin-1D 架构，验证 hierarchical local attention 能否突破 MLP/CNN 在 $\log g$ 回归上的性能瓶颈。

---

## 1.2 核心假设

### 主假设

> **Swin-1D 的分层窗口注意力机制能够有效捕获光谱的多尺度物理结构（局部谱线 → 谱线组），在足够数据量下超越 LightGBM。**

**如果假设成立，意味着**：
- Attention 是光谱建模的有效归纳偏置
- 扩大合成数据规模是有价值的投入
- 后续可扩展到多任务和物理约束

**如果假设不成立，则需要**：
- 重新评估"信息结构"假设（可能 $\log g$ 信息更 global）
- 考虑 hybrid 架构（如 Attention + Tree）
- 或接受 LightGBM 是当前最佳方案

### 子假设

| 编号 | 子假设 | 对应 MVP | 状态 |
|------|--------|---------|------|
| H1.1 | Swin-1D 在 noise=0 下能达到 $R^2 > 0.99$ | MVP-0.1 | ⏳ 待验证 |
| H1.2 | patch_size=8 (~64 pixels) 是合理的局部感受野 | MVP-1.0 | ⏳ 待验证 |
| H1.3 | 100k 数据下 Swin-1D 能超越 LightGBM | MVP-2.0 | ⏳ 待验证 |
| H1.4 | Attention weights 能对应到物理重要波段 | MVP-3.0 | ⏳ 待验证 |

---

## 1.3 验证问题

| # | 问题 | 验证目标 | 预期结果 | 实际结果 |
|---|------|---------|---------|---------|
| Q1 | Swin-1D 在 noise=0 下能否达到 $R^2 > 0.99$？ | Sanity check | $R^2 > 0.99$ | ⏳ 待填写 |
| Q2 | 最优 patch_size 是多少？ | 局部感受野 | 4 或 8 | ⏳ 待填写 |
| Q3 | Swin-1D (32k) vs LightGBM (32k) 差距多大？ | 小数据对比 | $R^2$ 差距 < 5% | ⏳ 待填写 |
| Q4 | 数据量从 32k → 100k，Swin $R^2$ 提升多少？ | 数据规模效应 | 提升 > 10% | ⏳ 待填写 |
| Q5 | Swin-1D (100k) 能否超越 LightGBM？ | 大数据对比 | Swin > LightGBM | ⏳ 待填写 |
| Q6 | Attention 权重是否聚焦在物理重要波段？ | 可解释性 | 对应 Balmer/Ca II | ⏳ 待填写 |

---

## 1.4 结论摘要（实验后填写）

### 1.4.1 已验证结论

| 结论 | 说明 | 来源 |
|------|------|------|
| - | - | - |

### 1.4.2 设计启示（已确认）

| 设计原则 | 具体建议 | 证据来源 |
|---------|---------|----------|
| - | - | - |

### 1.4.3 关键数字速查

| 指标 | 值 | 备注 |
|------|-----|------|
| - | - | - |

> **一句话总结**：⏳ 实验完成后填写

---

# 2. 🗺️ MVP 实验路线图

## 2.1 整体规划

### 实验分组

| Phase | 目的 | 包含 MVP | 状态 |
|-------|------|---------|------|
| **Phase 0: Sanity Check** | 确认 Swin-1D 实现正确 | MVP-0.x | ⏳ |
| **Phase 1: 架构探索** | patch_size, window_size, depth | MVP-1.x | ⏳ |
| **Phase 2: 数据规模** | 32k → 100k → 300k | MVP-2.x | ⏳ |
| **Phase 3: 可解释性** | Attention 权重分析 | MVP-3.x | ⏳ |

### 依赖关系图

```
┌─────────────────────────────────────────────────────────────┐
│                   Swin-1D MVP 实验依赖图                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   [MVP-0.0: Sanity Check (noise=0)]                         │
│         │                                                   │
│         ▼                                                   │
│   [MVP-0.1: Noise Robustness]                               │
│         │                                                   │
│         ├──────────────┬──────────────┐                     │
│         ▼              ▼              ▼                     │
│   [MVP-1.0]      [MVP-1.1]      [MVP-1.2]                   │
│   patch_size     window_size    depth/width                 │
│         │              │              │                     │
│         └──────────────┼──────────────┘                     │
│                        ▼                                    │
│              [MVP-1.3: 最优架构确定]                          │
│                        │                                    │
│         ┌──────────────┼──────────────┐                     │
│         ▼              ▼              ▼                     │
│   [MVP-2.0]      [MVP-2.1]      [MVP-3.0]                   │
│   100k data      300k data      Attention 分析               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 2.2 MVP 详细设计

### Phase 0: Sanity Check

#### MVP-0.0: Swin-1D Sanity Check (noise=0)

| 项目 | 配置 |
|------|------|
| **目标** | 确认 Swin-1D 实现正确，能在 noise=0 下达到 $R^2 > 0.99$ |
| **数据** | 32k train / 8k test, noise=0 |
| **模型** | Swin-1D Tiny (见下方架构) |
| **验收标准** | $R^2 > 0.99$ |
| **早停条件** | $R^2 < 0.90$ → 实现有问题 |

**排查清单**（若未达验收标准）：
- [ ] 检查 patch embedding 是否正确
- [ ] 检查 positional encoding
- [ ] 检查 window attention 实现
- [ ] 用简单 MLP head 替代检查

---

#### MVP-0.1: Noise Robustness

| 项目 | 配置 |
|------|------|
| **目标** | 测试 Swin-1D 在 noise=1.0 下的性能 |
| **依赖** | MVP-0.0 通过 |
| **数据** | 32k train / 8k test, noise=1.0 |
| **模型** | Swin-1D Tiny |
| **验收标准** | $R^2 > 0.45$（接近 MLP baseline） |
| **异常处理** | 若 $R^2 < 0.30$，检查正则化设置 |

---

### Phase 1: 架构探索

#### MVP-1.0: Patch Size Sweep

| 项目 | 配置 |
|------|------|
| **目标** | 找到最优 patch_size |
| **依赖** | MVP-0.1 |
| **数据** | 32k, noise=1.0 |
| **扫描范围** | patch_size ∈ {4, 8, 16} |
| **验收标准** | 找到使 $R^2$ 最高的 patch_size |

**→ 对假设的影响**：patch_size=8 (~64 pixels) 对应约 ~10Å 感受野，应与典型谱线宽度匹配

---

#### MVP-1.1: Window Size Sweep

| 项目 | 配置 |
|------|------|
| **目标** | 找到最优 window_size |
| **依赖** | MVP-1.0 |
| **数据** | 32k, noise=1.0 |
| **扫描范围** | window_size ∈ {4, 8, 16} (patches) |
| **验收标准** | 找到使 $R^2$ 最高的 window_size |

---

#### MVP-1.2: Depth/Width Sweep

| 项目 | 配置 |
|------|------|
| **目标** | 找到最优深度和宽度 |
| **依赖** | MVP-1.0, MVP-1.1 |
| **扫描范围** | stages ∈ {2, 3, 4}, d_model ∈ {64, 96, 128} |
| **验收标准** | $R^2$ 和模型大小的 Pareto 最优 |

---

### Phase 2: 数据规模

#### MVP-2.0: 100k Data Experiment

| 项目 | 配置 |
|------|------|
| **目标** | 测试 100k 数据下 Swin vs LightGBM |
| **依赖** | MVP-1.3 (最优架构) |
| **数据** | 100k train / 20k test |
| **对比** | LightGBM (100k), MLP (100k) |
| **验收标准** | Swin $R^2$ > LightGBM $R^2$ |

---

### Phase 3: 可解释性

#### MVP-3.0: Attention Weight Analysis

| 项目 | 配置 |
|------|------|
| **目标** | 分析 attention weights 是否对应物理重要波段 |
| **依赖** | MVP-2.0 |
| **方法** | 提取并可视化各层 attention weights |
| **验收标准** | 定性：attention 聚焦在 Balmer/Ca II 波段 |

---

## 2.3 实验配置总览

| MVP | 数据规模 | patch_size | window_size | stages | d_model | 验收标准 |
|-----|---------|-----------|-------------|--------|---------|---------|
| MVP-0.0 | 32k, noise=0 | 8 | 8 | 2 | 64 | $R^2 > 0.99$ |
| MVP-0.1 | 32k, noise=1 | 8 | 8 | 2 | 64 | $R^2 > 0.45$ |
| MVP-1.0 | 32k | {4,8,16} | 8 | 2 | 64 | 找最优 |
| MVP-1.1 | 32k | 最优 | {4,8,16} | 2 | 64 | 找最优 |
| MVP-1.2 | 32k | 最优 | 最优 | {2,3,4} | {64,96,128} | Pareto 最优 |
| MVP-2.0 | 100k | 最优 | 最优 | 最优 | 最优 | > LightGBM |
| MVP-3.0 | 100k | 最优 | 最优 | 最优 | 最优 | 物理对应 |

---

## 2.4 Swin-1D 架构设计（MVP 草图）

### 输入层

- **输入**：长度 4096 的 1D 光谱（先只用 flux，后面再加 error / global features）

### Patch Embedding

```python
# 1D Patch Embedding
Conv1d(in_channels=1, out_channels=d_model, kernel_size=P, stride=P)
# P=8 → 4096/8 = 512 patch tokens
```

- **通道维度**：`d_model=64 or 96`（别一上来就 256/384）

### Swin Stages (1D 版)

| Stage | Blocks | Window Size | 分辨率 | 维度 |
|-------|--------|-------------|--------|------|
| Stage 1 | 2 | 8 patches (~64 pixels) | 512 | 64 |
| Stage 2 | 2 | 8 patches | 256 (merge) | 128 |
| Stage 3 | 2 | 8 patches | 128 (merge) | 256 |

- 每个 stage 内用 `{window attention, shifted window attention}` 交替
- Patch merging: 相邻 2 个 patch concat + linear，长度减半，维度翻倍

### Head

```python
# Global Average Pooling + MLP Head
x = x.mean(dim=1)  # [B, seq_len, dim] → [B, dim]
out = MLP(x)  # → log_g (可加多任务头: Teff, [M/H])
```

### 模型规模控制

| 版本 | 参数量 | 适用场景 |
|------|-------|---------|
| Tiny | 1-2M | 32k sanity check |
| Small | 3-5M | 100k 实验 |
| Base | 10-15M | 300k+ 数据 |

---

## 2.5 训练策略

### 数据策略

1. **先用 32k 做 sanity check**：
   - noise=0 情况下把 $R^2$ 拉到 0.99+
   - 如果连 noiseless 都达不到，说明是实现或结构问题

2. **逐步增加合成数据**：
   - 10w → 30w → 100w
   - 观察 test $R^2$ 曲线随 N 的趋势（log-plot）

### Loss & 正则

| 配置项 | 推荐值 | 说明 |
|--------|-------|------|
| Loss | MSE on log_g | 主 loss |
| Weight decay | 0.01 | 略大的正则 |
| Dropout | 0 (start) | 先保证能 fit |
| Stochastic depth | 0 (start) | 后期可加 |
| Label smoothing | 不需要 | 回归任务 |

### 优化器

| 配置项 | 推荐值 |
|--------|-------|
| Optimizer | AdamW |
| Learning rate | 1e-3 |
| LR scheduler | Cosine decay |
| Batch size | 尽量大 (128-512) |
| Normalization | LayerNorm (不要 BatchNorm) |

---

# 3. 📋 子实验追踪

## 3.1 实验总览

| MVP | 实验名称 | 状态 | 报告链接 | 核心结论（一句话） |
|-----|---------|------|---------|------------------|
| MVP-0.0 | Swin-1D Sanity Check | ⏳ 计划中 | - | - |
| MVP-0.1 | Noise Robustness | ⏳ 计划中 | - | - |
| MVP-1.0 | Patch Size Sweep | ⏳ 计划中 | - | - |
| MVP-1.1 | Window Size Sweep | ⏳ 计划中 | - | - |
| MVP-1.2 | Depth/Width Sweep | ⏳ 计划中 | - | - |
| MVP-2.0 | 100k Data Experiment | ⏳ 计划中 | - | - |
| MVP-3.0 | Attention Analysis | ⏳ 计划中 | - | - |

**状态图例**：
- ⏳ 计划中（Planned）
- 🔄 进行中（In Progress）
- ✅ 已完成（Completed）
- ❌ 已取消（Cancelled）

---

## 3.2 进度看板

```
┌──────────────┬──────────────┬──────────────┬──────────────┐
│   ⏳ 计划中   │   🔄 进行中   │   ✅ 已完成   │   ❌ 已取消   │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ MVP-0.0      │              │              │              │
│ MVP-0.1      │              │              │              │
│ MVP-1.0      │              │              │              │
│ MVP-1.1      │              │              │              │
│ MVP-1.2      │              │              │              │
│ MVP-2.0      │              │              │              │
│ MVP-3.0      │              │              │              │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

---

## 3.3 结论提取记录

| 日期 | 来源实验 | 提取的核心结论 | 更新到 main 的位置 |
|------|---------|---------------|-------------------|
| - | - | - | - |

---

# 4. 🔗 跨实验分析

## 4.1 结果对比矩阵

| 模型 | 32k $R^2$ | 100k $R^2$ | 备注 |
|------|-----------|------------|------|
| Ridge | 0.458 | - | Linear baseline |
| LightGBM | 0.536 | - | Tree baseline |
| MLP (Residual) | 0.498 | 0.551 | NN baseline |
| Swin-1D (Tiny) | ⏳ | ⏳ | Target |

## 4.2 假设验证进度

| 子假设 | 验证 MVP | 结果 | 结论 |
|--------|---------|------|------|
| H1.1: noise=0 达到 $R^2 > 0.99$ | MVP-0.0 | ⏳ 待验证 | - |
| H1.2: patch_size=8 是最优 | MVP-1.0 | ⏳ 待验证 | - |
| H1.3: 100k 下超越 LightGBM | MVP-2.0 | ⏳ 待验证 | - |
| H1.4: Attention 对应物理波段 | MVP-3.0 | ⏳ 待验证 | - |

## 4.3 发现汇总

### 一致性发现（多个实验支持）

| 发现 | 支持实验 | 置信度 |
|------|---------|--------|
| - | - | - |

### 冲突性发现（需要进一步验证）

| 发现 | 实验 A 结论 | 实验 B 结论 | 可能原因 |
|------|-----------|-----------|---------|
| - | - | - | - |

---

# 5. 📝 总结与展望

## 5.1 核心结论

⏳ 实验完成后填写

## 5.2 设计原则提炼

⏳ 实验完成后填写

## 5.3 遗留问题

| 问题 | 优先级 | 建议后续实验 |
|------|--------|-------------|
| 多任务学习 ($\log g$, $T_{\text{eff}}$, $[\text{M/H}]$) | 🟡 中 | 在 Swin 基础上加 multi-head |
| Error channel 融合 | 🟡 中 | [flux, error] 双通道输入 |
| Physics-informed loss | 🟢 低 | 加入物理约束项 |

## 5.4 下一步计划

| 方向 | 具体任务 | 预计时间 | 优先级 |
|------|----------|---------|--------|
| MVP-0.0 | Swin-1D sanity check (noise=0) | 1-2 天 | 🔴 高 |
| MVP-0.1 | Noise robustness test | 1 天 | 🔴 高 |
| MVP-1.x | 架构超参数扫描 | 3-5 天 | 🟡 中 |
| MVP-2.0 | 100k 数据实验 | 2-3 天 | 🟡 中 |

---

# 6. 📎 附录

## 6.1 决策树：不同实验结果的解读与应对

### 情形 A: MVP-0.0 失败 ($R^2 < 0.90$ on noise=0)

**诊断**：Swin-1D 实现有问题

**应对策略**：
1. 检查 patch embedding 维度和 stride
2. 检查 window attention 的 mask 实现
3. 简化为单层 attention 验证
4. 对比 PyTorch 官方 Swin 实现

---

### 情形 B: MVP-0.1 失败 ($R^2 < 0.30$ on noise=1.0)

**诊断**：噪声鲁棒性不足

**应对策略**：
1. 增加 weight decay (0.01 → 0.05)
2. 添加 stochastic depth (0.1)
3. 尝试 residual 策略（学习 Ridge 残差）
4. 检查 LayerNorm 位置（pre-norm vs post-norm）

---

### 情形 C: 100k 数据仍不超越 LightGBM

**诊断**：Swin 归纳偏置可能不适合此任务

**物理/领域含义**：
- $\log g$ 信息可能更 global 而非 local
- 或者 LightGBM 的 feature interaction 更高效

**设计启示**：
1. 考虑 hybrid 架构（Attention + Tree ensemble）
2. 尝试更大的 window_size（更 global）
3. 或接受 LightGBM 是当前最佳方案

---

## 6.2 数值结果汇总表

### 主要指标对比

| MVP | 配置 | $R^2$ | MAE | RMSE | 参数量 | 备注 |
|-----|------|-------|-----|------|--------|------|
| - | - | - | - | - | - | - |

### Baseline 对比

| 模型 | 数据量 | $R^2$ | 来源 |
|------|-------|-------|------|
| Ridge (α=200) | 32k | 0.458 | ridge_main |
| LightGBM | 32k | 0.536 | lightgbm_main |
| MLP (Residual) | 32k | 0.498 | NN_main |
| MLP | 100k | 0.551 | NN_main |

---

## 6.3 相关文件索引

| 类型 | 文件路径 | 说明 |
|------|---------|------|
| 主框架 | `logg/swin/swin_main_20251201.md` | 当前文件 |
| 图表目录 | `logg/swin/img/` | 实验图表 |
| NN baseline | `logg/NN/NN_main_20251130.md` | MLP/CNN baseline |
| LightGBM baseline | `logg/lightgbm/lightgbm_main_20251130.md` | Tree baseline |
| Ridge baseline | `logg/ridge/ridge_main_20251130.md` | Linear baseline |

---

## 6.4 变更日志

| 日期 | 变更内容 | 影响 |
|------|---------|------|
| 2025-12-01 | 创建 Swin-1D 主实验框架 | - |

---

*最后更新: 2025-12-01*  
*核心问题: Swin-1D 能否突破 NN 在 $\log g$ 回归上的性能瓶颈*


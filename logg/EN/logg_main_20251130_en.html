<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Surface Gravity Prediction - Experiment Log</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
            font-size: 10pt;
            line-height: 1.5;
            color: #1e293b;
            background: #ffffff;
            max-width: 190mm;
            margin: 0 auto;
            padding: 10mm;
        }
        
        h1 {
            font-size: 1.6em;
            font-weight: 700;
            color: #2563eb;
            border-bottom: 2px solid #2563eb;
            padding-bottom: 0.3em;
            margin: 1em 0 0.5em 0;
        }
        
        h2 {
            font-size: 1.3em;
            font-weight: 600;
            color: #1e293b;
            border-bottom: 1px solid #e2e8f0;
            padding-bottom: 0.2em;
            margin: 0.9em 0 0.4em 0;
        }
        
        h3 {
            font-size: 1.1em;
            font-weight: 600;
            color: #64748b;
            margin: 0.7em 0 0.3em 0;
        }
        
        h4 {
            font-size: 1em;
            font-weight: 600;
            margin: 0.5em 0 0.2em 0;
        }
        
        p {
            margin: 0.4em 0;
        }
        
        blockquote {
            border-left: 3px solid #2563eb;
            padding: 0.4em 0.8em;
            margin: 0.8em 0;
            background: #f8fafc;
            font-style: italic;
        }
        
        blockquote strong {
            color: #2563eb;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 0.8em 0;
            font-size: 0.85em;
        }
        
        th, td {
            border: 1px solid #e2e8f0;
            padding: 0.3em 0.5em;
            text-align: left;
        }
        
        th {
            background: #f1f5f9;
            font-weight: 600;
        }
        
        tr:nth-child(even) {
            background: #fafafa;
        }
        
        code {
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.85em;
            background: #f1f5f9;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        
        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 0.8em;
            border-radius: 4px;
            margin: 0.8em 0;
            font-size: 0.8em;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: inherit;
        }
        
        ul, ol {
            margin: 0.4em 0;
            padding-left: 1.3em;
        }
        
        li {
            margin: 0.15em 0;
        }
        
        a {
            color: #2563eb;
            text-decoration: none;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e2e8f0;
            margin: 1em 0;
        }
        
        strong {
            font-weight: 600;
        }
        
        @media print {
            body {
                font-size: 9pt;
                padding: 5mm;
            }
            
            h1 {
                page-break-before: always;
            }
            
            h1:first-of-type {
                page-break-before: avoid;
            }
            
            table, pre, blockquote {
                page-break-inside: avoid;
            }
        }
    </style>
</head>
<body>
<h1 id="surface-gravity-log-g-prediction-master-experiment-log-as-of-2025-12-01">Surface Gravity ($\log g$) Prediction: Master Experiment Log (as of 2025-12-01)</h1>

<ul>
<li>Directory: <code>logg/</code></li>
<li>Last Updated: 2025-12-01</li>
<li>Style Reference: <code>/home/swei20/VIT/docs</code></li>
<li>Description: This document serves as the <strong>top-level index for all $\log g$ prediction experiments</strong>, consolidating core conclusions from each subdirectory.</li>
</ul>

<hr />

<h1 id="quick-reference-neural-network-design-guidelines">âš¡ Quick Reference: Neural Network Design Guidelines</h1>

<blockquote>
  <p><strong>Executive Summary</strong>: The $\log g$-flux mapping is inherently linear ($R^2=0.999$ @ noise=0), with information distributed across ~100-dimensional subspace; optimal NN strategy employs <strong>small-kernel CNN (k=9, $R^2=0.657$)</strong> or <strong>Residual MLP + Linear Shortcut</strong>; data volume is the critical leverage factor (32kâ†’100k: +10.6%).</p>
</blockquote>

<h3 id="recommended-nn-architectures-priority-ordered">Recommended NN Architectures (Priority-Ordered)</h3>

<table>
<thead>
<tr>
  <th style="text-align:center;">Priority</th>
  <th>Approach</th>
  <th>Configuration</th>
  <th>Expected $R^2$</th>
  <th>Key Points</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center;">ğŸ¥‡</td>
  <td><strong>Small-Kernel CNN</strong></td>
  <td>k=9, 2L, lr=3e-3, AdaptiveAvgPool</td>
  <td><strong>0.657</strong></td>
  <td>Small receptive field + global pooling</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥ˆ</td>
  <td><strong>Residual MLP</strong></td>
  <td>[256,64], learns Ridge residuals</td>
  <td><strong>0.498</strong></td>
  <td>Linear shortcut is critical</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥‰</td>
  <td><strong>Latent Probe</strong></td>
  <td>enc_pre_latent + seg_mean_K8</td>
  <td><strong>0.55</strong></td>
  <td>Extracted from Denoiser</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ†•</td>
  <td><strong>Top-K Window CNN</strong></td>
  <td>K=256/512, W=17, Residual on Ridge</td>
  <td>target â‰¥0.70</td>
  <td><a href="gta/exp_topk_window_cnn_transformer_20251201.md">MVP-Local-1 in progress</a></td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ†•</td>
  <td><strong>Global Feature Tower</strong></td>
  <td>158-dim (PCA+Ridge+TopK+Latent)</td>
  <td>target â‰¥0.50</td>
  <td><a href="gta/exp_global_feature_tower_mlp_20251201.md">MVP-Global-1 in progress</a></td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ†•</td>
  <td><strong>Swin-1D</strong></td>
  <td>Tiny (1-2M), patch=8, window=8</td>
  <td>surpass LGBM @100k</td>
  <td><a href="swin/swin_main_20251201.md">swin_main planned</a></td>
</tr>
</tbody>
</table>

<h3 id="critical-design-principles">Critical Design Principles</h3>

<table>
<thead>
<tr>
  <th>Principle</th>
  <th>Specific Recommendation</th>
  <th>Evidence Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Linear Shortcut</strong></td>
  <td>$\hat{y} = w^\top x + g_\theta(x)$</td>
  <td>Ridge $R^2=0.999$ @ noise=0</td>
</tr>
<tr>
  <td><strong>Small kernel superior to large kernel</strong></td>
  <td>k âˆˆ {7, 9}, avoid k &gt; 15</td>
  <td>CNN: k=9 (0.66) &gt;&gt; k=63 (0.02)</td>
</tr>
<tr>
  <td><strong>Data volume prioritized</strong></td>
  <td>100k &gt;&gt; 32k (+10.6%)</td>
  <td>Exceeds all architectural improvements</td>
</tr>
<tr>
  <td><strong>Preserve wavelength locality</strong></td>
  <td>Segmented pooling / TopK+window</td>
  <td>Latent: seg_mean +77.6%</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="table-of-contents">Table of Contents</h1>

<ol start="0">
<li><a href="#0-core-research-questions">Core Research Questions</a></li>
<li><a href="#1-project-overview">Project Overview</a></li>
<li><a href="#2-subdirectory-index">Subdirectory Index</a></li>
<li><a href="#3-global-core-conclusions">Global Core Conclusions</a></li>
<li><a href="#4-model-performance-leaderboard">Model Performance Leaderboard</a></li>
<li><a href="#5-key-design-principles">Key Design Principles</a></li>
<li><a href="#6-neural-network-best-practices">Neural Network Best Practices</a></li>
<li><a href="#7-future-research-directions">Future Research Directions</a></li>
<li><a href="#8-appendix-quick-navigation">Appendix: Quick Navigation</a></li>
</ol>

<hr />

<h1 id="0-core-research-questions">0. Core Research Questions</h1>

<h2 id="01-primary-objective">0.1 Primary Objective</h2>

<blockquote>
  <p><strong>Predict stellar surface gravity $\log g$ from synthetic stellar spectra (4096-dimensional flux), providing systematic experimental support for Physics-Informed Neural Network architecture design.</strong></p>
</blockquote>

<h2 id="02-hierarchical-research-questions">0.2 Hierarchical Research Questions</h2>

<h3 id="information-structure-layer-addressed-by-ridge-pca-noise">Information Structure Layer (Addressed by ridge/, pca/, noise/)</h3>

<table>
<thead>
<tr>
  <th>Question</th>
  <th>Status</th>
  <th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Is the $\log g$-flux mapping inherently linear?</td>
  <td>âœ… Answered</td>
  <td><strong>Yes</strong>, $R^2=0.999$ @ noise=0</td>
</tr>
<tr>
  <td>What is the effective dimensionality of $\log g$ information?</td>
  <td>âœ… Answered</td>
  <td><strong>~100-200 dimensions</strong> (PCA experiments)</td>
</tr>
<tr>
  <td>Is information distribution sparse or dispersed?</td>
  <td>âœ… Answered</td>
  <td><strong>Sparse</strong>, 24% of pixels match full spectrum</td>
</tr>
</tbody>
</table>

<h3 id="model-capability-layer-addressed-by-lightgbm-nn-cnn">Model Capability Layer (Addressed by lightgbm/, NN/, cnn/)</h3>

<table>
<thead>
<tr>
  <th>Question</th>
  <th>Status</th>
  <th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
  <td>What is the ceiling for linear models?</td>
  <td>âœ… Answered</td>
  <td>Ridge $R^2=0.458$ @ noise=1.0</td>
</tr>
<tr>
  <td>How much improvement can nonlinear models provide?</td>
  <td>âœ… Answered</td>
  <td>LightGBM +17%, MLP +8.7%</td>
</tr>
<tr>
  <td>Are CNNs suitable for spectroscopic tasks?</td>
  <td>âœ… <strong>Resolved</strong></td>
  <td><strong>Small-kernel CNN (k=9) achieves $R^2=0.657$</strong></td>
</tr>
<tr>
  <td>What is the optimal kernel size?</td>
  <td>âœ… Answered</td>
  <td><strong>k=7-9 optimal</strong>, large kernels degrade performance</td>
</tr>
<tr>
  <td>Is the receptive field hypothesis valid?</td>
  <td>âœ… Answered</td>
  <td><strong>No</strong>, RFâ†‘ â†’ performanceâ†“ (RF=25 optimal)</td>
</tr>
</tbody>
</table>

<h3 id="architecture-design-layer-addressed-by-gta-distill">Architecture Design Layer (Addressed by gta/, distill/)</h3>

<table>
<thead>
<tr>
  <th>Question</th>
  <th>Status</th>
  <th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
  <td>What capacity does Global Tower require?</td>
  <td>ğŸ“‹ In progress</td>
  <td>Requires ~100-150 dim input</td>
</tr>
<tr>
  <td>Can metadata predict $\log g$?</td>
  <td>âœ… Answered</td>
  <td><strong>No</strong>, $R^2 \approx 0$</td>
</tr>
<tr>
  <td>Is the representation learning approach viable?</td>
  <td>âœ… <strong>Validated</strong></td>
  <td><strong>Yes</strong>, optimized extraction achieves $R^2=0.55$ (+150%)</td>
</tr>
<tr>
  <td>Impact of pooling strategy?</td>
  <td>âœ… Answered</td>
  <td>Segmented pooling &gt;&gt; global mean (+77.6%)</td>
</tr>
</tbody>
</table>

<h2 id="03-experiment-dependency-graph">0.3 Experiment Dependency Graph</h2>

<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Phase 1: Information Structure               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  ridge/  â”‚    â”‚   pca/   â”‚    â”‚  noise/  â”‚               â”‚
â”‚  â”‚  Linear  â”‚    â”‚ Effective â”‚   â”‚  Sparse  â”‚               â”‚
â”‚  â”‚  Ceiling â”‚    â”‚   Dims    â”‚   â”‚  Distrib â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                       â–¼                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Phase 2: Model Capability                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ lightgbm/  â”‚         â”‚   NN/    â”‚                        â”‚
â”‚  â”‚  Nonlinear â”‚         â”‚ MLP/CNN  â”‚                        â”‚
â”‚  â”‚  Baseline  â”‚         â”‚          â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                  â–¼                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Phase 3: Architecture Design                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   gta/   â”‚    â”‚  distill/  â”‚    â”‚  train/  â”‚             â”‚
â”‚  â”‚  Global  â”‚    â”‚ Represent. â”‚    â”‚ Training â”‚             â”‚
â”‚  â”‚  Tower   â”‚    â”‚  Learning  â”‚    â”‚ Strategy â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

<hr />

<h1 id="1-project-overview">1. Project Overview</h1>

<h2 id="11-research-objectives">1.1 Research Objectives</h2>

<blockquote>
  <p><strong>Predict stellar surface gravity $\log g$ from synthetic stellar spectra (4096-dimensional flux)</strong></p>
</blockquote>

<p>Core scientific questions:
- How is $\log g$ information distributed in spectral space? (Sparse vs. dispersed)
- What performance ceiling can linear models achieve? How much improvement do nonlinear models provide?
- How does noise affect predictive performance? How to design noise-robust architectures?
- Can neural networks surpass LightGBM?</p>

<h2 id="12-data-summary">1.2 Data Summary</h2>

<table>
<thead>
<tr>
  <th>Configuration</th>
  <th>Value</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Training samples</td>
  <td>32,000 / 100,000</td>
</tr>
<tr>
  <td>Feature dimensionality</td>
  <td>4,096 (spectral flux)</td>
</tr>
<tr>
  <td>Target parameter</td>
  <td>$\log g$ (surface gravity)</td>
</tr>
<tr>
  <td>Noise levels</td>
  <td>$\sigma \in {0, 0.1, 0.5, 1.0, 2.0}$</td>
</tr>
<tr>
  <td>Data source</td>
  <td>BOSZ synthetic spectral library</td>
</tr>
</tbody>
</table>

<h2 id="13-experiment-scale-statistics">1.3 Experiment Scale Statistics</h2>

<table>
<thead>
<tr>
  <th>Subdirectory</th>
  <th># Experiments</th>
  <th>Primary Model</th>
  <th>Status</th>
  <th>Core Finding</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>ridge/</code></td>
  <td>52+</td>
  <td>Ridge Regression</td>
  <td>âœ… Complete</td>
  <td>Mapping is inherently linear</td>
</tr>
<tr>
  <td><code>pca/</code></td>
  <td>13+</td>
  <td>PCA + Ridge</td>
  <td>âœ… Complete</td>
  <td>Effective dimensionality ~100</td>
</tr>
<tr>
  <td><code>lightgbm/</code></td>
  <td>180</td>
  <td>LightGBM</td>
  <td>âœ… Complete</td>
  <td>32k SOTA $R^2=0.536$</td>
</tr>
<tr>
  <td><code>noise/</code></td>
  <td>104+</td>
  <td>TopK + LGBM/Ridge</td>
  <td>âœ… Complete</td>
  <td>24% pixels sufficient</td>
</tr>
<tr>
  <td><code>NN/</code></td>
  <td>53</td>
  <td>MLP</td>
  <td>âœ… Complete</td>
  <td>Residual +8.7% vs Ridge</td>
</tr>
<tr>
  <td><code>cnn/</code></td>
  <td>28+</td>
  <td>1D CNN</td>
  <td>âœ… Complete</td>
  <td><strong>k=9 achieves $R^2=0.657$</strong></td>
</tr>
<tr>
  <td><code>gta/</code></td>
  <td>4</td>
  <td>Global Tower design</td>
  <td>ğŸ”„ Dual-tower MVP in progress</td>
  <td>Metadata cannot predict; Top-K Window + Global Feature Tower</td>
</tr>
<tr>
  <td><code>distill/</code></td>
  <td>4</td>
  <td>Representation learning</td>
  <td>âœ… Complete</td>
  <td>Optimized extraction $R^2=0.55$</td>
</tr>
<tr>
  <td><code>train/</code></td>
  <td>1</td>
  <td>Training strategy</td>
  <td>âœ… Complete</td>
  <td>val_size configured by noise</td>
</tr>
<tr>
  <td><code>swin/</code></td>
  <td>0</td>
  <td>Swin-1D</td>
  <td>ğŸ”„ New MVP framework</td>
  <td>Validate hierarchical attention</td>
</tr>
<tr>
  <td><strong>Total</strong></td>
  <td><strong>430+</strong></td>
  <td>-</td>
  <td>-</td>
  <td>-</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="2-subdirectory-index">2. Subdirectory Index</h1>

<table>
<thead>
<tr>
  <th>Directory</th>
  <th>Topic</th>
  <th>Core Finding</th>
  <th>Main File</th>
</tr>
</thead>
<tbody>
<tr>
  <td><a href="ridge/">ridge/</a></td>
  <td><strong>Linear Baseline</strong></td>
  <td>Mapping inherently linear, $R^2=0.999$ @ noise=0</td>
  <td><a href="ridge/ridge_main_20251130.md">ridge_main</a></td>
</tr>
<tr>
  <td><a href="pca/">pca/</a></td>
  <td><strong>Dimensionality Reduction</strong></td>
  <td>Requires kâ‰¥100 PCs for $R^2â‰¥0.99$</td>
  <td><a href="pca/pca_main_20251130.md">pca_main</a></td>
</tr>
<tr>
  <td><a href="lightgbm/">lightgbm/</a></td>
  <td><strong>Tree Model Baseline</strong></td>
  <td>$R^2=0.9982$ @ noise=0, learning rate most critical</td>
  <td><a href="lightgbm/lightgbm_main_20251130.md">lightgbm_main</a></td>
</tr>
<tr>
  <td><a href="noise/">noise/</a></td>
  <td><strong>Noise &amp; Feature Selection</strong></td>
  <td>K=1000 (24%) matches full spectrum</td>
  <td><a href="noise/noise_main_20251130.md">noise_main</a></td>
</tr>
<tr>
  <td><a href="NN/">NN/</a></td>
  <td><strong>MLP Neural Networks</strong></td>
  <td>Residual MLP exceeds Ridge by +8.7%</td>
  <td><a href="NN/NN_main_20251130.md">NN_main</a></td>
</tr>
<tr>
  <td><a href="cnn/">cnn/</a></td>
  <td><strong>CNN Neural Networks</strong></td>
  <td><strong>Small kernel (k=9) achieves $R^2=0.657$</strong> â­</td>
  <td><a href="cnn/cnn_main_20251201.md">cnn_main</a></td>
</tr>
<tr>
  <td><a href="gta/">gta/</a></td>
  <td><strong>Global Tower Architecture</strong></td>
  <td>Metadata cannot predict; <strong>Dual-tower MVP in progress</strong></td>
  <td><a href="gta/gta_main_20251130.md">gta_main</a></td>
</tr>
<tr>
  <td><a href="distill/">distill/</a></td>
  <td><strong>Representation Learning</strong></td>
  <td>Optimized extraction achieves $R^2=0.55$ (+150%)</td>
  <td><a href="distill/distill_main_20251130.md">distill_main</a></td>
</tr>
<tr>
  <td><a href="train/">train/</a></td>
  <td><strong>Training Strategy</strong></td>
  <td>Optimal val_size depends on noise</td>
  <td><a href="train/train_main_20251130.md">train_main</a></td>
</tr>
<tr>
  <td><a href="swin/">swin/</a></td>
  <td><strong>Swin-1D Architecture</strong></td>
  <td>ğŸ†• Validate hierarchical attention</td>
  <td><a href="swin/swin_main_20251201.md">swin_main</a></td>
</tr>
</tbody>
</table>

<hr />

<h1 id="3-global-core-conclusions">3. Global Core Conclusions</h1>

<h2 id="31-information-structure">3.1 Information Structure</h2>

<table>
<thead>
<tr>
  <th>Finding</th>
  <th>Evidence</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Mapping is inherently linear</strong></td>
  <td>Ridge @ noise=0 achieves $R^2=0.999$</td>
  <td>ridge/</td>
</tr>
<tr>
  <td><strong>Information is high-dimensional and dispersed</strong></td>
  <td>Requires kâ‰¥100 PCs for $R^2â‰¥0.99$</td>
  <td>pca/</td>
</tr>
<tr>
  <td><strong>Information distribution is sparse</strong></td>
  <td>K=1000 (24%) matches full spectrum</td>
  <td>noise/</td>
</tr>
<tr>
  <td><strong>Low-variance directions are important</strong></td>
  <td>$\log g$ information resides in PC 20-200</td>
  <td>pca/</td>
</tr>
</tbody>
</table>

<h2 id="32-model-capability">3.2 Model Capability</h2>

<table>
<thead>
<tr>
  <th>Finding</th>
  <th>Evidence</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Small-kernel CNN is optimal</strong></td>
  <td>k=9 achieves $R^2=0.657$ @ noise=0.1</td>
  <td>cnn/ â­</td>
</tr>
<tr>
  <td><strong>Large kernels perform worse</strong></td>
  <td>k=9 (0.66) &gt;&gt; k=63 (0.02)</td>
  <td>cnn/</td>
</tr>
<tr>
  <td><strong>Receptive field hypothesis refuted</strong></td>
  <td>RF increase â†’ performance degradation</td>
  <td>cnn/</td>
</tr>
<tr>
  <td><strong>LightGBM is 32k SOTA</strong></td>
  <td>$R^2=0.536$ @ noise=1.0</td>
  <td>lightgbm/</td>
</tr>
<tr>
  <td><strong>MLP exceeds Ridge</strong></td>
  <td>$R^2$: 0.498 vs 0.458 (+8.7%)</td>
  <td>NN/</td>
</tr>
<tr>
  <td><strong>Latent representations contain information</strong></td>
  <td>Optimized extraction achieves $R^2=0.55$</td>
  <td>distill/</td>
</tr>
<tr>
  <td><strong>Data volume is critical</strong></td>
  <td>32kâ†’100k: $R^2$ +10.6%</td>
  <td>NN/</td>
</tr>
</tbody>
</table>

<h2 id="33-noise-impact">3.3 Noise Impact</h2>

<table>
<thead>
<tr>
  <th>Finding</th>
  <th>Evidence</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Optimal Î± scales with noise</strong></td>
  <td>0.001 (N=0) â†’ 1000 (N=2.0)</td>
  <td>ridge/</td>
</tr>
<tr>
  <td><strong>Noise training is 20Ã— TopK</strong></td>
  <td>train_noise 0â†’1: $\Delta R^2 \approx 0.49$</td>
  <td>noise/</td>
</tr>
<tr>
  <td><strong>Feature stability at high noise</strong></td>
  <td>noiseâ‰¥0.5 correlation &gt;0.95</td>
  <td>ridge/</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="4-model-performance-leaderboard">4. Model Performance Leaderboard</h1>

<h2 id="41-noise0-noise-free">4.1 noise=0 (Noise-Free)</h2>

<table>
<thead>
<tr>
  <th style="text-align:center;">Rank</th>
  <th>Model</th>
  <th style="text-align:center;">$R^2$</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center;">ğŸ¥‡</td>
  <td>LightGBM</td>
  <td style="text-align:center;"><strong>0.998</strong></td>
  <td>lightgbm/</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥ˆ</td>
  <td>Ridge (Î±=0.001)</td>
  <td style="text-align:center;"><strong>0.999</strong></td>
  <td>ridge/</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥‰</td>
  <td>PCA(500) + Ridge</td>
  <td style="text-align:center;"><strong>0.9995</strong></td>
  <td>pca/</td>
</tr>
</tbody>
</table>

<h2 id="42-noise01-low-noise-optimal-nn-test-scenario">4.2 noise=0.1 (Low Noise) â­ <strong>Optimal NN Test Scenario</strong></h2>

<table>
<thead>
<tr>
  <th style="text-align:center;">Rank</th>
  <th>Model</th>
  <th style="text-align:center;">$R^2$</th>
  <th>Parameters</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center;">ğŸ¥‡</td>
  <td>LightGBM</td>
  <td style="text-align:center;"><strong>0.962</strong></td>
  <td>-</td>
  <td>lightgbm/</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥ˆ</td>
  <td><strong>Small-Kernel CNN (k=9)</strong></td>
  <td style="text-align:center;"><strong>0.657</strong></td>
  <td>27K</td>
  <td>cnn/ â­</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥‰</td>
  <td>Ridge (Î±=1.0)</td>
  <td style="text-align:center;"><strong>0.909</strong></td>
  <td>-</td>
  <td>ridge/</td>
</tr>
<tr>
  <td style="text-align:center;">4</td>
  <td>Latent Probe (seg_mean)</td>
  <td style="text-align:center;">0.55</td>
  <td>-</td>
  <td>distill/</td>
</tr>
</tbody>
</table>

<blockquote>
  <p><strong>Note</strong>: CNN experiments conducted at noise=0.1, slightly different from other model noise settings.</p>
</blockquote>

<h2 id="43-noise10-standard-noise-32k-data">4.3 noise=1.0 (Standard Noise, 32k Data)</h2>

<table>
<thead>
<tr>
  <th style="text-align:center;">Rank</th>
  <th>Model</th>
  <th style="text-align:center;">$R^2$</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center;">ğŸ¥‡</td>
  <td><strong>MLP (100k)</strong></td>
  <td style="text-align:center;"><strong>0.551</strong></td>
  <td>NN/ (100k data)</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥ˆ</td>
  <td><strong>LightGBM</strong></td>
  <td style="text-align:center;"><strong>0.536</strong></td>
  <td>lightgbm/</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥‰</td>
  <td><strong>Residual MLP (32k)</strong></td>
  <td style="text-align:center;"><strong>0.498</strong></td>
  <td>NN/</td>
</tr>
<tr>
  <td style="text-align:center;">4</td>
  <td>Ridge (Î±=200)</td>
  <td style="text-align:center;">0.458</td>
  <td>ridge/</td>
</tr>
</tbody>
</table>

<h2 id="44-noise20-high-noise">4.4 noise=2.0 (High Noise)</h2>

<table>
<thead>
<tr>
  <th style="text-align:center;">Rank</th>
  <th>Model</th>
  <th style="text-align:center;">$R^2$</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td style="text-align:center;">ğŸ¥‡</td>
  <td>LightGBM</td>
  <td style="text-align:center;"><strong>0.268</strong></td>
  <td>MASTER_CONCLUSIONS</td>
</tr>
<tr>
  <td style="text-align:center;">ğŸ¥ˆ</td>
  <td>Ridge (Î±=1000)</td>
  <td style="text-align:center;"><strong>0.221</strong></td>
  <td>ridge/</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="5-key-design-principles">5. Key Design Principles</h1>

<h2 id="51-architecture-design">5.1 Architecture Design</h2>

<table>
<thead>
<tr>
  <th>Principle</th>
  <th>Recommendation</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Small-kernel CNN</strong></td>
  <td>k âˆˆ {7, 9}, avoid k &gt; 15</td>
  <td>cnn/ â­</td>
</tr>
<tr>
  <td><strong>Linear Shortcut</strong></td>
  <td>$\hat{y} = w^\top x + g_\theta(x)$</td>
  <td>ridge/, pca/</td>
</tr>
<tr>
  <td><strong>Residual Strategy</strong></td>
  <td>MLP learns Ridge residuals</td>
  <td>NN/</td>
</tr>
<tr>
  <td><strong>AdaptiveAvgPool</strong></td>
  <td>Use pooling for global aggregation, not large receptive fields</td>
  <td>cnn/</td>
</tr>
<tr>
  <td><strong>Bottleneck â‰¥ 100</strong></td>
  <td>Avoid excessive dimensionality reduction</td>
  <td>pca/</td>
</tr>
<tr>
  <td><strong>2-3 layers sufficient</strong></td>
  <td>Performance collapses at 4+ layers</td>
  <td>NN/</td>
</tr>
</tbody>
</table>

<h2 id="52-regularization-strategy">5.2 Regularization Strategy</h2>

<table>
<thead>
<tr>
  <th>Principle</th>
  <th>Recommendation</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Weight decay scales with noise</strong></td>
  <td>Higher noise â†’ stronger regularization</td>
  <td>ridge/</td>
</tr>
<tr>
  <td><strong>Variance-aware regularization</strong></td>
  <td>PCA whitening</td>
  <td>pca/</td>
</tr>
<tr>
  <td><strong>Dropout â‰¤ 0.3 (Residual)</strong></td>
  <td>Excessive dropout prevents residual learning</td>
  <td>NN/</td>
</tr>
<tr>
  <td><strong>Learning rate by kernel size</strong></td>
  <td>Small kernel: lr=3e-3, large kernel: lr=1e-3</td>
  <td>cnn/</td>
</tr>
</tbody>
</table>

<h2 id="53-input-design">5.3 Input Design</h2>

<table>
<thead>
<tr>
  <th>Principle</th>
  <th>Recommendation</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Dual-channel input</strong></td>
  <td>[flux, Ïƒ]</td>
  <td>ridge/ (error experiments)</td>
</tr>
<tr>
  <td><strong>TopK + window</strong></td>
  <td>Extract neighborhood, not single points</td>
  <td>noise/</td>
</tr>
<tr>
  <td><strong>Spectral data required</strong></td>
  <td>Metadata cannot predict $\log g$</td>
  <td>gta/</td>
</tr>
<tr>
  <td><strong>Preserve wavelength locality</strong></td>
  <td>Segmented pooling &gt;&gt; global mean</td>
  <td>distill/</td>
</tr>
</tbody>
</table>

<h2 id="54-training-strategy">5.4 Training Strategy</h2>

<table>
<thead>
<tr>
  <th>Principle</th>
  <th>Recommendation</th>
  <th>Source</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Noise training prioritized</strong></td>
  <td>train_noise â‰ˆ 1.0-1.2 Ã— test_noise</td>
  <td>noise/</td>
</tr>
<tr>
  <td><strong>val_size configured by noise</strong></td>
  <td>noise=0: use 128, noise=1: use 512</td>
  <td>train/</td>
</tr>
<tr>
  <td><strong>Data volume prioritized</strong></td>
  <td>Prioritize acquiring more data</td>
  <td>NN/</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="6-neural-network-best-practices">6. Neural Network Best Practices</h1>

<blockquote>
  <p><strong>This section consolidates NN design best practices derived from all experiments for quick reference.</strong></p>
</blockquote>

<h2 id="61-recommended-architecture-configurations">6.1 Recommended Architecture Configurations</h2>

<h3 id="option-a-small-kernel-cnn-recommended-for-initial-experiments">Option A: Small-Kernel CNN (Recommended for Initial Experiments)</h3>

<pre><code># Optimal configuration: RÂ² = 0.657 @ noise=0.1
Architecture: 2-Layer CNN
  Conv1D: in=1, out=32, kernel=9, padding=same
  BatchNorm + ReLU
  Conv1D: in=32, out=64, kernel=9, padding=same
  BatchNorm + ReLU
  AdaptiveAvgPool1D(1)
  Linear(64, 1)

Hyperparameters:
  lr: 3e-3  # Critical: small kernel requires high lr
  weight_decay: 0
  batch_size: 2048
  epochs: 50
  early_stopping: patience=20

Parameters: ~27K
Receptive Field: RF=25
</code></pre>

<h3 id="option-b-residual-mlp-classic-stable-approach">Option B: Residual MLP (Classic Stable Approach)</h3>

<pre><code># Optimal configuration: RÂ² = 0.498 @ noise=1.0, 32k data
Architecture: 2-Layer MLP + Ridge Residual
  Input: 4096 (full spectral flux)
  Linear(4096, 256) + ReLU + Dropout(0.3)
  Linear(256, 64) + ReLU + Dropout(0.3)
  Linear(64, 1)

Prediction: y_pred = Ridge_pred + MLP_output  # Linear Shortcut

Hyperparameters:
  lr: 0.001
  weight_decay: 0  # No wd for Residual mode
  dropout: 0.3
  batch_size: 2048
  epochs: 100

Parameters: ~1.1M
</code></pre>

<h3 id="option-c-large-data-mlp-for-100k-data">Option C: Large-Data MLP (For 100k+ Data)</h3>

<pre><code># Optimal configuration: RÂ² = 0.551 @ 100k data
Architecture: 3-Layer Wide MLP
  Linear(4096, 2048) + GELU + Dropout(0.4)
  Linear(2048, 1024) + GELU + Dropout(0.4)
  Linear(1024, 512) + GELU + Dropout(0.4)
  Linear(512, 1)

Hyperparameters:
  lr: 3e-4
  weight_decay: 1e-4
  activation: GELU
  dropout: 0.4

Parameters: ~11M
</code></pre>

<h2 id="62-critical-heuristics">6.2 Critical Heuristics</h2>

<h3 id="recommended-practices">Recommended Practices</h3>

<table>
<thead>
<tr>
  <th>Practice</th>
  <th>Rationale</th>
  <th>Evidence</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Use small kernel (k=7-9)</td>
  <td>Large kernels perform worse</td>
  <td>k=9 (0.66) &gt;&gt; k=63 (0.02)</td>
</tr>
<tr>
  <td>Use Linear Shortcut</td>
  <td>Mapping is inherently linear</td>
  <td>Ridge $R^2=0.999$ @ noise=0</td>
</tr>
<tr>
  <td>Prioritize data volume increase</td>
  <td>Data effect &gt;&gt; architecture effect</td>
  <td>32kâ†’100k: +10.6%</td>
</tr>
<tr>
  <td>Use AdaptiveAvgPool</td>
  <td>More effective than large receptive fields</td>
  <td>CNN experiments validated</td>
</tr>
<tr>
  <td>Preserve wavelength locality</td>
  <td>Critical for $\log g$</td>
  <td>Segmented pooling +77.6%</td>
</tr>
</tbody>
</table>

<h3 id="pitfalls-to-avoid">Pitfalls to Avoid</h3>

<table>
<thead>
<tr>
  <th>Pitfall</th>
  <th>Experimental Evidence</th>
  <th>Correct Approach</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Increasing kernel to k>15</td>
  <td>k=63 is 30Ã— worse than k=9</td>
  <td>Use k=7-9</td>
</tr>
<tr>
  <td>Using single lr for all architectures</td>
  <td>Small kernel with lr=1e-3 yields $R^2=0.01$</td>
  <td>Adjust lr by kernel size</td>
</tr>
<tr>
  <td>Deepening network to 4+ layers</td>
  <td>4-layer MLP $R^2$ drops from 0.49 to 0.10</td>
  <td>Use 2-3 layers</td>
</tr>
<tr>
  <td>Using TopK point features</td>
  <td>TopK MLP (0.46) &lt; full spectrum (0.49)</td>
  <td>Use TopK + window</td>
</tr>
<tr>
  <td>Strong regularization on Residual</td>
  <td>wd=1e-4 degrades performance</td>
  <td>Use wd=0 for Residual</td>
</tr>
</tbody>
</table>

<h2 id="63-true-relationship-between-receptive-field-and-performance">6.3 True Relationship Between Receptive Field and Performance</h2>

<blockquote>
  <p><strong>Core Finding: Receptive field hypothesis refuted â€” large receptive fields degrade performance</strong></p>
</blockquote>

<table>
<thead>
<tr>
  <th>Kernel</th>
  <th>Receptive Field</th>
  <th>Test $R^2$</th>
  <th>Parameters</th>
  <th>Conclusion</th>
</tr>
</thead>
<tbody>
<tr>
  <td>k=7</td>
  <td>19</td>
  <td>0.603</td>
  <td>23K</td>
  <td>Good</td>
</tr>
<tr>
  <td>k=9</td>
  <td>25</td>
  <td><strong>0.657</strong></td>
  <td>27K</td>
  <td><strong>Optimal</strong></td>
</tr>
<tr>
  <td>k=21</td>
  <td>61</td>
  <td>0.035</td>
  <td>52K</td>
  <td>Significant degradation</td>
</tr>
<tr>
  <td>k=45</td>
  <td>133</td>
  <td>0.008</td>
  <td>102K</td>
  <td>Failure</td>
</tr>
<tr>
  <td>k=63</td>
  <td>187</td>
  <td>-0.002</td>
  <td>140K</td>
  <td>Complete failure</td>
</tr>
</tbody>
</table>

<p><strong>Physical Interpretation</strong>:
- $\log g$ information primarily derives from <strong>local spectral line features</strong> (width ~10-50 pixels)
- Small kernel (k=9) suffices to capture individual line shape
- <strong>Global integration</strong> achieved via AdaptiveAvgPool, no need for large convolutional receptive fields
- Large kernels cause <strong>parameter explosion + overfitting</strong></p>

<hr />

<h1 id="7-future-research-directions">7. Future Research Directions</h1>

<h2 id="71-high-priority-p0">7.1 High Priority (P0)</h2>

<table>
<thead>
<tr>
  <th>Direction</th>
  <th>Task</th>
  <th>Expected Benefit</th>
  <th>Status</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>MVP-Local-1</strong></td>
  <td>Top-K Window + CNN/Transformer</td>
  <td>Surpass full-spectrum CNN (0.657) @ noise=0.1</td>
  <td>In progress</td>
</tr>
<tr>
  <td><strong>MVP-Global-1</strong></td>
  <td>Global Feature Tower + MLP</td>
  <td>Achieve RÂ²â‰¥0.50 @ noise=1.0</td>
  <td>In progress</td>
</tr>
<tr>
  <td><strong>CNN noise=1.0 test</strong></td>
  <td>Test optimal CNN config at noise=1.0</td>
  <td>Fair comparison with LightGBM/MLP</td>
  <td>Pending</td>
</tr>
<tr>
  <td><strong>100k fair comparison</strong></td>
  <td>Retrain Ridge/LightGBM with 100k data</td>
  <td>Fair comparison with NN</td>
  <td>Pending</td>
</tr>
<tr>
  <td><strong>Dilated CNN + lr=3e-3</strong></td>
  <td>Retest dilated CNN with high learning rate</td>
  <td>Validate dilation effectiveness</td>
  <td>Pending</td>
</tr>
<tr>
  <td><strong>Teacher-Student distillation</strong></td>
  <td>Distill using new Teacher latent (seg_mean)</td>
  <td>Validate distillation viability</td>
  <td>In progress</td>
</tr>
</tbody>
</table>

<h2 id="72-medium-priority-p1">7.2 Medium Priority (P1)</h2>

<table>
<thead>
<tr>
  <th>Direction</th>
  <th>Task</th>
  <th>Expected Benefit</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Swin-1D</strong></td>
  <td>Hierarchical local attention</td>
  <td>Surpass LightGBM with 100k data</td>
</tr>
<tr>
  <td><strong>CNN + Residual</strong></td>
  <td>Small-kernel CNN learns Ridge residuals</td>
  <td>Potential +3-5%</td>
</tr>
<tr>
  <td><strong>TopK + window</strong></td>
  <td>Extract TopK neighborhood windows (Â±8)</td>
  <td>Preserve local context</td>
</tr>
<tr>
  <td><strong>Dual-channel CNN</strong></td>
  <td>[flux, error] dual-channel input</td>
  <td>Leverage error information</td>
</tr>
<tr>
  <td><strong>GTA Phase 2-5</strong></td>
  <td>Complete F2-F6 feature family experiments</td>
  <td>Guide Global Tower design</td>
</tr>
</tbody>
</table>

<h2 id="73-low-priority-p2">7.3 Low Priority (P2)</h2>

<table>
<thead>
<tr>
  <th>Direction</th>
  <th>Task</th>
  <th>Expected Benefit</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>CNN + Attention</strong></td>
  <td>CNN local features + Attention global integration</td>
  <td>Hybrid architecture</td>
</tr>
<tr>
  <td><strong>Position Encoding</strong></td>
  <td>Add positional encoding to CNN</td>
  <td>Leverage absolute wavelength position</td>
</tr>
<tr>
  <td><strong>Physics-Informed ViT</strong></td>
  <td>Patch design + physics positional encoding</td>
  <td>Incorporate physical priors</td>
</tr>
</tbody>
</table>

<h2 id="74-hypotheses-to-validate">7.4 Hypotheses to Validate</h2>

<table>
<thead>
<tr>
  <th>Hypothesis</th>
  <th>Validation Experiment</th>
  <th>Current Status</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Top-K Window surpasses full-spectrum CNN</td>
  <td>MVP-Local-1</td>
  <td>In progress</td>
</tr>
<tr>
  <td>158-dim Global Feature achieves RÂ²â‰¥0.50 @ noise=1.0</td>
  <td>MVP-Global-1</td>
  <td>In progress</td>
</tr>
<tr>
  <td>Swin-1D surpasses LightGBM with 100k data</td>
  <td>Swin MVP</td>
  <td>Planned</td>
</tr>
<tr>
  <td>Small-kernel CNN remains superior to MLP at noise=1.0</td>
  <td>CNN @ noise=1.0</td>
  <td>Pending</td>
</tr>
<tr>
  <td>Dilated CNN with high lr improves performance</td>
  <td>Dilated + lr=3e-3</td>
  <td>Pending</td>
</tr>
<tr>
  <td>CNN + Linear Shortcut further improves</td>
  <td>CNN Residual</td>
  <td>Pending</td>
</tr>
<tr>
  <td>Distilled Student surpasses original MLP</td>
  <td>Teacher-Student</td>
  <td>In progress</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="8-appendix-quick-navigation">8. Appendix: Quick Navigation</h1>

<h2 id="81-by-model-type">8.1 By Model Type</h2>

<table>
<thead>
<tr>
  <th>Model</th>
  <th>Primary Directory</th>
  <th>Core File</th>
  <th>Best $R^2$</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>CNN (Small-Kernel)</strong></td>
  <td>cnn/</td>
  <td>exp_cnn_dilated_kernel_sweep</td>
  <td><strong>0.657</strong></td>
</tr>
<tr>
  <td><strong>Swin-1D</strong></td>
  <td>swin/</td>
  <td>swin_main</td>
  <td>Planned</td>
</tr>
<tr>
  <td><strong>MLP (Residual)</strong></td>
  <td>NN/</td>
  <td>exp_nn_comprehensive_analysis</td>
  <td>0.498</td>
</tr>
<tr>
  <td><strong>Latent Probe</strong></td>
  <td>distill/</td>
  <td>exp_latent_extraction_logg</td>
  <td>0.55</td>
</tr>
<tr>
  <td><strong>LightGBM</strong></td>
  <td>lightgbm/</td>
  <td>exp_lightgbm_hyperparam_sweep</td>
  <td>0.536</td>
</tr>
<tr>
  <td><strong>Ridge</strong></td>
  <td>ridge/</td>
  <td>exp_ridge_alpha_sweep</td>
  <td>0.458</td>
</tr>
</tbody>
</table>

<h2 id="82-by-research-topic">8.2 By Research Topic</h2>

<table>
<thead>
<tr>
  <th>Topic</th>
  <th>Directory</th>
  <th>Core File</th>
  <th>Core Finding</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>CNN Architecture</strong></td>
  <td>cnn/</td>
  <td>cnn_main</td>
  <td>Small kernel optimal</td>
</tr>
<tr>
  <td><strong>Swin-1D</strong></td>
  <td>swin/</td>
  <td>swin_main</td>
  <td>Hierarchical attention</td>
</tr>
<tr>
  <td><strong>Dimensionality Reduction</strong></td>
  <td>pca/</td>
  <td>pca_main</td>
  <td>~100 effective dims</td>
</tr>
<tr>
  <td><strong>Feature Selection</strong></td>
  <td>noise/</td>
  <td>noise_main</td>
  <td>24% pixels sufficient</td>
</tr>
<tr>
  <td><strong>Representation Learning</strong></td>
  <td>distill/</td>
  <td>distill_main</td>
  <td>Optimized extraction +150%</td>
</tr>
<tr>
  <td><strong>Architecture Design</strong></td>
  <td>gta/</td>
  <td>gta_main</td>
  <td>Spectral data required</td>
</tr>
</tbody>
</table>

<h2 id="83-key-experiment-report-index">8.3 Key Experiment Report Index</h2>

<table>
<thead>
<tr>
  <th>Experiment</th>
  <th>Date</th>
  <th>Core Finding</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Swin-1D Architecture</strong></td>
  <td>2025-12-01</td>
  <td>Hierarchical attention validation</td>
</tr>
<tr>
  <td><strong>Top-K Window CNN/Transformer</strong></td>
  <td>2025-12-01</td>
  <td>MVP-Local-1 experiment plan</td>
</tr>
<tr>
  <td><strong>Global Feature Tower MLP</strong></td>
  <td>2025-12-01</td>
  <td>MVP-Global-1 experiment plan</td>
</tr>
<tr>
  <td>CNN Kernel Sweep</td>
  <td>2025-12-01</td>
  <td>Small kernel (k=9) optimal</td>
</tr>
<tr>
  <td>NN Comprehensive Analysis</td>
  <td>2025-11-30</td>
  <td>Residual MLP optimal</td>
</tr>
<tr>
  <td>Latent Extraction Optimization</td>
  <td>2025-12-01</td>
  <td>Segmented pooling +77.6%</td>
</tr>
<tr>
  <td>PCA Dimension Analysis</td>
  <td>2025-11-28</td>
  <td>Requires 100+ dims</td>
</tr>
<tr>
  <td>Ridge Î± Sweep</td>
  <td>2025-11-27</td>
  <td>$R^2=0.999$ @ noise=0</td>
</tr>
</tbody>
</table>

<hr />

<h1 id="9-summary-selecting-the-optimal-nn-architecture">9. Summary: Selecting the Optimal NN Architecture</h1>

<pre><code>Decision Tree: NN Architecture Selection

Start
  â”‚
  â”œâ”€â”€ Data volume &lt; 32k?
  â”‚     â”‚
  â”‚     â””â”€â”€ Yes â†’ Use Ridge or LightGBM (insufficient data for NN)
  â”‚
  â”œâ”€â”€ Pursuing maximum performance?
  â”‚     â”‚
  â”‚     â”œâ”€â”€ Yes â†’ Small-kernel CNN (k=9, lr=3e-3)
  â”‚     â”‚         Expected RÂ² â‰ˆ 0.65 (noise=0.1)
  â”‚     â”‚
  â”‚     â””â”€â”€ No â†’ Residual MLP + Linear Shortcut
  â”‚               Expected RÂ² â‰ˆ 0.50 (noise=1.0)
  â”‚
  â”œâ”€â”€ Have pretrained Denoiser?
  â”‚     â”‚
  â”‚     â””â”€â”€ Yes â†’ Latent Probe (enc_pre_latent + seg_mean_K8)
  â”‚               Expected RÂ² â‰ˆ 0.55
  â”‚
  â”œâ”€â”€ Require physical priors + local features?
  â”‚     â”‚
  â”‚     â””â”€â”€ Yes â†’ Top-K Window CNN (K=256/512, W=17)
  â”‚               Residual on Ridge, leverage Top-K physical priors
  â”‚               Expected RÂ² â‰¥ 0.70 (noise=0.1) [MVP-Local-1]
  â”‚
  â”œâ”€â”€ Require global + local integration?
  â”‚     â”‚
  â”‚     â””â”€â”€ Yes â†’ Dual-Tower Architecture: Global Tower + Local Tower
  â”‚               Global: 158-dim features (PCA+Ridge+TopK+Latent)
  â”‚               Local: Top-K Window CNN
  â”‚               [MVP-Global-1 + MVP-Local-1 â†’ MVP-Joint-1]
  â”‚
  â”œâ”€â”€ Have 100k+ synthetic data + want to validate large models?
  â”‚     â”‚
  â”‚     â””â”€â”€ Yes â†’ Swin-1D (Tiny: 1-2M params)
  â”‚               patch=8, window=8, 2-3 stages
  â”‚               Validate RÂ² &gt; 0.99 at noise=0 first
  â”‚               Expected to surpass LightGBM with 100k data
  â”‚
  â””â”€â”€ Need higher performance?
        â”‚
        â”œâ”€â”€ Increase data volume to 100k+ (+10.6%)
        â”œâ”€â”€ Try CNN + Residual strategy
        â””â”€â”€ Try Swin-1D / Attention architectures
</code></pre>

<hr />

<p><em>Last Updated: 2025-12-01</em><br />
<em>Total Experiments: 430+ configuration combinations</em><br />
<em>Core Finding: Small-kernel CNN (k=9) achieves $R^2=0.657$ as current NN optimum; data volume is critical leverage; large receptive field hypothesis refuted</em><br />
<em>Current Progress: MVP-Local-1 (Top-K Window CNN) &amp; MVP-Global-1 (Global Feature Tower) &amp; Swin-1D Architecture Experiments</em></p>

<script>
// Simple LaTeX rendering fallback - just clean up the display
document.addEventListener('DOMContentLoaded', function() {
    // Math already shows as $...$ which is readable
});
</script>
</body>
</html>
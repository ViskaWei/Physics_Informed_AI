# ğŸ”¬ Tokenization Ablation Study - è°ƒæŸ¥æŠ¥å‘Š

> **Name:** Tokenization-Ablation-Investigation  
> **ID:** `VIT-20251228-tokenization-ablation-02`  
> **Topic:** `vit` | **Project:** `VIT`  
> **Author:** Viska Wei | **Date:** 2025-12-28 | **Status:** âœ… è°ƒæŸ¥å®Œæˆ
> **Root:** `logg/vit` | **Parent:** `exp_vit_sweep_hlshu8vl_20251227.md`

---

## ğŸ“Š é—®é¢˜æ‘˜è¦

Sweep `hlshu8vl` ä¸­ **SW (Sliding Window) çš„ 15 ä¸ª runs å…¨éƒ¨å¤±è´¥**ï¼Œè€Œ C1D æœ‰ 29% çš„æˆåŠŸç‡ã€‚

| Method | Total | Finished | Failed | Success Rate |
|--------|-------|----------|--------|--------------|
| **C1D** | 79 | 23 | 52 | **29%** |
| **SW** | 15 | 0 | 15 | **0%** |

---

## ğŸ” æ·±åº¦è°ƒæŸ¥ç»“æœ

### 1. SW å¤±è´¥æ¨¡å¼

```
- SW runs åœ¨ epoch 10-13 å°±å¤±è´¥äº†ï¼ˆç›®æ ‡æ˜¯ 50 epochsï¼‰
- mse_loss å§‹ç»ˆ â‰ˆ 1.0ï¼ˆç­‰äºæ ‡å‡†åŒ–æ ‡ç­¾çš„æ–¹å·®ï¼‰
- val_r2 â‰ˆ -0.01ï¼ˆæ¨¡å‹è¾“å‡ºå¸¸æ•°ï¼Œå®Œå…¨æ²¡æœ‰å­¦ä¹ ï¼‰
- å¹³å‡è¿è¡Œæ—¶é—´ 344sï¼ˆC1D æˆåŠŸçš„å¹³å‡ 1328sï¼‰
```

### 2. è¯¦ç»†æµ‹è¯•ç»“æœ

**Tokenizer å•å…ƒæµ‹è¯•ï¼šâœ… æ­£å¸¸**

```python
# æ¢¯åº¦æµæµ‹è¯•
SW (unfold+linear):
  Input grad norm: 246.35
  Input grad max:  4.24

C1D (conv1d):
  Input grad norm: 297.50
  Input grad max:  4.84

# ç»“è®ºï¼šæ¢¯åº¦æµç›¸ä¼¼
```

**ä¸€æ­¥è®­ç»ƒæµ‹è¯• (çœŸå®æ•°æ®)ï¼š**

```
C1D:
  Initial loss: 1.0180 â†’ After 1 step: 0.9772 (ä¸‹é™ 0.04)
  Transformer gradient norm: 5.63

SW:
  Initial loss: 1.3651 â†’ After 1 step: 0.8730 (ä¸‹é™ 0.49!)
  Transformer gradient norm: 11.46 (2x larger!)
```

**å…³é”®å‘ç°**: SW çš„ Transformer æ¢¯åº¦æ˜¯ C1D çš„ 2 å€ï¼ä½†ç¬¬ä¸€æ­¥ loss ä¸‹é™æ›´å¤šã€‚

**å®Œæ•´è®­ç»ƒæµ‹è¯• (256æ ·æœ¬, 50æ­¥)ï¼š**

```
C1D step 1:  loss=1.0448, Final RÂ²=-0.0004
SW step 1:   loss=1.3186, Final RÂ²=-0.0003

# åœ¨å°è§„æ¨¡æµ‹è¯•ä¸­è¡¨ç°ç›¸ä¼¼
```

### 3. æ ¹æœ¬åŸå› åˆ†æ

| åŸå›  | å¯èƒ½æ€§ | è¯æ® |
|------|--------|------|
| SW å®ç° bug | âŒ ä½ | å•å…ƒæµ‹è¯•é€šè¿‡ï¼Œæ¢¯åº¦æµæ­£å¸¸ |
| æ¢¯åº¦ä¸ç¨³å®š | âœ… é«˜ | Transformer æ¢¯åº¦ 2x larger |
| FP16 ç²¾åº¦é—®é¢˜ | âš ï¸ ä¸­ | æœªèƒ½åœ¨ GPU ä¸ŠéªŒè¯ |
| è®­ç»ƒåŠ¨æ€é—®é¢˜ | âœ… é«˜ | ç¬¬ä¸€æ­¥ OKï¼Œé•¿æœŸè®­ç»ƒå¤±è´¥ |

### 4. æŠ€æœ¯å·®å¼‚

**C1D (Conv1d) vs SW (Linear)**

```python
# C1D
x.reshape(-1, 1, 4096) â†’ Conv1d(1, 256, k=16, s=16) â†’ transpose

# SW  
x.unfold(1, 16, 16) â†’ Linear(16, 256)

# å…³é”®å·®å¼‚ï¼š
# 1. Conv1d: æƒé‡ shape (out, in, kernel) = (256, 1, 16)
# 2. Linear: æƒé‡ shape (out, in) = (256, 16)
# 3. ç›¸åŒçš„æœ‰æ•ˆå‚æ•°æ•°é‡ï¼Œä½†æ¢¯åº¦ä¼ æ’­è·¯å¾„ä¸åŒ
```

---

## âœ… ç»“è®º

**SW tokenizer å®ç°æ­£ç¡®**ï¼Œä½†åœ¨é•¿æœŸè®­ç»ƒä¸­å‡ºç°ä¸ç¨³å®šã€‚

**æ ¹æœ¬åŸå› **: Transformer å±‚æ¥æ”¶åˆ°çš„æ¢¯åº¦æ˜¯ C1D çš„ 2 å€ï¼Œå¯¼è‡´ï¼š
1. åˆæœŸå­¦ä¹ è¿‡å¿«
2. åæœŸæŒ¯è¡æˆ–æ¢¯åº¦çˆ†ç‚¸
3. æ¨¡å‹å´©å¡Œåˆ°è¾“å‡ºå¸¸æ•°

---

## ğŸ¯ æ¨è

### æ–¹æ¡ˆ A: ç›´æ¥ä½¿ç”¨ C1D (æ¨è)

```yaml
model:
  proj_fn: C1D  # å·²éªŒè¯å¯ç”¨
```

### æ–¹æ¡ˆ B: å¦‚éœ€ä½¿ç”¨ SW

```yaml
model:
  proj_fn: SW
opt:
  lr: 0.00015  # é™ä½ 2x åŒ¹é…æ¢¯åº¦å·®å¼‚
train:
  precision: 32  # ä½¿ç”¨ FP32
  gradient_clip: 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
```

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

- `src/models/tokenization.py` - æ·»åŠ äº† SW ä¸ç¨³å®šæ€§è­¦å‘Š
- `scripts/test_sw_sweep_config.py` - SW æµ‹è¯•è„šæœ¬
- `scripts/test_c1d_sweep_config.py` - C1D æµ‹è¯•è„šæœ¬

---

*Updated: 2025-12-28*

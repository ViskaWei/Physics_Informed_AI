# ğŸ¤– å®éªŒ Coding Prompt: Ridge-Init ResMLP

> **æ—¥æœŸ:** 2025-12-27 | **ID:** `VIT-20251227-nn-01`  
> **æ¥æº:** `logg/NN/exp/exp_ridge_init_resmlp_20251227.md`

---

## âš ï¸ æ ¸å¿ƒè§„åˆ™

| è§„åˆ™ | è¯´æ˜ |
|------|------|
| **nohup åå°è¿è¡Œ** | æ‰€æœ‰è®­ç»ƒå¿…é¡» `nohup ... &`ï¼Œ>5åˆ†é’Ÿä¸æŒç»­è¿½è¸ª |
| **è·¨ä»“åº“ç”¨ç»ˆç«¯** | å†™å…¥ Physics_Informed_AI ç”¨ `cat/echo/cp`ï¼Œç¦æ­¢ IDE å·¥å…· |
| **å›¾ç‰‡å¿…é¡»å…¥æŠ¥å‘Š** | æ‰€æœ‰å›¾è¡¨å¿…é¡»åœ¨æŠ¥å‘Š Â§3 ä¸­å¼•ç”¨ï¼Œè·¯å¾„ `logg/NN/img/` |
| **è¯­è¨€** | Header è‹±æ–‡ \| æ­£æ–‡ä¸­æ–‡ \| å›¾è¡¨æ–‡å­—è‹±æ–‡ |

---

## ğŸš€ ä»“åº“è·¯ç”±

| Topic | ä»“åº“ | å‰ç¼€ |
|-------|------|------|
| **NN / ResMLP** | `~/VIT` | VIT- |

---

## ğŸ¯ å®éªŒè§„æ ¼

```yaml
experiment_id: "VIT-20251227-nn-01"
experiment_name: "Ridge-Init ResMLP"
repo_path: "~/VIT"

data:
  source: "BOSZ â†’ PFS MR"
  path: "~/VIT/data/mag205_225_lowT_1M" 
  # ä½¿ç”¨ train_200k_0 (32k æ ·æœ¬) åˆå§‹éªŒè¯ï¼ŒæˆåŠŸåæ‰©å±•
  train_size: 32000
  val_size: 10000
  test_size: 10000

noise:
  type: "heteroscedastic"
  sigma: 1.0
  apply_to: "train + val + test"

model:
  type: "RidgeResMLP"
  variants:
    V1_baseline: { strategy: "none", description: "æ— Ridge, çº¯ResMLP" }
    V2_concat: { strategy: "concat", description: "è¾“å…¥concat Ridgeé¢„æµ‹" }
    V3_init: { strategy: "init", description: "ç¬¬ä¸€å±‚Ridgeæƒé‡åˆå§‹åŒ–" }
    V4_residual: { strategy: "residual", description: "å­¦ä¹ Ridgeæ®‹å·®" }
    V5_shortcut: { strategy: "shortcut", description: "è¾“å‡ºå±‚skip Ridge" }
  architecture:
    hidden_dim: 512
    n_blocks: 3  # 1 stem + 3 ResBlocks + 1 head = 5å±‚
    bottleneck_ratio: 0.5
    activation: "gelu"
    norm: "LayerNorm"
    dropout: 0.1

training:
  epochs: 200
  batch_size: 2048
  optimizer: "AdamW"
  lr: 3e-4
  weight_decay: 1e-4
  scheduler: "CosineAnnealingLR"
  warmup_epochs: 10
  seed: 42
  early_stopping: 50
  gradient_clip: 1.0

ridge_pretrain:
  alpha: 200  # 32k æ•°æ®æœ€ä¼˜ alpha
  model_path: "~/VIT/models/ridge/lnreg_l2_a200_n32k_nz1p0.pkl"

plots:
  - { type: "strategy_comparison", save: "ridge_resmlp_strategy_compare.png" }
  - { type: "training_curves", save: "ridge_resmlp_training_curves.png" }
  - { type: "depth_ablation", save: "ridge_resmlp_depth_ablation.png" }
  - { type: "residual_scatter", save: "ridge_resmlp_residual_scatter.png" }
```

---

## ğŸ“‹ æ‰§è¡Œæµç¨‹

### Step 0: å‡†å¤‡ Ridge æ¨¡å‹

```bash
cd ~/VIT && source init.sh

# æ£€æŸ¥æ˜¯å¦å·²æœ‰ Ridge æ¨¡å‹
ls -la models/ridge/*32k*nz1p0*

# å¦‚æœæ²¡æœ‰ï¼Œå…ˆè®­ç»ƒ Ridge (çº¦ 1-2 åˆ†é’Ÿ)
python -c "
from sklearn.linear_model import Ridge
import pickle
import h5py
import numpy as np
import pandas as pd

# Load 32k data
DATA_PATH = '/datascope/subaru/user/swei20/data/bosz50000/z0/mag205_225_lowT_1M/train_200k_0/dataset.h5'
with h5py.File(DATA_PATH, 'r') as f:
    flux = f['dataset/arrays/flux/value'][:32000].astype(np.float32)
    error = f['dataset/arrays/error/value'][:32000].astype(np.float32)
df = pd.read_hdf(DATA_PATH)[:32000]
logg = df['log_g'].values.astype(np.float32)

# Add noise
np.random.seed(42)
noisy_flux = flux + np.random.randn(*flux.shape) * error * 1.0

# Train Ridge
ridge = Ridge(alpha=200)
ridge.fit(noisy_flux, logg)

# Save
data = {'model': ridge, 'alpha': 200, 'noise_level': 1.0}
with open('models/ridge/ridge_a200_n32k_nz1p0.pkl', 'wb') as f:
    pickle.dump(data, f)
print('Ridge model saved!')
print(f'Weights shape: {ridge.coef_.shape}')
"
```

### Step 1: åˆ›å»º ResMLP æ¨¡å‹

åœ¨ `~/VIT/src/nn/models/resmlp.py` åˆ›å»ºæ–°æ¨¡å‹ï¼š

```python
"""
Ridge-Initialized ResMLP for Spectroscopic Regression.

Combines Ridge linear prior with deep residual MLP.
Supports 5 strategies:
  - V1 (none): Pure ResMLP baseline
  - V2 (concat): Concat Ridge prediction to input
  - V3 (init): Initialize first layer with Ridge weights
  - V4 (residual): Learn Ridge residual, add back at output
  - V5 (shortcut): Skip connection from Ridge pred to output
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Literal, Optional
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


@dataclass
class ResMLP_Config:
    """Configuration for Ridge ResMLP."""
    input_dim: int = 4096
    hidden_dim: int = 512
    n_blocks: int = 3
    bottleneck_ratio: float = 0.5
    dropout: float = 0.1
    activation: Literal["gelu", "relu", "silu"] = "gelu"
    
    # Ridge integration strategy
    strategy: Literal["none", "concat", "init", "residual", "shortcut"] = "concat"
    ridge_path: Optional[str] = None
    
    @property
    def bottleneck_dim(self) -> int:
        return int(self.hidden_dim * self.bottleneck_ratio)


class ResBlock(nn.Module):
    """Residual block with bottleneck structure."""
    
    def __init__(self, dim: int, bottleneck_dim: int, dropout: float = 0.1):
        super().__init__()
        self.block = nn.Sequential(
            nn.Linear(dim, bottleneck_dim),
            nn.LayerNorm(bottleneck_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(bottleneck_dim, dim),
            nn.Dropout(dropout),
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.block(x)


class RidgeResMLP(nn.Module):
    """
    Ridge-Initialized ResMLP.
    
    Architecture:
        Input (4096) â†’ Stem â†’ ResBlockÃ—N â†’ Head â†’ Output (1)
    
    With optional Ridge integration via:
        - concat: Input becomes (4097) with Ridge prediction
        - init: First layer uses Ridge weights
        - residual: Output = Ridge_pred + MLP(x)
        - shortcut: Output += Ridge_pred
    """
    
    def __init__(self, config: ResMLP_Config):
        super().__init__()
        self.config = config
        self.strategy = config.strategy
        
        # Load Ridge model if needed
        self.ridge_weights = None
        self.ridge_bias = None
        if config.ridge_path and config.strategy != "none":
            self._load_ridge(config.ridge_path)
        
        # Input dimension (depends on strategy)
        in_dim = config.input_dim
        if config.strategy == "concat":
            in_dim += 1  # Add Ridge prediction
        
        # Stem: project to hidden_dim
        self.stem = nn.Sequential(
            nn.Linear(in_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout),
        )
        
        # Initialize first layer with Ridge weights if strategy == "init"
        if config.strategy == "init" and self.ridge_weights is not None:
            self._init_stem_from_ridge()
        
        # ResBlocks
        self.blocks = nn.ModuleList([
            ResBlock(config.hidden_dim, config.bottleneck_dim, config.dropout)
            for _ in range(config.n_blocks)
        ])
        
        # Head: project to output
        self.head = nn.Linear(config.hidden_dim, 1)
        
        # Optional: learnable weight for shortcut
        if config.strategy == "shortcut":
            self.shortcut_weight = nn.Parameter(torch.tensor(0.5))
    
    def _load_ridge(self, path: str):
        """Load Ridge weights from pickle file."""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        ridge = data['model']
        weights = ridge.coef_.flatten().astype(np.float32)
        bias = float(ridge.intercept_) if np.isscalar(ridge.intercept_) else float(ridge.intercept_[0])
        
        self.register_buffer('ridge_weights', torch.from_numpy(weights))
        self.register_buffer('ridge_bias', torch.tensor([bias]))
        
        print(f"[RidgeResMLP] Loaded Ridge weights from {path}")
        print(f"  Shape: {weights.shape}, Strategy: {self.strategy}")
    
    def _init_stem_from_ridge(self):
        """Initialize stem's first layer with Ridge weights."""
        if self.ridge_weights is None:
            return
        
        with torch.no_grad():
            # Expand Ridge weights to all hidden neurons with scaling
            n_hidden = self.config.hidden_dim
            scale = 1.0 / np.sqrt(n_hidden)
            
            # Each neuron starts with scaled Ridge weights
            for i in range(n_hidden):
                self.stem[0].weight.data[i] = self.ridge_weights * scale
            
            self.stem[0].bias.data.fill_(self.ridge_bias.item() / n_hidden)
        
        print(f"[RidgeResMLP] Initialized stem with Ridge weights (scale={scale:.4f})")
    
    def _get_ridge_pred(self, x: torch.Tensor) -> torch.Tensor:
        """Compute Ridge prediction."""
        if self.ridge_weights is None:
            return torch.zeros(x.size(0), 1, device=x.device)
        return F.linear(x, self.ridge_weights.unsqueeze(0), self.ridge_bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: (batch, input_dim) spectral flux
        
        Returns:
            (batch,) predicted log_g
        """
        # Get Ridge prediction if needed
        ridge_pred = None
        if self.strategy in ["concat", "residual", "shortcut"]:
            ridge_pred = self._get_ridge_pred(x)
        
        # Modify input for concat strategy
        if self.strategy == "concat":
            x = torch.cat([x, ridge_pred], dim=-1)
        
        # Forward through network
        h = self.stem(x)
        for block in self.blocks:
            h = block(h)
        out = self.head(h)
        
        # Apply Ridge integration at output
        if self.strategy == "residual":
            out = ridge_pred + out
        elif self.strategy == "shortcut":
            out = self.shortcut_weight * ridge_pred + (1 - self.shortcut_weight) * out
        
        return out.squeeze(-1)
    
    def get_param_count(self) -> int:
        """Get total trainable parameters."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def __repr__(self) -> str:
        return (
            f"RidgeResMLP(\n"
            f"  strategy={self.strategy},\n"
            f"  hidden_dim={self.config.hidden_dim},\n"
            f"  n_blocks={self.config.n_blocks},\n"
            f"  params={self.get_param_count():,}\n"
            f")"
        )


def create_ridge_resmlp(
    input_dim: int = 4096,
    hidden_dim: int = 512,
    n_blocks: int = 3,
    dropout: float = 0.1,
    strategy: str = "concat",
    ridge_path: Optional[str] = None,
) -> RidgeResMLP:
    """Factory function for RidgeResMLP."""
    config = ResMLP_Config(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        n_blocks=n_blocks,
        dropout=dropout,
        strategy=strategy,
        ridge_path=ridge_path,
    )
    return RidgeResMLP(config)
```

### Step 2: åˆ›å»ºè®­ç»ƒè„šæœ¬

åˆ›å»º `~/VIT/scripts/train_ridge_resmlp.py`ï¼š

```python
#!/usr/bin/env python3
"""
Train Ridge-Initialized ResMLP for log_g prediction.

Usage:
    python scripts/train_ridge_resmlp.py --strategy concat --hidden 512 --blocks 3
    
Strategies:
    - none: Pure ResMLP (baseline)
    - concat: Concat Ridge prediction to input
    - init: Initialize first layer with Ridge weights
    - residual: Learn Ridge residual
    - shortcut: Output skip connection
"""

import os
import sys
import argparse
import time
from pathlib import Path
from datetime import datetime

import h5py
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import r2_score, mean_absolute_error

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from src.nn.models.resmlp import create_ridge_resmlp, ResMLP_Config


# =============================================================================
# Configuration
# =============================================================================

DATA_PATH = Path("/datascope/subaru/user/swei20/data/bosz50000/z0/mag205_225_lowT_1M/train_200k_0/dataset.h5")
RIDGE_PATH = Path(__file__).resolve().parents[1] / "models/ridge/ridge_a200_n32k_nz1p0.pkl"
OUTPUT_DIR = Path(__file__).resolve().parents[1] / "results/ridge_resmlp"
IMG_DIR = Path("/home/swei20/Physics_Informed_AI/logg/NN/img")

TRAIN_SIZE = 32000
VAL_SIZE = 10000
TEST_SIZE = 10000
NOISE_LEVEL = 1.0
SEED = 42


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--strategy", type=str, default="concat",
                        choices=["none", "concat", "init", "residual", "shortcut"])
    parser.add_argument("--hidden", type=int, default=512)
    parser.add_argument("--blocks", type=int, default=3)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--epochs", type=int, default=200)
    parser.add_argument("--batch", type=int, default=2048)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--wd", type=float, default=1e-4)
    parser.add_argument("--gpu", type=int, default=0)
    parser.add_argument("--seed", type=int, default=42)
    return parser.parse_args()


def load_data(path, n_train, n_val, n_test, noise_level, seed):
    """Load and split data with noise injection."""
    print(f"\n[Data] Loading from {path}...")
    
    with h5py.File(path, 'r') as f:
        total = n_train + n_val + n_test
        flux = f['dataset/arrays/flux/value'][:total].astype(np.float32)
        error = f['dataset/arrays/error/value'][:total].astype(np.float32)
    
    df = pd.read_hdf(path)[:total]
    logg = df['log_g'].values.astype(np.float32)
    
    # Add noise
    np.random.seed(seed)
    noisy_flux = flux + np.random.randn(*flux.shape).astype(np.float32) * error * noise_level
    
    # Split
    X_train = noisy_flux[:n_train]
    y_train = logg[:n_train]
    X_val = noisy_flux[n_train:n_train+n_val]
    y_val = logg[n_train:n_train+n_val]
    X_test = noisy_flux[n_train+n_val:]
    y_test = logg[n_train+n_val:]
    
    print(f"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
    return X_train, y_train, X_val, y_val, X_test, y_test


def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        pred = model(X)
        loss = criterion(pred, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        total_loss += loss.item() * len(X)
    return total_loss / len(loader.dataset)


@torch.no_grad()
def evaluate(model, loader, device):
    model.eval()
    preds, targets = [], []
    for X, y in loader:
        X = X.to(device)
        pred = model(X)
        preds.append(pred.cpu().numpy())
        targets.append(y.numpy())
    
    preds = np.concatenate(preds)
    targets = np.concatenate(targets)
    
    r2 = r2_score(targets, preds)
    mae = mean_absolute_error(targets, preds)
    return r2, mae, preds, targets


def main():
    args = parse_args()
    
    # Setup
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    IMG_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"Ridge-ResMLP Training: Strategy={args.strategy}")
    print(f"{'='*60}")
    
    # Load data
    X_train, y_train, X_val, y_val, X_test, y_test = load_data(
        DATA_PATH, TRAIN_SIZE, VAL_SIZE, TEST_SIZE, NOISE_LEVEL, args.seed
    )
    
    # Create dataloaders
    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
    val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))
    test_ds = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))
    
    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False, num_workers=4)
    
    # Create model
    ridge_path = str(RIDGE_PATH) if args.strategy != "none" else None
    model = create_ridge_resmlp(
        input_dim=4096,
        hidden_dim=args.hidden,
        n_blocks=args.blocks,
        dropout=args.dropout,
        strategy=args.strategy,
        ridge_path=ridge_path,
    )
    model = model.to(device)
    print(f"\n{model}")
    
    # Training setup
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # Training loop
    best_val_r2 = -float('inf')
    patience_counter = 0
    history = {'train_loss': [], 'val_r2': [], 'val_mae': []}
    
    print(f"\n[Training] Starting {args.epochs} epochs...")
    start_time = time.time()
    
    for epoch in range(args.epochs):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_r2, val_mae, _, _ = evaluate(model, val_loader, device)
        scheduler.step()
        
        history['train_loss'].append(train_loss)
        history['val_r2'].append(val_r2)
        history['val_mae'].append(val_mae)
        
        if val_r2 > best_val_r2:
            best_val_r2 = val_r2
            patience_counter = 0
            torch.save(model.state_dict(), OUTPUT_DIR / f"best_{args.strategy}.pt")
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"  Epoch {epoch+1:3d}: loss={train_loss:.4f}, val_RÂ²={val_r2:.4f}, val_MAE={val_mae:.4f}")
        
        if patience_counter >= 50:
            print(f"  Early stopping at epoch {epoch+1}")
            break
    
    elapsed = time.time() - start_time
    print(f"\n[Training] Completed in {elapsed:.1f}s")
    
    # Load best model and evaluate on test
    model.load_state_dict(torch.load(OUTPUT_DIR / f"best_{args.strategy}.pt"))
    test_r2, test_mae, test_preds, test_targets = evaluate(model, test_loader, device)
    
    print(f"\n{'='*60}")
    print(f"[RESULTS] Strategy: {args.strategy}")
    print(f"  Test RÂ²:  {test_r2:.4f}")
    print(f"  Test MAE: {test_mae:.4f}")
    print(f"  Best Val RÂ²: {best_val_r2:.4f}")
    print(f"{'='*60}")
    
    # Save results
    results = {
        'strategy': args.strategy,
        'hidden_dim': args.hidden,
        'n_blocks': args.blocks,
        'dropout': args.dropout,
        'test_r2': test_r2,
        'test_mae': test_mae,
        'best_val_r2': best_val_r2,
        'epochs_trained': len(history['train_loss']),
        'params': model.get_param_count(),
        'elapsed_seconds': elapsed,
    }
    
    # Append to CSV
    results_file = OUTPUT_DIR / "results.csv"
    df_results = pd.DataFrame([results])
    if results_file.exists():
        df_existing = pd.read_csv(results_file)
        df_results = pd.concat([df_existing, df_results], ignore_index=True)
    df_results.to_csv(results_file, index=False)
    
    print(f"\n[Output] Results saved to {results_file}")
    
    return results


if __name__ == "__main__":
    main()
```

### Step 3: è¿è¡Œæ‰€æœ‰å˜ä½“

```bash
cd ~/VIT && source init.sh

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs results/ridge_resmlp

# è¿è¡Œ 5 ä¸ªå˜ä½“ï¼ˆæ¯ä¸ªçº¦ 5-10 åˆ†é’Ÿï¼Œæ€»è®¡ ~30 åˆ†é’Ÿï¼‰
EXP_ID="VIT-20251227-nn-01"

# V1: Baseline (æ—  Ridge)
nohup python scripts/train_ridge_resmlp.py --strategy none --hidden 512 --blocks 3 --gpu 0 \
    > logs/${EXP_ID}_v1.log 2>&1 &
echo "V1 PID: $!"

# V2: Ridge-Concat
nohup python scripts/train_ridge_resmlp.py --strategy concat --hidden 512 --blocks 3 --gpu 1 \
    > logs/${EXP_ID}_v2.log 2>&1 &
echo "V2 PID: $!"

# V3: Ridge-Init
nohup python scripts/train_ridge_resmlp.py --strategy init --hidden 512 --blocks 3 --gpu 2 \
    > logs/${EXP_ID}_v3.log 2>&1 &
echo "V3 PID: $!"

# V4: Ridge-Residual
nohup python scripts/train_ridge_resmlp.py --strategy residual --hidden 512 --blocks 3 --gpu 3 \
    > logs/${EXP_ID}_v4.log 2>&1 &
echo "V4 PID: $!"

# V5: Ridge-Shortcut
nohup python scripts/train_ridge_resmlp.py --strategy shortcut --hidden 512 --blocks 3 --gpu 4 \
    > logs/${EXP_ID}_v5.log 2>&1 &
echo "V5 PID: $!"

echo "All variants launched!"
```

**ç¡®è®¤æ­£å¸¸åè¾“å‡º**ï¼š
```
âœ… ä»»åŠ¡å·²å¯åŠ¨ (5 ä¸ªå˜ä½“å¹¶è¡Œ)
ğŸ“‹ æŸ¥çœ‹æ—¥å¿—:
   tail -f ~/VIT/logs/VIT-20251227-nn-01_v1.log  # V1: Baseline
   tail -f ~/VIT/logs/VIT-20251227-nn-01_v2.log  # V2: Concat
   tail -f ~/VIT/logs/VIT-20251227-nn-01_v3.log  # V3: Init
   tail -f ~/VIT/logs/VIT-20251227-nn-01_v4.log  # V4: Residual
   tail -f ~/VIT/logs/VIT-20251227-nn-01_v5.log  # V5: Shortcut
â±ï¸ é¢„è®¡æ¯ä¸ª ~5-10 minï¼Œå®Œæˆåå‘Šè¯‰æˆ‘ç»§ç»­
```

### Step 4: ç”Ÿæˆå›¾è¡¨

åˆ›å»º `~/VIT/scripts/plot_ridge_resmlp.py`ï¼š

```python
#!/usr/bin/env python3
"""Generate plots for Ridge-ResMLP experiments."""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

OUTPUT_DIR = Path(__file__).resolve().parents[1] / "results/ridge_resmlp"
IMG_DIR = Path("/home/swei20/Physics_Informed_AI/logg/NN/img")
IMG_DIR.mkdir(parents=True, exist_ok=True)

# Baselines
BASELINES = {
    'Ridge': 0.458,
    'MLP (2L)': 0.498,
    'LightGBM': 0.536,
}

def plot_strategy_comparison():
    """Fig 1: Bar chart comparing all strategies."""
    df = pd.read_csv(OUTPUT_DIR / "results.csv")
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Plot bars
    strategies = df['strategy'].values
    r2_scores = df['test_r2'].values
    
    x = np.arange(len(strategies))
    bars = ax.bar(x, r2_scores, color=['#2ecc71' if s != 'none' else '#95a5a1' for s in strategies])
    
    # Add baseline lines
    colors = ['#e74c3c', '#3498db', '#9b59b6']
    for (name, val), color in zip(BASELINES.items(), colors):
        ax.axhline(y=val, linestyle='--', color=color, alpha=0.7, label=name)
    
    # Labels
    ax.set_xlabel('Strategy', fontsize=12)
    ax.set_ylabel('Test RÂ²', fontsize=12)
    ax.set_title('Ridge-ResMLP: Strategy Comparison (32k samples, noise=1.0)', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(['V1: Baseline', 'V2: Concat', 'V3: Init', 'V4: Residual', 'V5: Shortcut'])
    ax.legend(loc='lower right')
    ax.set_ylim(0.4, 0.6)
    
    # Add value labels on bars
    for bar, val in zip(bars, r2_scores):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{val:.3f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(IMG_DIR / "ridge_resmlp_strategy_compare.png", dpi=150)
    print(f"Saved: {IMG_DIR / 'ridge_resmlp_strategy_compare.png'}")
    plt.close()


if __name__ == "__main__":
    plot_strategy_comparison()
    print("\nAll plots generated!")
```

è¿è¡Œç»˜å›¾:
```bash
python scripts/plot_ridge_resmlp.py
```

### Step 5: å†™æŠ¥å‘Š

è®­ç»ƒå®Œæˆåï¼Œæ›´æ–°å®éªŒæŠ¥å‘Šï¼š

```bash
# è¯»å–ç»“æœ
cat ~/VIT/results/ridge_resmlp/results.csv

# ç”¨ç»ˆç«¯å‘½ä»¤æ›´æ–°æŠ¥å‘Šçš„æ ¸å¿ƒç»“è®ºéƒ¨åˆ†
# (æ ¹æ®å®é™…ç»“æœå¡«å†™)
```

---

## ğŸ—‚ï¸ å‚è€ƒä»£ç 

| å‚è€ƒè„šæœ¬ | å¯å¤ç”¨ | éœ€ä¿®æ”¹ |
|---------|--------|--------|
| `src/nn/models/mlp.py` | MLPConfig, åˆå§‹åŒ–é€»è¾‘ | æ·»åŠ  ResBlock ç»“æ„ |
| `train_nn.py` | è®­ç»ƒæ¡†æ¶, å‚æ•°è§£æ | ç®€åŒ–ä¸ºå•ç­–ç•¥è®­ç»ƒ |
| `scripts/train_ridge_1m_optimal.py` | æ•°æ®åŠ è½½é€»è¾‘ | ä½¿ç”¨ 32k æ•°æ® |
| `src/nn/trainer.py` | Trainer ç±» | å¯é€‰å¤ç”¨ |

---

## âœ… æ£€æŸ¥æ¸…å•

- [ ] Ridge æ¨¡å‹å‡†å¤‡å®Œæˆ (`models/ridge/ridge_a200_n32k_nz1p0.pkl`)
- [ ] ResMLP æ¨¡å‹ä»£ç  (`src/nn/models/resmlp.py`)
- [ ] è®­ç»ƒè„šæœ¬ (`scripts/train_ridge_resmlp.py`)
- [ ] 5 ä¸ªå˜ä½“è®­ç»ƒå®Œæˆ
- [ ] å›¾è¡¨ç”Ÿæˆ (è‹±æ–‡)
  - [ ] `ridge_resmlp_strategy_compare.png`
- [ ] æ›´æ–°å®éªŒæŠ¥å‘Š `logg/NN/exp/exp_ridge_init_resmlp_20251227.md`

---

## ğŸ”§ æ•…éšœæ’é™¤

| é—®é¢˜ | ä¿®å¤ |
|------|------|
| NaN / Loss çˆ†ç‚¸ | é™ lr åˆ° 1e-4ï¼ŒåŠ  warmup |
| OOM | å‡ batch_size åˆ° 1024 |
| Ridge æ–‡ä»¶ä¸å­˜åœ¨ | è¿è¡Œ Step 0 å…ˆè®­ç»ƒ Ridge |
| æ”¶æ•›æ…¢ | å¢åŠ  blocks åˆ° 4ï¼Œæˆ–å¢å¤§ hidden åˆ° 1024 |
| è¿‡æ‹Ÿåˆ | å¢åŠ  dropout åˆ° 0.2-0.3 |

---

## ğŸ“Š é¢„æœŸç»“æœå‚è€ƒ

| å˜ä½“ | é¢„æœŸ RÂ² | è¯´æ˜ |
|------|---------|------|
| V1: Baseline | ~0.50-0.52 | çº¯ ResMLPï¼Œåº”æ¥è¿‘ MLP baseline |
| V2: Concat | ~0.51-0.54 | æ³¨å…¥ Ridge å…ˆéªŒ |
| V3: Init | ~0.50-0.53 | æƒé‡åˆå§‹åŒ–å¯èƒ½æ”¶æ•›æ›´å¿« |
| V4: Residual | ~0.52-0.55 | å­¦ä¹ æ®‹å·®ï¼Œç†è®ºæœ€ä¼˜ |
| V5: Shortcut | ~0.51-0.54 | ä»‹äº concat å’Œ residual |

**æˆåŠŸæ ‡å‡†**:
- ä»»ä¸€å˜ä½“ RÂ² > 0.536 â†’ è¶…è¶Š LightGBM âœ…
- ä»»ä¸€å˜ä½“ RÂ² > 0.498 â†’ è¶…è¶Š MLP baseline âœ…
- Ridge å˜ä½“ > Baseline å˜ä½“ â†’ Ridge åˆå§‹åŒ–æœ‰æ•ˆ âœ…

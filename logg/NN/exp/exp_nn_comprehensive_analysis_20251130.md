# 📘 Neural Network 完整实验分析报告
> **Name:** TODO | **ID:** `VIT-20251130-nn-01`  
> **Topic:** `nn` | **MVP:** MVP-X.X | **Project:** `VIT`  
> **Author:** Viska Wei | **Date:** 2025-11-30 | **Status:** 🔄
```
💡 实验目的  
决定：影响的决策
```

---


## 🔗 Upstream Links
| Type | Link |
|------|------|
| 🧠 Hub | `logg/nn/nn_hub.md` |
| 🗺️ Roadmap | `logg/nn/nn_roadmap.md` |

---

---

# ⚡ 核心结论速览（供 main 提取）

### 一句话总结

> **在 32k 数据下，Residual MLP 达到 $R^2 = 0.498$，超过 Ridge +8.7% 但距 LightGBM 仍有 7.1% gap；CNN 全面失败（$R^2 \approx 0$）；数据量从 32k→100k 提升 +10.6%，是架构改进的 5-10 倍。**

### 对假设的验证

| 验证问题 | 结果 | 结论 |
|---------|------|------|
| MLP 能否超过 Ridge？ | ✅ 是，+8.7% | MLP 有效 |
| MLP 能否接近 LightGBM？ | ❌ 否，-7.1% | 32k 数据下树模型仍有优势 |
| Residual 策略是否有效？ | ✅ 是，+2.3% | 推荐作为默认策略 |
| CNN 能否有效学习？ | ❌ 否，$R^2 \approx 0$ | 可能是实现问题 |
| 数据量效应多大？ | ✅ 32k→100k: +10.6% | 数据量是关键杠杆 |

### 设计启示（1-2 条）

| 启示 | 具体建议 |
|------|---------|
| **Residual 策略** | 让 MLP 学习 Ridge 预测的残差，而非直接预测 $\log g$ |
| **2-3 层浅网络** | 4 层开始崩溃，2-3 层是最佳深度 |

### 关键数字

| 指标 | 值 |
|------|-----|
| 32k 最佳 MLP $R^2$ | **0.498** (Residual + [256,64]) |
| 100k 最佳 MLP $R^2$ | **0.551** |
| Ridge baseline | **0.458** |
| LightGBM baseline | **0.536** |
| MLP vs Ridge | **+8.7%** |
| 100k vs 32k | **+10.6%** |

---

# 📑 目录

- [1. 🎯 目标](#1--目标)
- [2. 🧪 实验设计](#2--实验设计experiment-design)
- [3. 📊 实验图表](#3--实验图表)
- [4. 💡 关键洞见](#4--关键洞见key-insights)
- [5. 📝 结论](#5--结论conclusion)
- [6. 📎 附录](#6--附录)

---

# 1. 🎯 目标

## 1.1 背景与动机

在前期实验中，我们已经建立了重要的线性和树模型基线：
- **Ridge Regression** 在 noise=1.0 时达到 $R^2 = 0.458$
- **LightGBM** 在 noise=1.0 时达到 $R^2 = 0.536$（32k 数据下的 SOTA）
- **PCA 实验** 表明有效信息维度约 100-200
- **Top-K 实验** 表明信息高度稀疏，约 24% 像素 (K≈1000) 即可接近全谱性能

本实验旨在：
> **穷举式地探索 MLP 架构空间（深度、宽度、激活函数、正则化、残差结构），量化数据量效应（32k vs 100k），并诊断 CNN 在该任务上的失败原因，为后续 Physics-Informed 架构提供明确的设计方向。**

具体目标：
1. **确定 MLP 在 32k 数据下的性能天花板**：能否超过 Ridge？能否接近 LightGBM？
2. **量化 Residual 策略的收益**：学习 Ridge 残差 vs 直接预测 $\log g$
3. **分析 Deep/Wide 的边际收益**：是否存在"过深"、"过宽"的拐点？
4. **诊断 CNN 的失败原因**：是架构不适配还是实现问题？
5. **量化数据量效应**：从 32k 到 100k，性能提升多少？

## 1.2 核心假设

> **在 32k 数据、noise=1.0 的设定下，MLP 通过 Residual 策略（学习 Ridge 预测的残差）可以稳定超过 Ridge baseline，但在不增加数据量的前提下，无法接近 LightGBM 的性能；CNN 由于 $\log g$ 任务需要全局特征整合，预期表现不如 MLP。**

如果假设成立，意味着：
- $\log g$ 信息确实需要**全局特征整合**，局部卷积的归纳偏置无效
- **Residual 策略有效**：NN 学习"线性预测器学不到的部分"是合理的范式
- **数据量是主要瓶颈**：在 32k regime 下，架构改进的边际收益有限
- 后续应优先考虑：增加数据、Attention 机制、物理先验注入

如果假设不成立，则需要：
- 重新审视 CNN 实现（可能是代码 bug 而非架构问题）
- 探索更深/更宽的 MLP 是否能持续提升
- 考虑是否存在 LightGBM 利用的"组合特征"是 MLP 难以学习的

## 1.3 验证问题

| # | 问题 | 验证目标 | 结果 |
|---|------|---------|------|
| Q1 | MLP 能否超过 Ridge ($R^2 = 0.458$)？ | 验证 MLP 的非线性建模能力 | ✅ **是**，最佳 $R^2 = 0.498$ (+8.7%) |
| Q2 | MLP 能否接近 LightGBM ($R^2 = 0.536$)？ | 对标 32k 数据下的 SOTA | ❌ **否**，差距 7.1% (0.498 vs 0.536) |
| Q3 | Residual 策略是否有效？ | 验证"学习 Ridge 残差"的增益 | ✅ **是**，Residual MLP (+0.01~0.02) vs 直接 MLP |
| Q4 | 更深/更宽的 MLP 是否持续提升？ | 确定架构复杂度上限 | ❌ **否**，3 层最佳，4 层开始崩溃；宽度边际收益很小 |
| Q5 | TopK 特征选择对 MLP 有帮助吗？ | 验证稀疏输入的有效性 | ❌ **否**，TopK MLP (0.457) < 全谱 MLP (0.487) |
| Q6 | CNN 能否有效学习？ | 验证卷积对光谱的适用性 | ❌ **否**，所有 CNN $R^2 \approx 0$（实现层面可能有问题） |
| Q7 | 数据量从 32k 到 100k 能提升多少？ | 量化数据 scaling 效应 | ✅ $R^2$: 0.498 → 0.551 (+10.6%) |

## 1.4 结论摘要

### 1.4.1 实验结论

| 结论 | 说明 |
|------|------|
| **MLP 已到 32k 极限** | 最佳 Residual MLP 达到 $R^2 = 0.498$，超过 Ridge +8.7%，但距 LightGBM 仍有 7.1% gap |
| **数据量是主要瓶颈** | 100k 数据 MLP 达到 $R^2 = 0.551$，比 32k 提升 10.6%，超过同期 LightGBM |
| **架构改进边际收益小** | 激活函数、宽度、深度的调整 $\Delta R^2 \leq 0.02$，远小于数据量增益 |
| **CNN 全面失败** | 18 个 CNN 实验 $R^2 \approx 0$，可能是实现问题而非架构不适配 |

### 1.4.2 设计启示

| 设计原则 | 具体建议 |
|---------|---------|
| **Residual 策略** | 让 NN 学习 Ridge 预测的残差，而非直接预测 $\log g$ |
| **2-3 层即可** | 4 层及以上网络训练不稳定，3 层是最佳深度 |
| **数据优先** | 在追求更复杂架构前，优先获取更多训练数据 |
| **修复 CNN** | 在进一步实验前，需做 sanity check 确认 CNN 实现正确 |

> **一句话总结**：在 32k 数据、noise=1.0 下，Residual MLP 已达到 MLP 架构的极限 ($R^2 \approx 0.50$)，稳定超过 Ridge 但距 LightGBM 有 ~0.04 的 gap；真正能显著推 $R^2$ 的是增加数据量到 100k，而不是更深更宽的网络。

---

# 2. 🧪 实验设计（Experiment Design）

## 2.1 数据（Data）

### 2.1.1 数据集配置

| 数据集 | 训练样本数 | 验证样本数 | 测试样本数 | 特征维度 | 使用实验 |
|--------|-----------|-----------|-----------|----------|----------|
| **32k** | 32,000 | 10,000 | 10,000 | 4,096 | 45 个实验 |
| **100k** | 100,000 | 10,000 | 10,000 | 4,096 | 8 个实验 (MLP_Big) |

**⚠️ 重要**：MLP_Big 系列使用 100k 数据，与 Ridge/LightGBM baseline 不可直接比较！

### 2.1.2 标签与噪声

| 配置项 | 值 |
|--------|-----|
| 标签参数 | $\log g$ |
| 噪声水平 | 1.0 (标准噪声) |

**噪声模型**：
$$
\text{noisy\_flux} = \text{flux} + \mathcal{N}(0, 1) \times \text{error} \times \text{noise\_level}
$$

### 2.1.3 Baseline 参考 (32k 数据)

| 模型 | Test $R^2$ | Test MAE | 说明 |
|------|-----------|----------|------|
| **LightGBM** | **0.536** | ~0.16 | 32k 数据下的 SOTA |
| **Ridge ($\alpha=200$)** | **0.458** | 0.173 | 线性 baseline |

## 2.2 使用的特征类型

| 特征类型 | 维度 | 说明 | 使用实验 |
|---------|------|------|----------|
| 全谱 flux | 4096 | 标准化后的光谱流量 | MLP 全系列 |
| TopK flux | 256-1024 | LightGBM 重要性排序的 TopK 像素 | MLP_TopK 系列 |

## 2.3 模型与算法（Model & Algorithm）

### 2.3.1 MLP 架构系列

**基础 MLP**：
$$
\text{Input} (4096) \xrightarrow{\text{Linear}} h_1 \xrightarrow{\text{Act}} \cdots \xrightarrow{\text{Linear}} 1
$$

**Residual MLP**：
$$
\hat{y} = \underbrace{\hat{y}_{\text{Ridge}}}_{\text{Ridge 预测}} + \underbrace{g_\theta(\text{flux})}_{\text{MLP 残差}}
$$

### 2.3.2 实验的架构配置空间

| 系列 | 层数 | 隐藏层配置 | 参数量范围 | 特点 |
|------|------|-----------|-----------|------|
| **Step1/Step2** | 2 | [256,64], [128,32] | 0.5M-1.1M | 基础 baseline |
| **MLP_Residual** | 2 | [256,64], [512,256] | 1.1M-2.2M | 学习 Ridge 残差 |
| **MLP_Deep** | 3-4 | [512,256,128], [256,256,128,64] | 1.5M-2.5M | 深度探索 |
| **MLP_Wide** | 1-2 | [1024], [2048,512] | 4.2M-9.4M | 宽度探索 |
| **MLP_Big** | 2-4 | [2048,1024,512], [4096,1024] | 4.9M-21M | 100k 数据大模型 |
| **MLP_TopK** | 2 | [256,64] | 82K-279K | TopK 特征选择 |

### 2.3.3 CNN 架构

```
Input (4096) → Conv1d(32, k=7) → ReLU → MaxPool(2)
            → Conv1d(64, k=7) → ReLU → MaxPool(2)
            → AdaptiveAvgPool → FC → Output
```

| 变体 | 通道配置 | Kernel | BatchNorm | 参数量 |
|------|---------|--------|-----------|--------|
| Stage1 | [32,64] | 7 | ✗ | 23K |
| Stage2 | [32]-[64,128] | 7-21 | ✓/✗ | 4.6K-171K |

## 2.4 超参数（Hyperparameters）

### 2.4.1 训练配置（固定）

| 参数 | 值 |
|------|-----|
| Optimizer | AdamW |
| Batch size | 2048 |
| Max epochs | 100-150 |
| Early stopping patience | 50 |
| Gradient clip | 1.0 |
| Mixed precision | True |
| Seed | 42 |

### 2.4.2 搜索空间

| 参数 | 搜索范围 | 最佳值 |
|------|----------|--------|
| Learning rate | $\{0.1, 0.01, 0.001, 5\times10^{-4}, 3\times10^{-4}\}$ | **0.001** (小模型), **3e-4** (大模型) |
| Dropout | $\{0.1, 0.3, 0.4, 0.5\}$ | **0.3-0.4** |
| Weight decay | $\{0, 10^{-4}\}$ | **0** (Residual), **1e-4** (大模型) |
| 激活函数 | $\{\text{ReLU}, \text{GELU}, \text{SiLU}\}$ | **GELU** (大模型), **ReLU** (Residual) |

---

# 3. 📊 实验图表

> 实验图表待生成。以下为建议绘制的图表。

### 图 1：32k vs 100k 数据量效应（建议绘制）

**建议内容**：
- X 轴：模型配置（按复杂度排序）
- Y 轴：Test $R^2$
- 两条曲线：32k 数据 vs 100k 数据
- 标注：Ridge/LightGBM baseline 水平线

**预期观察**：
- 100k 数据曲线整体高于 32k ~0.05
- 数据量效应 >> 架构改进效应

### 图 2：MLP 架构深度 vs 宽度热力图（建议绘制）

**建议内容**：
- X 轴：宽度 (隐藏层最大维度)
- Y 轴：深度 (层数)
- 颜色：Test $R^2$

**预期观察**：
- 2-3 层 + 256-512 宽度 颜色最深
- 4 层以上颜色骤降

### 图 3：Residual vs 直接预测对比（建议绘制）

**建议内容**：
- 分组柱状图：相同架构下 Residual vs 直接 MLP
- Y 轴：Test $R^2$

**预期观察**：
- Residual 系列普遍高 0.01-0.02

### 图 4：CNN 诊断图（建议绘制）

**建议内容**：
- 训练曲线：Train Loss vs Epochs
- 对比：MLP（正常下降）vs CNN（几乎不下降）

**预期观察**：
- CNN 的 loss 几乎不变，说明模型根本没在学习

---

# 4. 💡 关键洞见（Key Insights）

## 4.1 宏观层洞见（用于指导 Neural Network 架构设计）

### Insight 1: 在 32k regime，LightGBM 仍是明确的 SOTA

- **现象**：最佳 MLP ($R^2 = 0.498$) vs LightGBM ($R^2 = 0.536$)，差距 7.1%
- **含义**：树模型的特征组合能力在该数据量下仍有优势
- **设计启示**：
  - 在同等数据量下，不应期望简单 MLP 超过 LightGBM
  - 需要引入更强的归纳偏置（Attention、物理先验）或更多数据

### Insight 2: 数据量是目前唯一能显著推 $R^2$ 的因素

- **现象**：
  - 32k Residual MLP: $R^2 = 0.498$
  - 100k MLP_Big: $R^2 = 0.551$
  - 差距 0.053，比所有架构改进大一个数量级
- **含义**：当前处于"数据受限"regime，模型容量已足够
- **设计启示**：
  - 优先获取更多训练数据
  - 在 100k 数据下重新训练 LightGBM 做公平对比

### Insight 3: Residual 策略是有效的架构改进

- **现象**：
  - `MLP_Residual_nz1p0`: $R^2 = 0.4985$ (32k SOTA)
  - 直接 MLP `Step1_lr1en03_wd0`: $R^2 = 0.4874$
  - 增益约 +1.1%
- **含义**：让 NN 只学习"线性学不到的部分"是合理的范式
- **设计启示**：
  - 将 Residual 策略作为默认设计
  - 可尝试更强的残差目标（如 LightGBM 残差）

## 4.2 模型层洞见（用于优化模型）

### Insight 4: 架构空间已被充分探索，边际收益很小

- **现象**：
  - 激活函数：GELU ($R^2 = 0.483$) > ReLU ($R^2 = 0.474$) > SiLU ($R^2 = 0.449$)，差距 ~0.01
  - 宽度：[2048,512] ($R^2 = 0.489$) vs [256,64] ($R^2 = 0.487$)，差距 ~0.002
  - 深度：2 层最佳，3 层略降，4 层崩溃
- **含义**：架构复杂度已不是主要瓶颈
- **设计启示**：
  - 固定使用 2 层 [256,64] 或 [512,256] + GELU + Residual
  - 不必再大规模扫架构超参数

### Insight 5: 3 层是深度上限，4 层开始崩溃

- **现象**：
  - 2 层 `MLP_Wide_2L_2048`: $R^2 = 0.489$
  - 3 层 `MLP_Deep_3L_512_gelu`: $R^2 = 0.470$
  - 4 层 `MLP_Deep_4L_512_gelu`: $R^2 = 0.356$
  - 4 层 ReLU `MLP_Deep_4L_256_relu`: $R^2 = 0.100$
- **含义**：
  - 在 32k 数据上，4 层网络过深，优化难度暴增
  - ReLU 在深网络中更不稳定
- **设计启示**：
  - 如需更深网络，必须配合残差连接、LayerNorm
  - 优先使用 GELU 而非 ReLU

### Insight 6: Residual 需要轻正则化

- **现象**：
  - `MLP_Res_v2_2L_512_gelu` (dropout=0.3, wd=0): $R^2 = 0.490$
  - `MLP_Res_v2_2L_512_reg` (dropout=0.4, wd=1e-4): $R^2 = 0.470$
- **含义**：过强的正则化会让 Residual 学不到东西
- **设计启示**：Residual 模式下，dropout ≤ 0.3，weight decay = 0

## 4.3 实验层细节洞见

### Insight 7: TopK 特征选择对 MLP 是负收益

- **现象**：
  - 全谱 MLP: $R^2 = 0.487$
  - TopK=1024 MLP: $R^2 = 0.457$（-6.2%）
  - TopK=256 MLP: $R^2 = 0.416$（-14.6%）
- **含义**：
  - LightGBM 的 TopK 强调的是"对树有用的分裂点"，不等于对 MLP 最优
  - 单点选择丢失了局部 shape / continuum 信息
- **设计启示**：
  - 不应直接使用 LightGBM 的 TopK 作为 MLP 输入
  - 如需降维，应使用 "TopK + window" 或 Attention 机制

### Insight 8: CNN 全面失败可能是实现问题

- **现象**：
  - 所有 18 个 CNN 实验 $R^2 \in [-0.0005, 0.016]$
  - Train $R^2 \approx 0.01$，MAE 接近 label std
  - 3 层 CNN 出现 NaN
- **含义**：
  - 这不像"架构不适合"，更像实现 bug
  - 正常的函数逼近器训练几十个 epoch，train $R^2$ 至少应该达到 0.2-0.3
- **设计启示**：
  - 先做 sanity check：noise=0 或人造简单 target
  - 检查 Conv1d 输入维度顺序 [B,C,L] vs [B,L,C]
  - 确认 flatten 和 fc 维度匹配

---

# 5. 📝 结论（Conclusion）

## 5.1 核心发现

> **在 32k 数据、noise=1.0 的设定下，纯 MLP baseline 已被"挖干净"：Residual MLP 达到 $R^2 \approx 0.50$，稳定超过 Ridge (+8.7%)，但距离 LightGBM 仍有稳定 ~0.04 的 gap。CNN 全面失败（可能是实现问题）。真正能推 $R^2$ 的是增加数据到 100k，而不是更深更宽的网络。**

- ❌ 原假设：更深/更宽的 MLP 可以持续提升性能，接近 LightGBM
- ✅ 实验结果：架构改进边际收益 ≤0.02；数据量 32k→100k 提升 +0.05

## 5.2 关键结论（2-4 条）

| # | 结论 | 证据 |
|---|------|------|
| 1 | **Residual MLP 是 32k 数据下的 MLP SOTA** | $R^2 = 0.498$ vs 直接 MLP $R^2 = 0.487$ (+2.3%) |
| 2 | **架构复杂度已到瓶颈** | 2 层 vs 3 层差距 < 0.02；4 层崩溃；宽度边际收益 < 0.01 |
| 3 | **数据量是关键杠杆** | 32k → 100k: $R^2$ 0.498 → 0.551 (+10.6%)，大于所有架构改进之和 |
| 4 | **CNN 需要诊断修复** | 18 个实验 $R^2 \approx 0$，训练曲线几乎不下降 |

## 5.3 设计启示

### 架构原则

| 原则 | 建议 | 原因 |
|------|------|------|
| **Residual 策略** | 让 MLP 学习 Ridge 预测的残差 | +1-2% 稳定增益 |
| **2-3 层浅网络** | hidden=[256,64] 或 [512,256] | 更深反而崩溃 |
| **GELU 激活** | 优于 ReLU 和 SiLU | 更平滑，大模型更稳定 |
| **轻正则化** | dropout≤0.3, wd=0 (Residual 模式) | 过强正则会让残差学不到东西 |

### 正则化策略

| 模式 | dropout | weight decay | 备注 |
|------|---------|--------------|------|
| Residual MLP (32k) | 0.3 | 0 | 最佳配置 |
| 大模型 (100k) | 0.4 | 1e-4 | 数据多时可加正则 |
| 深层网络 (不推荐) | 0.4 | 1e-4 | 需配合残差连接 |

### ⚠️ 常见陷阱

| 常见做法 | 实验证据 |
|----------|----------|
| "加深网络到 4+ 层" | $R^2$ 从 0.49 骤降到 0.10-0.35 |
| "加宽网络到 2048+" | $R^2$ 仅从 0.487 提升到 0.489，边际收益几乎为 0 |
| "使用 TopK 特征选择" | TopK MLP (0.457) 不如全谱 MLP (0.487) |
| "使用 CNN 捕获局部特征" | 所有 CNN $R^2 \approx 0$（实现可能有问题） |

## 5.4 物理解释

### MLP 成功但有限的原因

| 原因 | 说明 |
|------|------|
| **$\log g$ 信息分散** | PCA 实验显示需要 100+ PC 才能达到高 $R^2$，MLP 需要学习复杂的权重组合 |
| **线性成分主导** | Ridge 已达 $R^2 = 0.458$，非线性修正空间有限 |
| **噪声限制** | noise=1.0 下 SNR 较低，限制了可学习的信息量 |

### CNN 失败的可能原因

| 原因 | 说明 | 证据 |
|------|------|------|
| **实现 bug** | Conv1d 输入维度顺序错误、flatten 维度不匹配 | Train $R^2 \approx 0$，完全没学习 |
| **感受野不足** | 2-3 层 CNN 感受野 ~50 像素 vs 4096 输入 | 但这不应导致完全不学习 |
| **全局特征需求** | $\log g$ 需要全局谱线组合，非局部纹理 | 但这只能解释"比 MLP 差"，不能解释"完全不学" |

### TopK 失效的原因

| 原因 | 说明 |
|------|------|
| **丢失上下文** | LightGBM TopK 是"对树有用的分裂点"，周围 continuum 对 MLP 同样重要 |
| **分布差异** | TopK 选择的像素分布与 MLP 需要的信息分布不同 |
| **解决方案** | 使用 "TopK + window" 保留局部上下文，或使用 Attention 机制 |

## 5.5 关键数字速查

| 指标 | 值 |
|------|-----|
| **32k 最佳 MLP** | $R^2 = 0.498$ (Residual + [256,64]) |
| **100k 最佳 MLP** | $R^2 = 0.551$ ([2048,1024,512] + GELU) |
| **Ridge baseline** | $R^2 = 0.458$ |
| **LightGBM baseline** | $R^2 = 0.536$ |
| **MLP vs Ridge** | **+8.7%** |
| **MLP vs LightGBM** | **-7.1%** |
| **100k vs 32k** | **+10.6%** |
| **Residual vs 直接 MLP** | **+2.3%** |
| **最佳深度** | 2-3 层 |
| **CNN 最佳** | $R^2 \approx 0.016$（基本不工作） |

## 5.6 下一步工作

| 方向 | 具体任务 | 优先级 | 预期收益 |
|------|----------|--------|----------|
| **公平比较** | 用 100k 数据重新训练 LightGBM 和 Ridge | P0 | 确定 NN 是否真的能超越树模型 |
| **修复 CNN** | 做 sanity check (noise=0, toy target)，检查实现 | P0 | 排除实现问题 |
| **Residual LightGBM** | MLP 学习 LightGBM 预测的残差 | P1 | 可能 +1-2% |
| **TopK + window** | 取 TopK 索引的邻域窗口 (±W)，再给 MLP/CNN | P1 | 结合降维和上下文 |
| **双通道输入** | [flux, error] 双通道输入 | P1 | Error 实验显示 error 有额外信息 |
| **权重/saliency 分析** | 分析 MLP 第一层权重，对比 Ridge/LightGBM 重要性 | P2 | 指导下一代架构设计 |

---

# 6. 📎 附录

## 6.1 数值结果表（Results）

### 6.1.1 32k 数据 MLP 排行榜 (Top 10)

| 排名 | 实验ID | Test $R^2$ | 参数量 | 架构 | 说明 |
|------|--------|---------|--------|------|------|
| 🥇 1 | MLP_Residual_nz1p0 | **0.4985** | 1.1M | Res+[256,64] | 32k SOTA |
| 🥈 2 | MLP_Res_v2_2L_512_gelu | 0.4901 | 2.2M | Res+[512,256] | Residual |
| 🥉 3 | MLP_Wide_2L_2048 | 0.4889 | 9.4M | [2048,512] | Wide |
| 4 | Step1_lr1en03_wd0 | 0.4874 | 1.1M | [256,64] | 基础 MLP |
| 5 | Step2_2L_256_gelu_xavier | 0.4833 | 1.1M | [256,64] | GELU |
| 6 | MLP_Res_v2_2L_256_gelu | 0.4815 | 1.1M | Res+[256,64] | Residual |
| 7 | MLP_Wide_1L_1024 | 0.4739 | 4.2M | [1024] | 单层宽 |
| 8 | Step2_2L_256_relu_xavier | 0.4737 | 1.1M | [256,64] | ReLU |
| 9 | MLP_Deep_3L_512_gelu | 0.4698 | 1.5M | [512,256,128] | 3 层 |
| 10 | MLP_Wide_2L_1024 | 0.4671 | 4.5M | [1024,256] | Wide |

### 6.1.2 100k 数据 MLP 排行榜 (MLP_Big 系列)

| 排名 | 实验ID | Test $R^2$ | 参数量 | 架构 | epochs |
|------|--------|---------|--------|------|--------|
| 🥇 1 | MLP_Big_3L_2048_gelu | **0.5505** | 11.0M | [2048,1024,512] | 150 |
| 🥈 2 | MLP_Big_2L_4096 | 0.5346 | 21.0M | [4096,1024] | 150 |
| 🥉 3 | MLP_Big_4L_2048_gelu | 0.5257 | 11.1M | [2048,1024,512,256] | 150 |
| 4 | MLP_Big_2L_2048 | 0.5249 | 9.4M | [2048,512] | 150 |
| 5 | MLP_Big_3L_1024_reg | 0.5167 | 4.9M | [1024,512,256] | 150 |
| 6 | MLP_Big_res_3L_2048 | 0.5008 | 11.0M | Res+[2048,1024,512] | 150 |

### 6.1.3 TopK MLP 结果

| 实验ID | K | Test $R^2$ | 参数量 | vs 全谱 MLP |
|--------|---|---------|--------|-------------|
| MLP_TopK_K1024 | 1024 | 0.4566 | 279K | **-6.3%** |
| MLP_TopK_K512 | 512 | 0.4492 | 148K | **-7.8%** |
| MLP_TopK_K256 | 256 | 0.4162 | 82K | **-14.6%** |

### 6.1.4 CNN 结果（全部失败）

| 实验ID | 架构 | Kernel | Test $R^2$ | 状态 |
|--------|------|--------|---------|------|
| CNN_S1_lr3en03_wd5en04 | [32,64] | 7 | 0.0158 | ❌ 失败 |
| CNN_S2_2L_c32_64_k15_bn | [32,64] | 15 | -0.0003 | ❌ 失败 |
| CNN_S2_3L_c32_64_128_k15 | [32,64,128] | 15 | **NaN** | 💀 崩溃 |

### 6.1.5 深度/宽度探索结果

**深度探索**：

| 实验ID | 层数 | 架构 | Test $R^2$ | 趋势 |
|--------|------|------|---------|------|
| MLP_Wide_2L_2048 | 2 | [2048,512] | **0.4889** | 最佳 |
| MLP_Deep_3L_512_gelu | 3 | [512,256,128] | 0.4698 | 略降 |
| MLP_Deep_3L_1024_gelu | 3 | [1024,512,256] | 0.4619 | 继续降 |
| MLP_Deep_4L_512_gelu | 4 | [512,256,128,64] | 0.3556 | 崩溃 |
| MLP_Deep_4L_256_relu | 4 | [256,256,128,64] | 0.0998 | 完全崩溃 |

**宽度探索**：

| 实验ID | 宽度 | Test $R^2$ | 参数量 | 边际收益 |
|--------|------|---------|--------|----------|
| Step1_lr1en03_wd0 | 256 | 0.4874 | 1.1M | baseline |
| MLP_Wide_1L_1024 | 1024 | 0.4739 | 4.2M | **-2.8%** (单层不够) |
| MLP_Wide_2L_2048 | 2048 | 0.4889 | 9.4M | **+0.3%** |

## 6.2 建议绘图（Plot Suggestions）

### 6.2.1 数据量效应图
- **目的**：直观展示数据量对性能的影响
- **X 轴**：数据量 (32k, 100k)
- **Y 轴**：Test $R^2$
- **系列**：MLP, Residual MLP
- **标注**：Ridge/LightGBM baseline

### 6.2.2 架构深度 vs 性能
- **目的**：展示深度的最优区间
- **X 轴**：层数 (1, 2, 3, 4)
- **Y 轴**：Test $R^2$
- **图上标注**：4 层崩溃点

### 6.2.3 Residual vs 直接 MLP 对比
- **目的**：量化 Residual 策略的增益
- **类型**：分组柱状图
- **分组**：不同架构 ([256,64], [512,256])
- **系列**：直接 MLP vs Residual MLP

### 6.2.4 TopK 特征数量 vs 性能
- **目的**：展示 TopK 对 MLP 的影响
- **X 轴**：K (256, 512, 1024, 4096)
- **Y 轴**：Test $R^2$
- **标注**：全谱 MLP baseline

### 6.2.5 CNN 诊断：训练曲线对比
- **目的**：展示 CNN 训练异常
- **X 轴**：Epochs
- **Y 轴**：Train Loss / Train $R^2$
- **系列**：MLP (正常下降) vs CNN (几乎不变)

## 6.3 最佳配置推荐

### 32k 数据（公平比较）
```yaml
model: MLP + Residual
hidden_sizes: [256, 64]
activation: relu
dropout: 0.3
lr: 0.001
weight_decay: 0
epochs: 100
预期 R²: 0.498
```

### 100k 数据（最高精度）
```yaml
model: MLP
hidden_sizes: [2048, 1024, 512]
activation: gelu
dropout: 0.4
lr: 0.0003
weight_decay: 0.0001
epochs: 150
预期 R²: 0.55+
```

## 6.4 下一步实验设计

### 方向 A：100k 公平比较

| 实验 | 模型 | 数据量 | 目的 |
|------|------|--------|------|
| A1 | LightGBM | 100k | 公平对比 |
| A2 | Ridge | 100k | 公平对比 |
| A3 | Residual MLP | 100k | 验证 Residual 在大数据下是否仍有效 |

### 方向 B：输入/目标设计

| 实验 | 设计 | 预期收益 |
|------|------|----------|
| B1 | LightGBM Residual | MLP 学习 LightGBM 预测的残差 |
| B2 | TopK + window (±W=8) | 降维 + 保留局部上下文 |
| B3 | [flux, error] 双通道 | 利用 error 通道的额外信息 |

### 方向 C：CNN 修复

| 实验 | 设计 | 目的 |
|------|------|------|
| C1 | noise=0 sanity check | 验证 CNN 能否在简单情况下学习 |
| C2 | 人造简单 target (如 mean flux) | 排除数据问题 |
| C3 | 代码 review | 检查 Conv1d 维度、flatten 等 |

## 6.5 相关文件

| 类型 | 路径 |
|------|------|
| 原始结果汇总 | `/home/swei20/Physics_Informed_AI/raw/FULL_RESULTS_SUMMARY.md` |
| 结果 CSV | `/home/swei20/VIT/results/nn_baselines/nn_vs_ml_results.csv` |
| MLP Baseline 报告 | `/home/swei20/Physics_Informed_AI/logg/NN/exp_mlp_baseline_20251130.md` |
| NN 架构设计文档 | `/home/swei20/Physics_Informed_AI/logg/NN/exp_nn_architecture_design_20251129.md` |
| 实验脚本 | `/home/swei20/VIT/scripts/run_nn_baselines.py` |
| 模型定义 | `/home/swei20/VIT/src/nn/models/` |

---

*报告创建时间: 2025-11-30*  
*总实验数: 53 个 (18 CNN + 35 MLP)*  
*成功实验: 35 个 MLP*  
*失败实验: 18 个 CNN*  
*核心发现: Residual MLP 达到 32k 极限 ($R^2 \approx 0.50$)，数据量是关键杠杆*


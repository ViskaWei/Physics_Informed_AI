# ğŸ“˜ Neural Network æ¶æ„è®¾è®¡å®éªŒæŠ¥å‘Š

---
> **å®éªŒåç§°ï¼š** Physics-Informed Neural Network Architecture Design for $\log g$ Prediction  
> **å¯¹åº” MVPï¼š** NN æ¶æ„ç³»ç»Ÿæ€§è®¾è®¡ï¼ˆå¤šé˜¶æ®µï¼‰  
> **ä½œè€…ï¼š** Viska Wei  
> **æ—¥æœŸï¼š** 2025-11-29  
> **æ•°æ®ç‰ˆæœ¬ï¼š** HDF5 å…‰è°±æ•°æ®ï¼ˆ4096 åƒç´ åˆæˆå…‰è°±ï¼‰  
> **æ¨¡å‹ç‰ˆæœ¬ï¼š** Phase 1 - Baseline MLP/CNN (PyTorch)  
> **çŠ¶æ€ï¼š** ğŸ”„ è¿›è¡Œä¸­

---

# âš¡ æ ¸å¿ƒç»“è®ºé€Ÿè§ˆï¼ˆä¾› main æå–ï¼‰

### ä¸€å¥è¯æ€»ç»“

> **ç»¼åˆå‰æœŸå®éªŒï¼ŒNN è®¾è®¡æ ¸å¿ƒåŸåˆ™ï¼š(1) çº¿æ€§ shortcut å¿…éœ€ï¼Œ(2) å™ªå£°å¢å¼ºè®­ç»ƒ >> æ˜¾å¼ç‰¹å¾é€‰æ‹©ï¼Œ(3) åŒé€šé“ [flux, Ïƒ] è¾“å…¥ï¼Œ(4) bottleneck â‰¥ 100 ç»´ã€‚**

### å¯¹å‡è®¾çš„éªŒè¯

| éªŒè¯é—®é¢˜ | ç»“æœ | ç»“è®º |
|---------|------|------|
| çº¿æ€§æˆåˆ†æ˜¯å¦ä¸»è¦ï¼Ÿ | âœ… æ˜¯ï¼Œnoise=0 æ—¶ $R^2=0.999$ | çº¿æ€§ shortcut å¿…éœ€ |
| å™ªå£°è®­ç»ƒæ˜¯å¦å…³é”®ï¼Ÿ | âœ… æ˜¯ï¼Œæ•ˆæœæ˜¯ TopK çš„ 12 å€ | Noise augmentation æ ¸å¿ƒ |
| Error é€šé“æ˜¯å¦æœ‰ç”¨ï¼Ÿ | âœ… æ˜¯ï¼Œå•ç‹¬ $R^2=0.91$ | åŒé€šé“è¾“å…¥å¿…é¡» |
| æœ€å° bottleneck ç»´åº¦ï¼Ÿ | âœ… 100+ï¼ŒåŸºäº PCA å®éªŒ | ä¸èƒ½è¿‡åº¦é™ç»´ |

### è®¾è®¡å¯ç¤ºï¼ˆ1-2 æ¡ï¼‰

| å¯ç¤º | å…·ä½“å»ºè®® |
|------|---------|
| **æ¶æ„è®¾è®¡åŸåˆ™** | Linear shortcut + Learnable attention + [flux, Ïƒ] åŒé€šé“ |
| **è®­ç»ƒç­–ç•¥** | å™ªå£°å¢å¼ºè®­ç»ƒ + ä¸å™ªå£°æŒ‚é’©çš„ weight decay |

### å…³é”®æ•°å­—

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| çº¿æ€§ $R^2$ (noise=0) | **0.999** |
| æœ€å°æœ‰æ•ˆ K | **~50 (tokens)** |
| Error-only $R^2$ | **0.91** |
| æœ€å° bottleneck | **100 ç»´** |

---

# ğŸ“‘ ç›®å½•

- [1. ğŸ¯ ç›®æ ‡](#1--ç›®æ ‡)
- [2. ğŸ§ª å®éªŒè®¾è®¡](#2--å®éªŒè®¾è®¡experiment-design)
- [3. ğŸ“Š å®éªŒå›¾è¡¨](#3--å®éªŒå›¾è¡¨)
- [4. ğŸ’¡ å…³é”®æ´è§](#4--å…³é”®æ´è§key-insights)
- [5. ğŸ“ ç»“è®º](#5--ç»“è®ºconclusion)
- [6. ğŸ“ é™„å½•](#6--é™„å½•)

---

# 1. ğŸ¯ ç›®æ ‡

## 1.1 èƒŒæ™¯ä¸åŠ¨æœº

å½“å‰æˆ‘ä»¬å·²ç»ç”¨ Ridge Regression å’Œ LightGBM ç³»ç»Ÿæ€§åœ°åˆ†æäº†å…‰è°±åˆ° $\log g$ çš„æ˜ å°„å…³ç³»ï¼Œå¾—åˆ°äº†ä¸€äº›æ¯”è¾ƒæ¸…æ™°çš„ç»“è®ºã€‚æœ¬å®éªŒæ—¨åœ¨ï¼š

> **ä»ç®€å•åˆ°å¤æ‚åœ°æ­å»ºç¥ç»ç½‘ç»œï¼ˆMLP â†’ CNN â†’ Vision Transformerï¼‰ï¼ŒéªŒè¯å¹¶åˆ©ç”¨è¿™äº›çº¿æ€§ / ç‰¹å¾é€‰æ‹© / å™ªå£°ç›¸å…³çš„ç»“è®ºï¼ŒæŠŠå®ƒä»¬è½¬åŒ–æˆå¯å·¥ä½œçš„ NN æ¶æ„è®¾è®¡åŸåˆ™ã€‚**

æ›´å…·ä½“åœ°è¯´ï¼š
1. ä»¥ **MLP / 1D CNN** ä¸ºèµ·ç‚¹ï¼Œåœ¨ç›¸åŒè®­ç»ƒé›†ã€å™ªå£°è®¾å®šå’Œè¯„ä¼°æŒ‡æ ‡ä¸‹ï¼Œç³»ç»Ÿæ¯”è¾ƒå®ƒä»¬ä¸ Ridge / LightGBM çš„è¡¨ç°
2. åˆ†ç¦»å¹¶é‡åŒ–ï¼š
   - "**çº¿æ€§æˆåˆ†**"ï¼šå¯ä»¥ç›´æ¥ç”±çº¿æ€§å±‚ / åˆå§‹åŒ–ç»§æ‰¿è‡ª Ridge
   - "**éçº¿æ€§ä¿®æ­£æˆåˆ†**"ï¼šMLP/CNN åªåœ¨å±€éƒ¨çª—å£ã€Top-K åŒºåŸŸåšå°çš„éçº¿æ€§æ ¡æ­£
3. æ¢ç´¢ **è¾“å…¥ç»“æ„**ï¼ˆå…¨è°± vs Top-K vs Top-K+å±€éƒ¨çª—å£ï¼‰ã€**å™ªå£°æ°´å¹³** ä¸ **NN æ¶æ„é€‰æ‹©**ï¼ˆæ·±åº¦ã€å®½åº¦ã€å·ç§¯æ„Ÿå—é‡ï¼‰çš„å…³ç³»ï¼Œä¸ºåç»­ Vision Transformer çš„ token è®¾è®¡/æ³¨æ„åŠ›ç¨€ç–åŒ–æä¾›ä¾æ®

### ğŸ”¬ å‰æœŸå®éªŒçš„æ ¸å¿ƒå‘ç°ï¼ˆæ”¯æ’‘ NN è®¾è®¡çš„ Insightsï¼‰

| æ¥æºå®éªŒ | æ ¸å¿ƒå‘ç° | å¯¹ NN è®¾è®¡çš„å¯ç¤º |
|---------|---------|-----------------|
| **Ridge Î± Sweep** | $\log g$-flux æ˜ å°„æœ¬è´¨çº¿æ€§ (noise=0 æ—¶ $R^2=0.999$) | çº¿æ€§ shortcut æ˜¯å¿…é¡»çš„ |
| **Ridge Î± Sweep** | NN ä¸»è¦ä»»åŠ¡æ˜¯"å¿½ç•¥æ— å…³åƒç´ "ï¼Œè€Œé"æå–ä¿¡æ¯" | éœ€è¦ Attention/Sparse/Denoising æœºåˆ¶ |
| **Ridge Î± Sweep** | æœ€ä¼˜ $\alpha$ éšå™ªå£°å•è°ƒå¢å¤§ (è·¨è¶Š 6 ä¸ªæ•°é‡çº§) | Weight decay éœ€ä¸å™ªå£°æŒ‚é’© |
| **PCA å®éªŒ** | $\log g$ ä¿¡æ¯åˆ†æ•£åœ¨ PC 20-200 çš„ä½æ–¹å·®æ–¹å‘ | Bottleneck â‰¥ 100 ç»´ |
| **PCA å®éªŒ** | éœ€è¦ 100+ PC æ‰èƒ½è¾¾åˆ° $R^2 \geq 0.99$ | ä¸èƒ½è¿‡åº¦é™ç»´ |
| **PCA å®éªŒ** | å‰ 5-10 PC ä»…ä¿ç•™ 67% ä¿¡æ¯ | ä¸èƒ½ç®€å•ç”¨ PCA é¢„å¤„ç† |
| **Feature Stability** | N=0 æ˜¯"å­¤å²›"ï¼Œä¸å…¶ä»–å™ªå£°å®Œå…¨ä¸ç›¸å…³ | é¿å…åœ¨æ— å™ªå£°æ•°æ®ä¸Šè®­ç»ƒ |
| **Feature Stability** | é«˜å™ªå£°ä¸‹ç‰¹å¾é‡è¦æ€§é«˜åº¦ç¨³å®š | é«˜å™ªå£°è®­ç»ƒå¯èƒ½æ›´é²æ£’ |
| **Top-K å®éªŒ** | ä¿¡æ¯é«˜åº¦ç¨€ç–ï¼Œ~24% åƒç´  (K=1000) å³å¯ | Learnable soft mask / attention |
| **Top-K å®éªŒ** | é«˜å™ªå£°è®­ç»ƒæ•ˆæœæ˜¯ Top-K çš„ 12 å€ | Noise augmentation æ˜¯æ ¸å¿ƒç­–ç•¥ |
| **Small K Limit** | å…³é”®è°±çº¿ï¼šCa II 8542, Mg I 8807, Na I 8183 | Patch èšç„¦ç‰¹å®šæ³¢é•¿åŒºåŸŸ |
| **Small K Limit** | K=50 è¾¾ $R^2=0.39$ï¼ŒK=100 è¾¾ $R^2=0.49$ | æœ€å° Token æ•° ~50 |
| **Small K Limit** | éçº¿æ€§ç»„åˆå¿…è¦ï¼šLGBM >> Ridge ($\Delta R^2 \approx 0.16$) | éœ€è¦éçº¿æ€§å±‚ |
| **Error å®éªŒ** | Error $\sigma$ å•ç‹¬å¯è¾¾ $R^2=0.91$ | å¿…é¡»ä½¿ç”¨ [flux, $\sigma$] åŒé€šé“è¾“å…¥ |
| **Error å®éªŒ** | Error-$\log g$ å…³ç³»æ˜¯éçº¿æ€§çš„ | éœ€è¦éçº¿æ€§å±‚æå– error ä¿¡æ¯ |
| **LGBM å®éªŒ** | æ ‘æ¨¡å‹è‡ªå¸¦ç‰¹å¾é€‰æ‹©ï¼ŒTop-K è¾¹é™…æ”¶ç›Šå° | Attention æ›¿ä»£æ˜¾å¼ç‰¹å¾é€‰æ‹© |
| **LGBM Sweep** | æœ€ä¼˜ LightGBM $R^2=0.9982$ | NN åŸºçº¿éœ€è¶…è¿‡æ­¤å€¼ |

### ğŸ¯ è®¾è®¡å“²å­¦

åŸºäºä»¥ä¸Šå‘ç°ï¼ŒNN è®¾è®¡åº”éµå¾ªä»¥ä¸‹æ ¸å¿ƒåŸåˆ™ï¼š

1. **"Linear + Residual"**: ä¸»å¹²æ˜¯çº¿æ€§æ˜ å°„ï¼ŒNN åªå­¦ä¹ æ®‹å·®
2. **"Attention for Filtering"**: ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å®ç°éšå¼ç‰¹å¾é€‰æ‹©
3. **"Variance-Aware"**: å¯¹ä½æ–¹å·®ä½†é«˜ä¿¡æ¯é‡çš„æ–¹å‘ç»™äºˆè¶³å¤Ÿæƒé‡
4. **"Noise-Adaptive"**: æ ¹æ®å™ªå£°æ°´å¹³åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦
5. **"Dual-Channel"**: åŒæ—¶åˆ©ç”¨ flux å’Œ error Ïƒ ä½œä¸ºè¾“å…¥

## 1.2 æ ¸å¿ƒå‡è®¾

> **æ ¸å¿ƒå‡è®¾ï¼šåœ¨ç»™å®šç›¸åŒè¾“å…¥å’Œå‚æ•°é¢„ç®—çš„å‰æä¸‹ï¼Œå°å‹ MLP/CNN è‹¥æ˜¾å¼åˆ©ç”¨"çº¿æ€§ baseline + Top-K ç‰¹å¾é€‰æ‹©"ï¼Œå¯ä»¥åœ¨å„å™ªå£°æ°´å¹³ä¸Šç¨³å®šè¶…è¿‡ Ridge Regression / LightGBM çš„ $R^2$ï¼Œä¸”æ€§èƒ½æå‡ä¸»è¦æ¥è‡ªå¯¹å±€éƒ¨éçº¿æ€§ line profile çš„å»ºæ¨¡ï¼Œè€Œä¸æ˜¯é‡æ–°å­¦ä¹ æ•´ä¸ªçº¿æ€§æ˜ å°„ã€‚**

å¦‚æœå‡è®¾æˆç«‹ï¼Œæ„å‘³ç€ï¼š
- $\log g$ çš„ **ä¸»ä½“ä¿¡æ¯ç¡®å®æ˜¯é«˜ç»´ä½†è¿‘ä¼¼çº¿æ€§çš„**ï¼ŒNN åªéœ€åœ¨å°‘æ•°å…³é”®åŒºåŸŸåšå±€éƒ¨éçº¿æ€§ä¿®æ­£
- æ˜¾å¼æ³¨å…¥å…ˆéªŒï¼ˆRidge æƒé‡åˆå§‹åŒ–ã€Top-K æ³¢é•¿å­é›†ï¼‰æ˜¯æœ‰æ•ˆçš„ï¼š
  - å¯ä»¥æ˜¾è‘—å‡å°‘ç½‘ç»œæ‰€éœ€å‚æ•°ä¸è®­ç»ƒæ•°æ®é‡
  - å¯¹äºä¸­é«˜å™ªå£°ï¼Œ**"å…ˆé€‰ç‚¹å†å»ºæ¨¡"** æ¯”"åœ¨å…¨è°±ä¸Šç›²ç›®å †æ·±åº¦/å®½åº¦"æ›´ä¼˜
- ä¸ºåç»­ ViT æä¾›è®¾è®¡æ–¹å‘ï¼š
  - åªéœ€è¦å¯¹å°‘é‡ **ä¿¡æ¯å¯†é›†çš„ token / patch** ä½¿ç”¨é«˜åˆ†è¾¨ç‡æ³¨æ„åŠ›
  - å…¶å®ƒåŒºåŸŸå¯ä»¥é™æƒã€ä¸‹é‡‡æ ·æˆ–ç²—ç•¥å»ºæ¨¡

å¦‚æœå‡è®¾ä¸æˆç«‹ï¼Œåˆ™éœ€è¦ï¼š
- é‡æ–°è¯„ä¼°ï¼šä¼ ç»Ÿ ML å¾—åˆ°çš„"çº¿æ€§ + Top-K"ç»“æ„æ˜¯å¦åªæ˜¯åœ¨æ¨¡å‹å®¶æ—é™åˆ¶ä¸‹çš„"å‡çº¿æ€§"
- æ˜¯å¦å­˜åœ¨å¤§è§„æ¨¡ã€åˆ†å¸ƒå¹¿æ³›çš„ **å¼ºéçº¿æ€§æ¨¡å¼**ï¼Œåªæœ‰è¶³å¤Ÿ expressive çš„ NN æ‰èƒ½æŒ–å‡ºæ¥
- å¯èƒ½éœ€è¦è½¬å‘ï¼šæ›´æ·±/æ›´å®½çš„ç½‘ç»œï¼ˆæ›´å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼‰ã€æ›´å¤æ‚çš„éçº¿æ€§ç»“æ„ï¼ˆå¤šå°ºåº¦ CNNã€Transformerã€ç‰©ç†å…ˆéªŒåµŒå…¥ï¼‰

## 1.3 éªŒè¯é—®é¢˜

> è¿™äº›é—®é¢˜æ˜¯ä¸º"æ ¸å¿ƒå‡è®¾"æœåŠ¡çš„ï¼Œæ¯ä¸ªé—®é¢˜éƒ½å¯¹åº”ä¸€ä¸ªå¯é‡åŒ–çš„å®éªŒã€‚
> ç»“æœä¸€æ å®éªŒåå¡«ï¼š`âœ…/âŒ + å…³é”®æ•°å€¼ï¼ˆRÂ² / MAE / ç›¸å¯¹æå‡ï¼‰`ã€‚

### 1.3.1 Baseline NN éªŒè¯é—®é¢˜ï¼ˆæœ¬é˜¶æ®µé‡ç‚¹ï¼‰

| # | é—®é¢˜ | éªŒè¯ç›®æ ‡ | ç»“æœ |
|---|------|---------|------|
| Q1 | **åœ¨æ— å™ªå£°æ•°æ®ä¸Šï¼Œå…¨è°±è¾“å…¥çš„ç®€å• MLPï¼ˆ2â€“3 å±‚ï¼‰èƒ½å¦è¾¾åˆ°æˆ–è¶…è¿‡ Ridge baseline çš„ $R^2$ï¼Ÿ** | éªŒè¯åœ¨ clean regime ä¸‹ï¼ŒNN è‡³å°‘ä¸æ¯”çº¿æ€§å·®ï¼Œæ”¯æ’‘"çº¿æ€§ + å°éçº¿æ€§ä¿®æ­£"èŒƒå¼æ˜¯åˆç†èµ·ç‚¹ | [å¾…å¡«] |
| Q2 | **åœ¨ä¸­ç­‰å™ªå£°æ°´å¹³ï¼ˆnoise=1.0ï¼‰ä¸‹ï¼Œä½¿ç”¨ Top-K é‡è¦æ³¢é•¿å­é›†è®­ç»ƒçš„ MLPï¼Œæ˜¯å¦æ˜¾è‘—ä¼˜äºå…¨è°± MLPï¼Ÿ** | éªŒè¯"Top-K ç‰¹å¾é€‰æ‹©å¯ä»¥å‡å°‘ NN åœ¨å»å™ªä¸Šçš„æµªè´¹"ï¼Œæ”¯æŒ"å…ˆé€‰ç‚¹å†å»ºæ¨¡"çš„ç­–ç•¥ | [å¾…å¡«] |
| Q3 | **åœ¨ç›¸åŒè¾“å…¥ï¼ˆTop-K+å±€éƒ¨çª—å£ï¼‰å’Œå‚æ•°é‡çš„å‰æä¸‹ï¼Œå°å‹ 1D CNN æ˜¯å¦èƒ½ç¨³å®šè¶…è¿‡ MLP çš„ $R^2$ï¼Ÿ** | éªŒè¯"å±€éƒ¨ line profile / continuum ç»“æ„æ˜¯æœ‰ç”¨çš„"ï¼ŒCNN çš„å±€éƒ¨æ„Ÿå—é‡æ˜¯å¦å¯ä»¥æ›´å¥½åˆ©ç”¨è¿™äº›ä¿¡æ¯ | [å¾…å¡«] |
| Q4 | **ä»¥ Ridge æƒé‡åˆå§‹åŒ–ç¬¬ä¸€å±‚çº¿æ€§å±‚çš„ MLP/CNNï¼Œç›¸æ¯”éšæœºåˆå§‹åŒ–ï¼Œæ˜¯å¦åœ¨æ”¶æ•›é€Ÿåº¦æˆ–æœ€ç»ˆ $R^2$ ä¸Šæœ‰æ”¶ç›Šï¼Ÿ** | éªŒè¯"æ˜¾å¼æ³¨å…¥çº¿æ€§å…ˆéªŒ"æ˜¯å¦èƒ½å‡å°‘è®­ç»ƒéš¾åº¦ã€æå‡æ•°æ®æ•ˆç‡ï¼Œä¸ºåç»­"çº¿æ€§+Transformer"æ··åˆæ¶æ„æä¾›æ”¯æŒ | [å¾…å¡«] |
| Q5 | **åœ¨é«˜å™ªå£°æ°´å¹³ï¼ˆnoise=2.0ï¼‰ä¸‹ï¼Œæœ€ä¼˜ NNï¼ˆMLP/CNNï¼‰çš„æå‡ä¸»è¦é›†ä¸­åœ¨å“ªäº›å™ªå£° regimeï¼Ÿ** | å®šä½"NN ç›¸å¯¹ä¼ ç»Ÿ ML çš„ä¼˜åŠ¿åŒºåŸŸ"ï¼Œç¡®è®¤æ€§èƒ½æå‡æ˜¯å¦å¦‚å‡è®¾æ‰€è¯´ä¸»è¦æ¥è‡ªä¸­é«˜å™ªå£°è€Œé clean regime | [å¾…å¡«] |

### 1.3.2 Physics-Informed æ¶æ„éªŒè¯é—®é¢˜ï¼ˆåç»­é˜¶æ®µï¼‰

| # | é—®é¢˜ | éªŒè¯ç›®æ ‡ | é¢„æœŸç»“æœ |
|---|------|---------|----------|
| Q6 | Linear shortcut æ˜¯å¦æ˜¾è‘—æå‡æ€§èƒ½ï¼Ÿ | éªŒè¯"çº¿æ€§æœ¬è´¨"å‡è®¾ | $\Delta R^2 \geq 0.02$ vs æ—  shortcut |
| Q7 | Learnable attention æ˜¯å¦ä¼˜äº Full Spectrumï¼Ÿ | éªŒè¯"ä¿¡æ¯ç¨€ç–"å‡è®¾ | noise=1.0 æ—¶ $\Delta R^2 \geq 0.05$ |
| Q8 | Dual-channel [flux, $\sigma$] æ˜¯å¦ä¼˜äº flux-onlyï¼Ÿ | éªŒè¯"error ä¿¡æ¯"å‡è®¾ | $\Delta R^2 \geq 0.03$ |
| Q9 | Noise augmentation æ˜¯å¦æå‡å™ªå£°é²æ£’æ€§ï¼Ÿ | éªŒè¯"å™ªå£°è®­ç»ƒ"å‡è®¾ | è·¨å™ªå£°æ³›åŒ– $\Delta R^2 \geq 0.1$ |
| Q10 | èƒ½å¦è¶…è¶Š LightGBM åŸºçº¿ï¼Ÿ | æ•´ä½“æ¶æ„éªŒè¯ | noise=1.0 æ—¶ $R^2 > 0.52$ |

## 1.4 ç»“è®ºæ‘˜è¦ï¼ˆå®éªŒåå¡«å†™ï¼‰

### 1.4.1 å®éªŒç»“è®º

| ç»“è®º | è¯´æ˜ |
|------|------|
| TODO | TODO |

### 1.4.2 è®¾è®¡å¯ç¤º

| è®¾è®¡åŸåˆ™ | å…·ä½“å»ºè®® |
|---------|---------|
| TODO | TODO |

> **ä¸€å¥è¯æ€»ç»“**ï¼šTODO

---

# 2. ğŸ§ª å®éªŒè®¾è®¡ï¼ˆExperiment Designï¼‰

## 2.1 æ•°æ®ï¼ˆDataï¼‰

| é…ç½®é¡¹ | å€¼ |
|--------|-----|
| è®­ç»ƒæ ·æœ¬æ•° | 32,000 |
| éªŒè¯æ ·æœ¬æ•° | 10,000 |
| æµ‹è¯•æ ·æœ¬æ•° | 10,000 |
| ç‰¹å¾ç»´åº¦ | 4,096 (flux) + 4,096 (error) = 8,192 |
| æ ‡ç­¾å‚æ•° | $\log g$ |
| å™ªå£°æ°´å¹³ | test: {0.0, 1.0, 2.0} |

### 2.1.1 æ•°æ®æ ¼å¼

| å­—æ®µ | ç»´åº¦ | è¯´æ˜ |
|------|------|------|
| `flux` | (N, 4096) | å…‰è°±æµé‡å‘é‡ |
| `error` | (N, 4096) | æ¯ä¸ªåƒç´ çš„æµ‹é‡è¯¯å·® |
| `log_g` | (N,) | ç›®æ ‡æ ‡ç­¾ |

### 2.1.2 é¢„å¤„ç†

```python
# Flux æ ‡å‡†åŒ–ï¼ˆæŒ‰è®­ç»ƒé›†ç»Ÿè®¡ï¼‰
flux_normalized = (flux - flux_mean) / flux_std

# å™ªå£°æ³¨å…¥
noisy_flux = flux + randn() * error * noise_level
```

**å™ªå£°æ¨¡å‹ï¼š**
$$
\text{noisy\_flux} = \text{flux} + \mathcal{N}(0, 1) \times \text{error} \times \text{noise\_level}
$$

### 2.1.3 å™ªå£°æ°´å¹³ä¸ Ridge Baseline

| noise_level | å«ä¹‰ | Ridge æœ€ä¼˜ $\alpha$ | Ridge Test $R^2$ | Ridge Test MAE | Ridge Test RMSE |
|-------------|------|---------------------|------------------|----------------|-----------------|
| 0.0 | æ— å™ªå£° | 0.001 | **0.999** | 0.005 | 0.009 |
| 1.0 | æ ‡å‡†å™ªå£° | 200.0 | **0.458** | 0.173 | 0.215 |
| 2.0 | é«˜å™ªå£° | 1000.0 | **0.221** | 0.212 | 0.258 |

## 2.2 ä½¿ç”¨çš„ç‰¹å¾ç±»å‹

| ç‰¹å¾ç±»å‹ | ç»´åº¦ | æ¥æº Insight |
|---------|------|-------------|
| flux (åŸå§‹å…‰è°±) | 4096 | ä¸»è¦ä¿¡æ¯è½½ä½“ |
| error Ïƒ (æµ‹é‡è¯¯å·®) | 4096 | Error å®éªŒ: $R^2=0.91$ |
| flux / error (SNR) | 4096 | Error å®éªŒæ¨è |
| PCA whitened flux | 200 | PCA å®éªŒ: æœ‰æ•ˆç»´åº¦ ~200 |

## 2.3 æ¨¡å‹ä¸ç®—æ³•ï¼ˆModel & Algorithmï¼‰

### 2.3.1 æ¶æ„ A: Linear + Residual MLP

**Insight æ¥æº**: Ridge Î± Sweep ("çº¿æ€§æœ¬è´¨") + PCA å®éªŒ ("ä½æ–¹å·®ä¿¡æ¯")

$$
\hat{y} = \underbrace{w^\top x}_{\text{Linear Shortcut}} + \underbrace{g_\theta(x)}_{\text{Residual MLP}}
$$

```
Input (4096) â†’ [Linear Shortcut] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                              â”‚
      â””â†’ MLP(4096 â†’ 512 â†’ 128 â†’ 32 â†’ 1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [Add] â†’ Output
                                                     
å…¶ä¸­ Linear Shortcut: y_linear = w^T x + b (å¯åˆå§‹åŒ–ä¸º Ridge è§£)
```

**å…³é”®è®¾è®¡**:
- Linear shortcut åˆå§‹åŒ–ä¸º Ridge æœ€ä¼˜è§£
- MLP åªéœ€å­¦ä¹ æ®‹å·® (é¢„æœŸå¾ˆå°)
- MLP æ·±åº¦æµ… (2-3 å±‚å³å¯)

### 2.3.2 æ¶æ„ B: Attention-based Feature Selection

**Insight æ¥æº**: Top-K å®éªŒ ("ä¿¡æ¯ç¨€ç–") + Small K Limit ("å…³é”®è°±çº¿")

$$
\hat{y} = f_\theta\left(\sum_{i=1}^{D} \alpha_i \cdot x_i\right), \quad \alpha_i = \text{softmax}(W_\alpha x)_i
$$

```
Input (4096) â”€â”€â†’ Attention Weights (learnable) â”€â”€â†’ Weighted Sum â”€â”€â†’ MLP â”€â”€â†’ Output
      â”‚                                                             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Linear Shortcut â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®è®¾è®¡**:
- Learnable soft attention æ›¿ä»£ hard Top-K
- æ³¨æ„åŠ›æƒé‡å¯è§£é‡Š (åº”èšç„¦ Ca II, Mg I, Na I åŒºåŸŸ)
- ä¿ç•™çº¿æ€§ shortcut ä½œä¸ºåå¤‡

### 2.3.3 æ¶æ„ C: Dual-Channel (Flux + Error)

**Insight æ¥æº**: Error å®éªŒ ("Error å«ç‰©ç†ä¿¡æ¯") + Top-K å®éªŒ ("SNR ç‰¹å¾")

```
Flux (4096) â”€â”€â”€â”€â†’ Encoder_flux â”€â”€â”
                                 â”œâ”€â”€â†’ Fusion Layer â”€â”€â†’ MLP â”€â”€â†’ Output
Error Ïƒ (4096) â”€â”€â†’ Encoder_err â”€â”€â”˜
                                 â”‚
                     Linear Shortcut (from flux only)
```

**å…³é”®è®¾è®¡**:
- ä¸¤è·¯ç‹¬ç«‹ encoder (å› ä¸º error ä¿¡æ¯æ˜¯éçº¿æ€§çš„)
- å¯é€‰: ä½¿ç”¨ SNR = flux / error ä½œä¸ºç¬¬ä¸‰é€šé“
- Error encoder éœ€è¦æ›´æ·±çš„éçº¿æ€§å±‚

### 2.3.4 æ¶æ„ D: Physics-Informed ViT Variant

**Insight æ¥æº**: Small K Limit ("å…³é”®è°±çº¿èšç„¦") + PCA å®éªŒ ("åˆ†å¸ƒå¼ç¼–ç ")

```
Input (4096) â†’ [Patch Embedding (patch_size=64, num_patches=64)]
                     â†“
              [Positional Encoding (wavelength-aware)]
                     â†“
              [Transformer Encoder (2-4 layers)]
                     â†“
              [CLS Token] â†’ Linear â†’ Output
                     â”‚
         [Linear Shortcut from Global Average]
```

**å…³é”®è®¾è®¡**:
- Patch size é€‰æ‹© ~64 åƒç´  (è¦†ç›–å•æ¡è°±çº¿)
- ä½ç½®ç¼–ç ä½¿ç”¨ç‰©ç†æ³¢é•¿è€Œéåºå·
- å…³é”®åŒºåŸŸ (Ca II, Mg I, Na I) å¯ä½¿ç”¨ä¸“é—¨çš„ patch tokens
- å±‚æ•° 2-4 å±‚å³å¯ (ä¿¡æ¯ç»´åº¦ ~200)

### 2.3.5 æ¶æ„ E: Noise-Adaptive Network

**Insight æ¥æº**: Feature Stability ("å™ªå£°å†³å®šç¨³å®šæ€§") + Ridge Î± Sweep ("æœ€ä¼˜ Î± éšå™ªå£°å˜åŒ–")

$$
\hat{y} = f_\theta(x; \hat{\sigma}), \quad \hat{\sigma} = \text{NoiseEstimator}(x)
$$

```
Input (4096) â”€â”€â†’ Noise Estimator â”€â”€â†’ ÏƒÌ‚ (estimated noise level)
      â”‚                               â”‚
      â””â”€â”€â†’ Main Network â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (noise-conditional)
                â”‚
           [Noise-dependent weight decay / dropout]
```

**å…³é”®è®¾è®¡**:
- ç½‘ç»œè‡ªåŠ¨ä¼°è®¡è¾“å…¥å™ªå£°æ°´å¹³
- æ ¹æ®ä¼°è®¡å™ªå£°è°ƒæ•´å†…éƒ¨æ­£åˆ™åŒ–
- ç±»ä¼¼äº Noise2Noise çš„æ€æƒ³

## 2.4 è¶…å‚æ•°ï¼ˆHyperparametersï¼‰

### 2.4.1 è®­ç»ƒé…ç½®ï¼ˆå›ºå®šï¼‰

```yaml
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-4
batch_size: 128

scheduler: ReduceLROnPlateau
  factor: 0.5
  patience: 10

early_stopping:
  patience: 20
  min_delta: 1e-5
  monitor: val_loss

max_epochs: 100
gradient_clip: 1.0
use_amp: true  # æ··åˆç²¾åº¦è®­ç»ƒ
seed: 42
```

### 2.4.2 é€šç”¨è¶…å‚æ•°

| å‚æ•° | å€¼/æœç´¢èŒƒå›´ | æ¥æº Insight |
|------|----------|-------------|
| Learning rate | **1e-3** (å›ºå®š) | - |
| Weight decay | **1e-4** (å›ºå®š) | Ridge: æœ€ä¼˜ Î± è·¨ 6 ä¸ªæ•°é‡çº§ |
| Batch size | **128** (å›ºå®š) | - |
| Epochs | **100** + Early Stopping (patience=20) | LGBM Sweep: æ”¶ç›Šé€’å‡ |
| Dropout | **0.1** (å›ºå®š) | - |

### 2.4.2 æ¶æ„ç‰¹å®šè¶…å‚æ•°

| æ¶æ„ | å‚æ•° | æœç´¢èŒƒå›´ | æ¥æº Insight |
|------|------|----------|-------------|
| Linear+Residual | MLP hidden dims | [256, 512], [128, 256, 512] | PCA: æœ‰æ•ˆç»´åº¦ ~200 |
| Attention | Temperature | [0.1, 1.0, 10.0] | Top-K: K=1000 æœ€ä¼˜ |
| Dual-Channel | Error encoder depth | [2, 3, 4] | Error: éçº¿æ€§å…³ç³» |
| ViT | Patch size | [32, 64, 128] | Small K: ~50 åƒç´ æœ‰æ•ˆ |
| ViT | Num layers | [2, 4, 6] | PCA: ç»´åº¦ ~200 |
| Noise-Adaptive | Noise estimator arch | [MLP, CNN] | - |

### 2.4.3 è®­ç»ƒç­–ç•¥

| ç­–ç•¥ | é…ç½® | æ¥æº Insight |
|------|------|-------------|
| **Noise Augmentation** | train_noise âˆˆ {0.0, 0.5, 1.0, 1.2} | Top-K: é«˜å™ªå£°è®­ç»ƒæ•ˆæœæœ€ä½³ |
| **Curriculum Learning** | ä»ä½å™ªå£°åˆ°é«˜å™ªå£° | Feature Stability: å™ªå£°å†³å®šç¨³å®šæ€§ |
| **Linear Warmup** | å…ˆè®­ç»ƒ shortcutï¼Œå†è®­ç»ƒ residual | Ridge: çº¿æ€§å‡ ä¹è¶³å¤Ÿ |
| **Variance-Aware Normalization** | PCA whitening æˆ– per-channel normalization | PCA: ä½æ–¹å·®æ–¹å‘é‡è¦ |

## 2.5 è¯„ä¼°æŒ‡æ ‡ä¸åŸºçº¿

### 2.5.1 åŸºçº¿æ¨¡å‹

| æ¨¡å‹ | noise=0.0 | noise=1.0 | æ¥æº |
|------|-----------|-----------|------|
| Ridge (æœ€ä¼˜ Î±) | $R^2=0.999$ | $R^2=0.45$ | Ridge Î± Sweep |
| LightGBM | $R^2=0.998$ | $R^2=0.52$ | LGBM Sweep |
| Ridge + Top-K (K=1000) | - | $R^2=0.34$ | Top-K å®éªŒ |
| Ridge (train_noise=1.2) | - | $R^2=0.47$ | Top-K å®éªŒ |

### 2.5.2 è¯„ä¼°çŸ©é˜µ

| train_noise | test_noise | ç›®æ ‡ $R^2$ | å¯¹æ ‡åŸºçº¿ |
|-------------|------------|-----------|----------|
| 0.0 | 0.0 | â‰¥ 0.999 | Ridge |
| 1.0 | 0.0 | â‰¥ 0.85 | LGBM |
| 1.0 | 1.0 | **â‰¥ 0.55** | LightGBM ($R^2=0.52$) |
| 1.2 | 1.0 | **â‰¥ 0.55** | Ridge nz1.2 ($R^2=0.47$) |
| 1.0 | 2.0 | â‰¥ 0.35 | LGBM ($R^2=0.27$) |

## 2.6 Baseline NN å®éªŒè®¡åˆ’ï¼ˆç¬¬ä¸€æ‰¹å¿…é¡»å®Œæˆçš„å®éªŒï¼‰

> æœ¬é˜¶æ®µèšç„¦äº MLP å’Œ 1D CNN çš„ baseline å®éªŒï¼Œä¸ºåç»­ Physics-Informed æ¶æ„æä¾›åŸºå‡†ã€‚
> **æ³¨æ„**ï¼šè¿™æ˜¯çº¯ç²¹çš„ baseline NNï¼Œæ²¡æœ‰ä»»ä½•èŠ±å“¨è®¾è®¡ï¼

**å™ªå£°æ°´å¹³è®¾å®š**ï¼š`noise_levels = [0.0, 1.0, 2.0]`

### å®éªŒæ€»è§ˆ

| Group | å®éªŒæ•° | éªŒè¯é—®é¢˜ | è¯´æ˜ |
|-------|--------|----------|------|
| A | 24 | Q1, Q4 | å…¨è°± MLP vs Ridge |
| B | 12 | Q2 | Top-K MLP |
| C | 6 | Q3 | CNN vs MLP |
| **æ€»è®¡** | **42** | Q1-Q5 | |

---

### å®éªŒ Group Aï¼šå…¨è°± MLP vs Ridge (24 ä¸ªå®éªŒ)

**ç›®çš„**ï¼šå¯¹åº”éªŒè¯é—®é¢˜ Q1ã€Q4 â€” éªŒè¯ NN åœ¨å…¨è°±è¾“å…¥ä¸‹èƒ½å¦åŒ¹é… Ridge

**è®¾å®š**ï¼š

| å˜é‡ | å–å€¼ |
|------|------|
| è¾“å…¥ | å…¨è°± 4096 ç»´ |
| æ¶æ„ | 2Ã—256, 2Ã—512, 3Ã—256, 3Ã—512 |
| åˆå§‹åŒ– | random, ridge |
| å™ªå£° | 0.0, 1.0, 2.0 |

**MLP æ¶æ„å˜ä½“**ï¼š

| åç§° | å±‚æ•° | éšè—å±‚é…ç½® | å‚æ•°é‡ (å…¨è°± 4096 è¾“å…¥) |
|------|------|-----------|------------------------|
| `mlp_full_2x256` | 2 | [256, 128] | ~1.1M |
| `mlp_full_2x512` | 2 | [512, 256] | ~2.2M |
| `mlp_full_3x256` | 3 | [256, 256, 128] | ~1.1M |
| `mlp_full_3x512` | 3 | [512, 512, 256] | ~2.5M |

**åˆå§‹åŒ–æ–¹å¼**ï¼š

| init_type | è¯´æ˜ |
|-----------|------|
| `random` | PyTorch é»˜è®¤åˆå§‹åŒ– |
| `ridge` | ç¬¬ä¸€å±‚ç”¨ Ridge å›å½’æƒé‡åˆå§‹åŒ– |

**å®éªŒæ•°**ï¼š4 æ¶æ„ Ã— 2 åˆå§‹åŒ– Ã— 3 å™ªå£° = **24 å®éªŒ**

**é¢„æœŸç»“æœåˆ†æ**ï¼š

| å™ªå£° | Ridge $R^2$ | MLP é¢„æœŸ | å‡è®¾ |
|------|------------|----------|------|
| 0.0 | 0.999 | ~0.99 | çº¿æ€§å·²æœ€ä¼˜ï¼ŒMLP å¯èƒ½ç•¥é€Š |
| 1.0 | 0.458 | 0.45-0.55 | MLP å¯èƒ½é€šè¿‡éçº¿æ€§å­¦åˆ°æ›´é²æ£’è¡¨ç¤º |
| 2.0 | 0.221 | 0.22-0.30 | é«˜å™ªå£°ä¸‹ NN å¯èƒ½æœ‰ä¼˜åŠ¿ |

---

### å®éªŒ Group Bï¼šTop-K MLP (12 ä¸ªå®éªŒ)

**ç›®çš„**ï¼šå¯¹åº”éªŒè¯é—®é¢˜ Q2 â€” æµ‹è¯• Top-K ç‰¹å¾é€‰æ‹©æ˜¯å¦å¯¹ NN æœ‰å¸®åŠ©

**è®¾å®š**ï¼š

| å˜é‡ | å–å€¼ |
|------|------|
| è¾“å…¥ | Top-K ç‰¹å¾ (ä» Ridge é‡è¦æ€§) |
| K å€¼ | 128, 256, 512, 1024 |
| æ¶æ„ | 2Ã—256 å›ºå®š (hidden=[256, 128]) |
| å™ªå£° | 0.0, 1.0, 2.0 |

**å®éªŒæ•°**ï¼š4 Kå€¼ Ã— 3 å™ªå£° = **12 å®éªŒ**

**å®éªŒ ID ç¤ºä¾‹**ï¼š
- `B_mlp_topk_K128_nz0.0`
- `B_mlp_topk_K256_nz1.0`
- `B_mlp_topk_K1024_nz2.0`

---

### å®éªŒ Group Cï¼šTop-K+Window CNN vs MLP (6 ä¸ªå®éªŒ)

**ç›®çš„**ï¼šå¯¹åº”éªŒè¯é—®é¢˜ Q3 â€” æµ‹è¯• CNN èƒ½å¦æ¯” MLP æ›´å¥½åœ°æ•è·å±€éƒ¨è°±çº¿ç»“æ„

**è®¾å®š**ï¼š

| å˜é‡ | å–å€¼ |
|------|------|
| K | 256 (å›ºå®š) |
| window_size | Â±8 åƒç´  |
| æ¨¡å‹ | CNN, MLP |
| å™ªå£° | 0.0, 1.0, 2.0 |

**1D CNN æ¶æ„**ï¼š

```
è¾“å…¥: (batch, 1, seq_len)
    â†“
Conv1d(1â†’32, k=7, padding=3) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2)
    â†“
Conv1d(32â†’64, k=7, padding=3) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2)
    â†“
AdaptiveAvgPool1d(1)
    â†“
Linear(64â†’128) â†’ ReLU â†’ Dropout(0.1)
    â†“
Linear(128â†’1)
```

**å®éªŒæ•°**ï¼š2 æ¨¡å‹ Ã— 3 å™ªå£° = **6 å®éªŒ**

**å®éªŒ ID ç¤ºä¾‹**ï¼š
- `C_cnn_topk_window_K256_nz0.0`
- `C_mlp_topk_window_K256_nz1.0`

---

### å®Œæ•´å®éªŒæ¸…å• (42 ä¸ª)

<details>
<summary>ğŸ“‹ ç‚¹å‡»å±•å¼€å®Œæ•´å®éªŒæ¸…å•</summary>

#### Group A: Full Spectrum MLP (24 ä¸ª)

| # | experiment_id | æ¶æ„ | åˆå§‹åŒ– | å™ªå£° |
|---|---------------|------|--------|------|
| 1 | A_mlp_2x256_rand_nz0.0 | 2Ã—256 | random | 0.0 |
| 2 | A_mlp_2x256_ridge_nz0.0 | 2Ã—256 | ridge | 0.0 |
| 3 | A_mlp_2x512_rand_nz0.0 | 2Ã—512 | random | 0.0 |
| 4 | A_mlp_2x512_ridge_nz0.0 | 2Ã—512 | ridge | 0.0 |
| 5 | A_mlp_3x256_rand_nz0.0 | 3Ã—256 | random | 0.0 |
| 6 | A_mlp_3x256_ridge_nz0.0 | 3Ã—256 | ridge | 0.0 |
| 7 | A_mlp_3x512_rand_nz0.0 | 3Ã—512 | random | 0.0 |
| 8 | A_mlp_3x512_ridge_nz0.0 | 3Ã—512 | ridge | 0.0 |
| 9-16 | ... | ... | ... | 1.0 |
| 17-24 | ... | ... | ... | 2.0 |

#### Group B: Top-K MLP (12 ä¸ª)

| # | experiment_id | K | å™ªå£° |
|---|---------------|---|------|
| 25 | B_mlp_topk_K128_nz0.0 | 128 | 0.0 |
| 26 | B_mlp_topk_K256_nz0.0 | 256 | 0.0 |
| 27 | B_mlp_topk_K512_nz0.0 | 512 | 0.0 |
| 28 | B_mlp_topk_K1024_nz0.0 | 1024 | 0.0 |
| 29-32 | ... | ... | 1.0 |
| 33-36 | ... | ... | 2.0 |

#### Group C: CNN vs MLP (6 ä¸ª)

| # | experiment_id | æ¨¡å‹ | å™ªå£° |
|---|---------------|------|------|
| 37 | C_cnn_topk_window_K256_nz0.0 | CNN | 0.0 |
| 38 | C_mlp_topk_window_K256_nz0.0 | MLP | 0.0 |
| 39 | C_cnn_topk_window_K256_nz1.0 | CNN | 1.0 |
| 40 | C_mlp_topk_window_K256_nz1.0 | MLP | 1.0 |
| 41 | C_cnn_topk_window_K256_nz2.0 | CNN | 2.0 |
| 42 | C_mlp_topk_window_K256_nz2.0 | MLP | 2.0 |

</details>

---

## 2.7 ç»“æœè®°å½•æ ¼å¼

### 2.7.1 ç»Ÿä¸€ç»“æœè¡¨

**è·¯å¾„**: `results/nn_baselines/nn_vs_ml_results.csv`

æ¯ä¸€è¡ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| åˆ—å | è¯´æ˜ |
|------|------|
| `experiment_id` | å”¯ä¸€æ ‡è¯† (å¦‚ `A_mlp_2x256_rand_nz0.0`) |
| `experiment_group` | A, B, æˆ– C |
| `model_family` | MLP æˆ– CNN |
| `model_name` | æ¨¡å‹åç§° |
| `init_type` | random æˆ– ridge |
| `input_type` | full_spectrum, topk, topk_window |
| `K` | Top-K å€¼ (null = å…¨è°±) |
| `noise_level` | å™ªå£°æ°´å¹³ |
| `test_R2`, `test_MAE`, `test_RMSE` | **æµ‹è¯•é›†æŒ‡æ ‡** |
| `epochs_to_best` | æœ€ä½³ epoch |
| `training_time_sec` | è®­ç»ƒæ—¶é—´ |
| `num_params` | å‚æ•°é‡ |

### 2.7.2 è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| **$R^2$** | $1 - SS_{res}/SS_{tot}$ | ä¸»è¦æŒ‡æ ‡ï¼Œè¶Šé«˜è¶Šå¥½ |
| **MAE** | $\text{mean}(\|y - \hat{y}\|)$ | å¹³å‡ç»å¯¹è¯¯å·® |
| **RMSE** | $\sqrt{\text{mean}((y - \hat{y})^2)}$ | å‡æ–¹æ ¹è¯¯å·® |

### 2.7.3 è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š

```bash
python scripts/summarize_nn_results.py
```

è¾“å‡º: `results/nn_baselines/NN_BASELINE_REPORT.md`

å†…å®¹åŒ…æ‹¬:
- æŒ‰ noise_level åˆ†ç»„çš„ç»“æœè¡¨
- "NN ç›¸å¯¹ Ridge/LightGBM çš„ $\Delta R^2$" å¯è§†åŒ–æˆ–è¡¨æ ¼
- å¯¹åº” Q1â€“Q5 çš„ç®€çŸ­æ–‡å­—æ€»ç»“ï¼ˆâœ…/âŒï¼‰

## 2.8 è¿è¡Œå®éªŒ

### 2.8.1 ç¯å¢ƒå‡†å¤‡

```bash
cd /home/swei20/VIT
source init.sh
```

### 2.8.2 é¢„è§ˆå®éªŒï¼ˆDry Runï¼‰

```bash
python scripts/run_nn_baselines.py --dry-run
```

### 2.8.3 å¹¶è¡Œè¿è¡Œ (8 GPU æœ€å¿«)

```bash
# ä½¿ç”¨æ‰€æœ‰ GPU
python scripts/run_nn_baselines.py --parallel

# æŒ‡å®š GPU
python scripts/run_nn_baselines.py --parallel --gpus 0,1,2,3,4,5,6,7
```

### 2.8.4 è¿è¡Œç‰¹å®š Group

```bash
python scripts/run_nn_baselines.py --parallel -e A      # åªè·‘ Group A
python scripts/run_nn_baselines.py --parallel -e A,B    # è·‘ A å’Œ B
```

### 2.8.5 å¿«é€Ÿæµ‹è¯•

```bash
python scripts/run_nn_baselines.py --parallel --num-train 4000 --epochs 10 -e A
```

---

# 3. ğŸ“Š å®éªŒå›¾è¡¨

> å®éªŒå®Œæˆåå¡«å†™

### å›¾ 1ï¼š[TODO]

### å›¾ 2ï¼š[TODO]

---

# 4. ğŸ’¡ å…³é”®æ´è§ï¼ˆKey Insightsï¼‰

> å®éªŒå®Œæˆåå¡«å†™

### 4.1 å®è§‚å±‚æ´è§

TODO

### 4.2 æ¨¡å‹å±‚æ´è§

TODO

### 4.3 å®éªŒå±‚ç»†èŠ‚æ´è§

TODO

---

# 5. ğŸ“ ç»“è®ºï¼ˆConclusionï¼‰

> å®éªŒå®Œæˆåå¡«å†™

## 5.1 æ ¸å¿ƒå‘ç°

TODO

## 5.2 å…³é”®ç»“è®º

TODO

## 5.3 è®¾è®¡å¯ç¤º

TODO

## 5.4 ç‰©ç†è§£é‡Š

TODO

## 5.5 å…³é”®æ•°å­—é€ŸæŸ¥

TODO

## 5.6 ä¸‹ä¸€æ­¥å·¥ä½œ

TODO

---

# 6. ğŸ“ é™„å½•

## 6.1 å®éªŒä¼˜å…ˆçº§æ’åº

åŸºäºå‰æœŸå®éªŒ insights çš„ç½®ä¿¡åº¦å’Œé¢„æœŸæ”¶ç›Šï¼Œæ¨èä»¥ä¸‹å®éªŒä¼˜å…ˆçº§ï¼š

### ğŸ”´ é«˜ä¼˜å…ˆçº§ (Must Do)

| ä¼˜å…ˆçº§ | å®éªŒ | é¢„æœŸæ”¶ç›Š | æ”¯æ’‘ Insight |
|--------|------|----------|-------------|
| P0 | **Linear + Residual (æ¶æ„ A)** | éªŒè¯"çº¿æ€§æœ¬è´¨"æ ¸å¿ƒå‡è®¾ | Ridge: $R^2=0.999$ @ noise=0 |
| P0 | **Noise Augmentation** | æå‡å™ªå£°é²æ£’æ€§ | Top-K: é«˜å™ªå£°è®­ç»ƒæ•ˆæœæ˜¯ Top-K çš„ 12 å€ |
| P0 | **Dual-Channel (æ¶æ„ C)** | åˆ©ç”¨ error ä¿¡æ¯ | Error: $R^2=0.91$ from Ïƒ only |

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ (Should Do)

| ä¼˜å…ˆçº§ | å®éªŒ | é¢„æœŸæ”¶ç›Š | æ”¯æ’‘ Insight |
|--------|------|----------|-------------|
| P1 | **Attention-based (æ¶æ„ B)** | éšå¼ç‰¹å¾é€‰æ‹© | Top-K: K=1000 (24%) è¶³å¤Ÿ |
| P1 | **Variance-Aware Normalization** | ä¿æŠ¤ä½æ–¹å·®ä¿¡å· | PCA: ä¿¡æ¯åœ¨ä½æ–¹å·® PC |
| P1 | **ViT (æ¶æ„ D)** | æ•è·å±€éƒ¨è°±çº¿ç»“æ„ | Small K: å…³é”®è°±çº¿èšç„¦ |

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ (Nice to Have)

| ä¼˜å…ˆçº§ | å®éªŒ | é¢„æœŸæ”¶ç›Š | æ”¯æ’‘ Insight |
|--------|------|----------|-------------|
| P2 | **Noise-Adaptive (æ¶æ„ E)** | è‡ªé€‚åº”æ­£åˆ™åŒ– | Ridge: æœ€ä¼˜ Î± éšå™ªå£°å˜åŒ– |
| P2 | **Physical Positional Encoding** | ç‰©ç†å¯è§£é‡Šæ€§ | Small K: ç‰¹å®šæ³¢é•¿é‡è¦ |
| P2 | **Ensemble (Linear + NN)** | ç¨³å®šæ€§æå‡ | - |

## 6.2 é¢„æœŸé£é™©ä¸ç¼“è§£

| é£é™© | å¯èƒ½åŸå›  | ç¼“è§£ç­–ç•¥ |
|------|---------|----------|
| Linear shortcut ä¸»å¯¼ï¼Œæ®‹å·®æ— è´¡çŒ® | éçº¿æ€§æˆåˆ†ç¡®å®å¾ˆå° | åˆ†ææ®‹å·®åˆ†å¸ƒï¼Œå¦‚ç¡®å®å¾ˆå°åˆ™æ¥å— |
| Attention æƒé‡ä¸èšç„¦äºç‰©ç†è°±çº¿ | æ•°æ®é©±åŠ¨çš„"æ·å¾„" | æ·»åŠ ç‰©ç†çº¦æŸ (Ca II, Mg I åŒºåŸŸ prior) |
| åŒé€šé“æ²¡æœ‰é¢å¤–æå‡ | Error ä¿¡æ¯å·²è¢« flux éšå¼ç¼–ç  | æ¶ˆèå®éªŒç¡®è®¤ |
| æ— æ³•è¶…è¶Š LightGBM | æ ‘æ¨¡å‹çš„ç»„åˆä¼˜åŒ–æ›´å¼º | å°è¯•æ›´æ·±çš„ç½‘ç»œæˆ–é›†æˆæ–¹æ³• |

## 6.3 ä»£ç æ¡†æ¶å»ºè®®

```python
# æ ¸å¿ƒæ¶æ„å®ç°éª¨æ¶

class LinearResidualNet(nn.Module):
    """æ¶æ„ A: Linear + Residual MLP"""
    def __init__(self, input_dim=4096, hidden_dims=[512, 128, 32]):
        super().__init__()
        # Linear shortcut (å¯åˆå§‹åŒ–ä¸º Ridge è§£)
        self.linear = nn.Linear(input_dim, 1)
        
        # Residual MLP
        layers = []
        prev_dim = input_dim
        for dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = dim
        layers.append(nn.Linear(prev_dim, 1))
        self.residual = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.linear(x) + self.residual(x)


class AttentionNet(nn.Module):
    """æ¶æ„ B: Attention-based Feature Selection"""
    def __init__(self, input_dim=4096, hidden_dim=256):
        super().__init__()
        # Learnable attention weights
        self.attention = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Softmax(dim=-1)
        )
        # Linear shortcut
        self.linear = nn.Linear(input_dim, 1)
        # MLP on weighted features
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self, x):
        attn = self.attention(x)  # (B, D)
        weighted = x * attn       # (B, D)
        return self.linear(x) + self.mlp(weighted)


class DualChannelNet(nn.Module):
    """æ¶æ„ C: Dual-Channel (Flux + Error)"""
    def __init__(self, input_dim=4096, hidden_dim=256):
        super().__init__()
        # Flux encoder (æµ…å±‚ï¼Œå› ä¸ºçº¿æ€§ä¸ºä¸»)
        self.flux_enc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        # Error encoder (æ·±å±‚ï¼Œå› ä¸ºéçº¿æ€§)
        self.error_enc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        # Linear shortcut (flux only)
        self.linear = nn.Linear(input_dim, 1)
    
    def forward(self, flux, error):
        flux_feat = self.flux_enc(flux)
        error_feat = self.error_enc(error)
        fused = torch.cat([flux_feat, error_feat], dim=-1)
        return self.linear(flux) + self.fusion(fused)
```

## 6.4 ä»£ç æ–‡ä»¶ç»“æ„

```
/home/swei20/VIT/
â”œâ”€â”€ src/nn/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mlp.py            # MLP æ¨¡å‹ (æ”¯æŒ Ridge åˆå§‹åŒ–)
â”‚   â”‚   â””â”€â”€ cnn1d.py          # CNN1D + TopKWindowCNN
â”‚   â”œâ”€â”€ data_adapter.py       # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ baseline_trainer.py   # train_and_evaluate
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_nn_baselines.py       # ä¸»è„šæœ¬ (æ”¯æŒå¤š GPU å¹¶è¡Œ)
â”‚   â””â”€â”€ summarize_nn_results.py   # æŠ¥å‘Šç”Ÿæˆ
â”‚
â”œâ”€â”€ results/nn_baselines/
â”‚   â”œâ”€â”€ nn_vs_ml_results.csv      # ç»Ÿä¸€ç»“æœè¡¨
â”‚   â””â”€â”€ NN_BASELINE_REPORT.md     # è‡ªåŠ¨ç”Ÿæˆçš„æŠ¥å‘Š
â”‚
â””â”€â”€ docs/
    â””â”€â”€ NN_BASELINE_EXPERIMENTS.md # å®éªŒè®¾è®¡æ–‡æ¡£
```

## 6.5 Baseline MLP/CNN ä»£ç ç¤ºä¾‹

```python
class MLP(nn.Module):
    """Baseline MLP for log g prediction"""
    def __init__(self, input_dim=4096, hidden_sizes=[256, 128], 
                 activation='relu', dropout=0.1):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_sizes:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU() if activation == 'relu' else nn.GELU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)
    
    def init_from_ridge(self, ridge_weights, ridge_bias):
        """Initialize first layer from Ridge regression solution"""
        with torch.no_grad():
            # Expand ridge weights to first hidden layer
            first_linear = self.network[0]
            hidden_dim = first_linear.weight.shape[0]
            # Simple approach: tile the ridge solution
            first_linear.weight.data = ridge_weights.unsqueeze(0).repeat(hidden_dim, 1)
            first_linear.bias.data = ridge_bias.repeat(hidden_dim)


class CNN1D(nn.Module):
    """1D CNN for local spectral feature extraction"""
    def __init__(self, input_channels=1, seq_len=4096, 
                 channels=[32, 64], kernel_size=5, fc_dims=[128]):
        super().__init__()
        # Conv layers
        conv_layers = []
        prev_ch = input_channels
        for ch in channels:
            conv_layers.extend([
                nn.Conv1d(prev_ch, ch, kernel_size, padding=kernel_size//2),
                nn.ReLU(),
                nn.MaxPool1d(2)
            ])
            prev_ch = ch
        self.conv = nn.Sequential(*conv_layers)
        
        # Calculate flattened size
        with torch.no_grad():
            dummy = torch.zeros(1, input_channels, seq_len)
            conv_out = self.conv(dummy)
            flat_size = conv_out.view(1, -1).shape[1]
        
        # FC layers
        fc_layers = []
        prev_dim = flat_size
        for dim in fc_dims:
            fc_layers.extend([nn.Linear(prev_dim, dim), nn.ReLU()])
            prev_dim = dim
        fc_layers.append(nn.Linear(prev_dim, 1))
        self.fc = nn.Sequential(*fc_layers)
    
    def forward(self, x):
        # x: (batch, seq_len) -> (batch, 1, seq_len)
        if x.dim() == 2:
            x = x.unsqueeze(1)
        conv_out = self.conv(x)
        flat = conv_out.view(conv_out.shape[0], -1)
        return self.fc(flat)
```

## 6.6 Coding Agent Promptï¼ˆç”¨äºè‡ªåŠ¨åŒ–å®éªŒï¼‰

<details>
<summary>ğŸ“‹ ç‚¹å‡»å±•å¼€å®Œæ•´ Prompt</summary>

```text
ä½ æ˜¯ä¸€ä¸ªç†Ÿæ‚‰ Pythonã€PyTorch å’Œç§‘å­¦è®¡ç®—çš„ coding agentã€‚  
ç°åœ¨è¯·ä½ åŸºäºæˆ‘ç°æœ‰çš„ Ridge Regression / LightGBM åˆ†æç»“æœï¼Œæ­å»ºå¹¶è¿è¡Œä¸€ç»„ç¥ç»ç½‘ç»œ baseline å®éªŒï¼ˆMLP å’Œ 1D CNNï¼‰ï¼Œç”¨äºé¢„æµ‹å…‰è°±çš„ log_gï¼Œå¹¶ç³»ç»Ÿå¯¹æ¯” NN ä¸ä¼ ç»Ÿ ML çš„è¡¨ç°ã€‚

è¯·æ³¨æ„ï¼š  
- ç›®æ ‡æ˜¯ **ä»ç®€å• NN å¼€å§‹ï¼Œé€æ­¥å¤æ‚åŒ–ï¼ˆæœªæ¥ä¼šæ‰©å±•åˆ° Vision Transformerï¼‰**ã€‚è¿™ä¸€è½®åªåš MLP/CNNã€‚  
- å®éªŒè®¾è®¡è¦å’Œæˆ‘ç°æœ‰çš„ Ridge/LightGBM å®éªŒé«˜åº¦å¯¹é½ï¼šç›¸åŒçš„æ•°æ®åˆ†å‰²ã€å™ªå£°è®¾å®šå’Œè¯„ä¼°æŒ‡æ ‡ã€‚  
- æ‰€æœ‰ç»“æœè¦ç»“æ„åŒ–è¾“å‡ºï¼ˆä¾‹å¦‚ CSV/Parquet + Markdown æ±‡æ€»è¡¨ï¼‰ï¼Œä¾¿äºåç»­åˆ†æã€‚

--------------------------------
ä¸€ã€æ•°æ®ä¸å·²æœ‰ç»“æœï¼ˆè¯·å¤ç”¨ï¼‰
--------------------------------

1. æ•°æ®æ ¼å¼
   - è¾“å…¥ï¼šå…‰è°± flux å‘é‡ï¼Œç»´åº¦çº¦ä¸º 4096
   - è¾“å‡ºï¼šæ ‡é‡æ ‡ç­¾ log_g
   - å¯èƒ½è¿˜å­˜åœ¨æ¯ä¸ªåƒç´ çš„ error / noise æ–¹å·®å‘é‡

2. æ•°æ®åŠ è½½
   - å¤ç”¨/å°è£…ç°æœ‰çš„ data loader / preprocessing
   - ä½¿ç”¨ä¸ Ridge/LightGBM ç›¸åŒçš„ train/valid/test åˆ’åˆ†
   - ç›¸åŒçš„æ ‡å‡†åŒ–æ–¹å¼ï¼ˆæŒ‰è®­ç»ƒé›†ç»Ÿè®¡å¯¹ flux åšæ ‡å‡†åŒ–ï¼‰

3. ä¼ ç»Ÿ ML baseline
   - åŠ è½½ Ridge/LightGBM çš„è¯„ä¼°ç»“æœ
   - åœ¨ NN çš„ç»“æœè¡¨ä¸­åŒæ—¶æ˜¾ç¤ºå¯¹åº”å™ªå£°æ°´å¹³ä¸‹çš„ Ridge/LightGBM æŒ‡æ ‡

4. Top-K ç‰¹å¾ä¿¡æ¯
   - åŠ è½½ feature importance CSVï¼ŒæŒ‰ importance æ’åºåå– Top-K

--------------------------------
äºŒã€éœ€è¦å®ç°çš„æ¨¡å‹
--------------------------------

1. MLP æ¨¡å‹ï¼ˆå…¨è¿æ¥ç½‘ç»œï¼‰
   - å¯é…ç½®ï¼šhidden_sizes, num_layers, activation, dropout
   - æ”¯æŒä¸¤ç§åˆå§‹åŒ–ï¼šRandom init / Ridge åˆå§‹åŒ–ç¬¬ä¸€å±‚

2. 1D CNN æ¨¡å‹
   - è§†å…‰è°±ä¸ºä¸€ç»´åºåˆ— [batch_size, 1, seq_len]
   - å¯é…ç½®ï¼šnum_conv_layers, channels, kernel_size, stride/pooling

3. ç»Ÿä¸€æ¥å£
   - train_and_evaluate(model, train_loader, valid_loader, test_loader, config)
   - ä½¿ç”¨ MSELossï¼Œearly stopping (patience=20)
   - è¿”å› RÂ², MAE, RMSE å’Œè®­ç»ƒæ—¥å¿—

--------------------------------
ä¸‰ã€å®éªŒè®¾è®¡
--------------------------------

noise_levels = [0.0, 0.5, 1.0, 2.0]

ã€å®éªŒ Aï¼šå…¨è°± MLP vs Ridgeã€‘
- è¾“å…¥ï¼šæ ‡å‡†åŒ–åçš„å…¨è°± flux
- MLP: num_layers âˆˆ {2, 3}, hidden_size âˆˆ {256, 512}
- åˆå§‹åŒ–ï¼šRandom / Ridge-init
- ä¸ Ridge baseline å¯¹æ¯”

ã€å®éªŒ Bï¼šTop-K MLP vs å…¨è°± MLPã€‘
- Top-K = {128, 256, 512, 1024}
- å¯¹ç…§ï¼šRandom-Kï¼ˆç›¸åŒç»´åº¦ï¼Œéšæœºé€‰æ‹©ï¼‰
- MLP å›ºå®š 3 å±‚ï¼Œhidden_size=256

ã€å®éªŒ Cï¼šTop-K+å±€éƒ¨çª—å£ CNN vs MLPã€‘
- æ¯ä¸ª Top-K æ³¢é•¿å– Â±8 åƒç´ çª—å£
- CNN: 2 conv layers, channels=[32,64], kernel=5æˆ–7
- å¯¹ç…§ï¼šç›¸è¿‘å‚æ•°é‡çš„ MLP

ã€å®éªŒ Dï¼šRidge åˆå§‹åŒ–æ•ˆæœã€‘
- å¯¹æ¯” Random init vs Ridge-init
- è®°å½• epochs_to_best å’Œæ”¶æ•›æ›²çº¿

--------------------------------
å››ã€ç»“æœè®°å½•
--------------------------------

è¾“å‡º nn_vs_ml_results.csvï¼Œå­—æ®µåŒ…æ‹¬ï¼š
model_family, model_name, init_type, input_type, K, noise_level,
train_R2, valid_R2, test_R2, train_MAE, valid_MAE, test_MAE,
train_RMSE, valid_RMSE, test_RMSE, epochs_to_best

è‡ªåŠ¨ç”Ÿæˆ Markdown æŠ¥å‘Šç‰‡æ®µ
```

</details>

## 6.7 ç›¸å…³æ–‡ä»¶

| ç±»å‹ | è·¯å¾„ |
|------|------|
| **NN å®éªŒè®¾è®¡æ–‡æ¡£** | `/home/swei20/VIT/docs/NN_BASELINE_EXPERIMENTS.md` |
| Ridge å®éªŒ | `logg/ridge/exp_ridge_alpha_sweep_20251127.md` |
| Ridge Top-K å®éªŒ | `logg/ridge/exp_ridge_topk_20251129.md` |
| PCA å®éªŒ | `logg/pca/exp_pca_linear_regression_20251128.md` |
| Top-K å®éªŒ | `logg/noise/exp_noise_topk_feature_selection_20251128.md` |
| Small K å®éªŒ | `logg/noise/exp_small_k_limit_20251129.md` |
| Error å®éªŒ | `logg/ridge/exp_error_logg_20251127.md` |
| Feature Stability | `logg/ridge/exp_feature_importance_stability_20251128.md` |
| LightGBM Sweep | `logg/lightgbm/exp_lightgbm_hyperparam_sweep_20251129.md` |
| LGBM vs Ridge Top-K | `logg/noise/exp_topk_feature_selection_lgbm_vs_ridge_20251129.md` |

---

*æŠ¥å‘Šåˆ›å»ºæ—¶é—´: 2025-11-29*  
*æ›´æ–°æ—¶é—´: 2025-11-29 (æ•´åˆè¯¦ç»†å®éªŒè®¾è®¡ from VIT/docs/NN_BASELINE_EXPERIMENTS.md)*  
*åŸºäº 8 ä»½å‰æœŸå®éªŒæŠ¥å‘Šçš„ insights è®¾è®¡*  
*å®éªŒæ€»æ•°: 42 ä¸ª (Group A: 24, Group B: 12, Group C: 6)*  
*å®éªŒé˜¶æ®µ: Phase 1 - Baseline MLP/CNN â†’ Phase 2 - Physics-Informed Architectures â†’ Phase 3 - Vision Transformer*


# SpecViT-Scale: Scaling Vision Transformers for Stellar Spectra Regression Towards the CramÃ©râ€“Rao Bound

> **Status:** ðŸ“ Draft (v2)  
> **Date:** 2025-12-27  
> **Authors:** Viska Wei et al.

---

## Title Options

1. **SpecViT-Scale: Scaling Vision Transformers for Stellar Spectra Regression Towards the CramÃ©râ€“Rao Bound**
2. Approaching the Fisher/CRLB Limit for Stellar log(g) Inference with Vision Transformers
3. Physics-Informed Tokenization Enables Data-Scaled Vision Transformers for Stellar Spectral Inference

---

## Abstract

Large-scale spectroscopic surveys require rapid and robust stellar parameter inference; noise and line blending limit traditional template fitting and shallow machine learning approaches. We propose and systematically evaluate SpecViT, a Vision Transformer-based 1D spectral regression framework with spectra-tailored tokenization (Conv1D/Sliding Window patch embedding, chunk normalization, and optional physics-informed positional encoding) trained under heteroscedastic noise conditions.

**Key Contribution 1 (Scaling):** We demonstrate that **data scale is the critical bottleneck for ViT success**; on 1M BOSZ synthetic spectra, log(g) regression achieves **RÂ² = 0.711 (test, 10k samples)** with MAE â‰ˆ 0.37 dex, significantly outperforming tree-based methods (LightGBM: RÂ² = 0.614, +16%) and template fitting (RÂ² = 0.404, +76%).

**Key Contribution 2 (Theoretical Ceiling):** We introduce the Fisher Information / CramÃ©râ€“Rao Lower Bound (CRLB) to establish the **SNR-conditioned theoretical upper limit** for any unbiased estimator. The 5D marginalized ceiling (accounting for Teff, [M/H], and chemical abundances) gives RÂ²_max = 0.874 at mag=21.5 (SNRâ‰ˆ7).

**Results:** SpecViT approaches the theoretical ceiling across all SNR regimes: at mag=22.0 (SNRâ‰ˆ5), ViT achieves RÂ² = 0.68 vs. ceiling RÂ²_max = 0.70, with gap = 0.02â€”demonstrating that the model extracts nearly all available information under challenging noise conditions. The remaining gap at higher SNR is attributed to tokenization design and model capacity rather than data noise itself.

---

## 1. Introduction

### 1.1 Astronomical Motivation

Large-scale spectroscopic surveys (SDSS, LAMOST, Gaia, DESI, PFS) are producing millions of stellar spectra, requiring automated and robust stellar parameter estimation. Traditional physics-based template fitting and spectral line fitting methods are computationally expensive and sensitive to noise, line blending, and template coverage limitations.

### 1.2 Machine Learning for Spectra

Classical ML approaches (Random Forest, Gradient Boosted Trees, The Cannon, The Payne) have shown success but rely on hand-crafted features or assume specific functional forms. Deep learning methods (1D CNNs, autoencoders) capture local patterns but may miss long-range spectral dependencies.

### 1.3 The Transformer Opportunity

Vision Transformers (ViT) have revolutionized image processing through global self-attention, but their application to 1D spectra faces unique challenges:
- **Spectra â‰  Images**: 1D continuous signals with physically meaningful wavelength coordinates
- **Tokenization matters**: Patch embedding must preserve spectral line structure and continuity
- **Data hunger**: Transformers lack the inductive bias of CNNs, requiring larger datasets

### 1.4 Our Contributions

1. **SpecViT Framework**: A Vision Transformer architecture adapted for 1D stellar spectra with tailored tokenization (C1D/SW patch embedding), positional encoding, and regression head.

2. **Scaling Law Discovery**: We demonstrate that **data scale is the key to ViT success** for stellar parameter inference. With 1M training spectra, ViT achieves RÂ² = 0.711 (test) for log(g) prediction, outperforming LightGBM by +16% and template fitting by +76%. ViT first surpasses LightGBM at 100k samples, with a scaling slope 2.2Ã— that of traditional ML.

3. **Fisher/CRLB Theoretical Ceiling**: We derive the SNR-conditioned theoretical upper bound using 5D Fisher Information (marginalizing over Teff, [M/H], Î±_M, C_M), providing RÂ²_max = 0.874 at mag=21.5 as a physics-grounded benchmark.

4. **Gap Analysis**: We quantify the gap between SpecViT and the theoretical ceiling across all SNR regimes. At mag=22.0 (SNRâ‰ˆ5), the gap shrinks to just 0.02, demonstrating that SpecViT approaches physical limits.

---

## 2. Related Work

### 2.1 Physics-Based Methods

- **Template Fitting**: Ï‡Â² minimization against synthetic spectral libraries (BOSZ, PHOENIX, Kurucz)
- **Spectral Line Fitting**: Direct measurement of equivalent widths and line ratios
- **Limitations**: Computational cost, template coverage, line blending, noise sensitivity

### 2.2 Classical Machine Learning

- **PCA + Regression**: Dimensionality reduction followed by Ridge/LASSO regression
- **Random Forest / Gradient Boosted Trees**: Feature importance and non-linear mapping
- **The Cannon / The Payne**: Data-driven models with physical priors
- **Limitations**: Feature engineering, limited capacity for complex patterns

### 2.3 Deep Learning for Spectra

- **1D CNNs**: Local receptive fields, translation invariance
- **Autoencoders**: Unsupervised feature learning
- **Transformers for Spectra**: Emerging work on attention mechanisms for spectral analysis
- **Key insight**: Tokenization and positional encoding are critical engineering choices

---

## 3. Problem Setup & Data

### 3.1 Task Definition

**Primary Task:** Stellar surface gravity (log g) regression from 1D spectra.

**Metrics:**
- $R^2$ (coefficient of determination): scale-invariant, comparable across normalization schemes
- MAE (Mean Absolute Error): in original log(g) space (dex)

### 3.2 Dataset

| Property | Value |
|----------|-------|
| Source | BOSZ synthetic spectra (Bohlin et al. 2017) |
| Wavelength Range | 7100â€“8850 Ã… (MR arm) |
| Input Dimension | 4096 pixels |
| Parameter Ranges | Teff: 3750â€“6000 K, log g: 1â€“5, [M/H]: -0.25â€“0.75 |
| Magnitude Range | 20.5â€“22.5 mag |
| **Training Set** | **1,000,000** spectra (5 shards Ã— 200k) |
| Validation Set | 1,000 spectra |
| Test Set | 10,000 spectra |

### 3.3 Noise Model

We employ heteroscedastic Gaussian noise modeling the Poisson statistics of photon counting:

$$\text{noisy\_flux}_i = \text{flux}_i + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, (\text{noise\_level} \times \text{error}_i)^2)$$

where $\text{error}_i$ is the per-pixel error estimate from the noise model.

**Training:** On-the-fly noise injection (data augmentation, ~200Ã— effective samples)
**Validation/Test:** Pre-generated with fixed seed for reproducibility

---

## 4. Method: SpecViT

### 4.1 Overview

```
Input Spectrum (4096,)
       â†“
[Patch Embedding / Tokenization]
   â”œâ”€â”€ Tokenizer: C1D or SW â†’ (batch, 256, patch_size=16)
   â”œâ”€â”€ Linear Projection â†’ (batch, 256, hidden_size=256)
   â”œâ”€â”€ Prepend CLS Token â†’ (batch, 257, 256)
   â””â”€â”€ Add Position Embeddings â†’ (batch, 257, 256)
       â†“
[ViT Encoder] Ã— 6 layers
   â”œâ”€â”€ Multi-Head Self-Attention (8 heads)
   â”œâ”€â”€ LayerNorm + Residual
   â”œâ”€â”€ FFN (hidden â†’ 4Ã—hidden â†’ hidden)
   â””â”€â”€ LayerNorm + Residual
       â†“
[CLS Token Extraction]
   â””â”€â”€ outputs[:, 0, :] â†’ (batch, 256)
       â†“
[Regression Head]
   â””â”€â”€ Linear(256, 1) â†’ log_g prediction
```

### 4.2 Tokenization / Patch Embedding

We explore two tokenization strategies for converting 1D spectra into token sequences:

| Method | Implementation | Formula | Properties |
|--------|---------------|---------|------------|
| **C1D** (Conv1D) | `nn.Conv1d(1, 256, kernel=16, stride=16)` | $y_i = W * x[i \cdot s : i \cdot s + k]$ | Learned local features, parameter sharing |
| **SW** (Sliding Window) | `unfold() + nn.Linear(16, 256)` | $y_i = W \cdot x[i \cdot s : i \cdot s + k] + b$ | Linear projection, simpler |

With patch_size=16 and stride=16, we obtain 256 tokens from 4096 wavelength pixels.

### 4.3 Positional Encoding

We use learned positional embeddings:

$$\text{tokens} = \text{tokens} + \text{PE}, \quad \text{PE} \in \mathbb{R}^{(257 \times 256)}$$

*Future work: Physics-Informed Positional Encoding (PIPE) using wavelength coordinates*

### 4.4 Transformer Encoder

Standard ViT encoder with:
- $L = 6$ layers
- $d_{\text{model}} = 256$ (hidden size)
- $h = 8$ attention heads
- $d_k = 32$ (head dimension)
- FFN intermediate size: 1024

**Attention mechanism:**
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

### 4.5 Training Objective

**Loss Functions:**
- MSE: $L = \frac{1}{n}\sum(y - \hat{y})^2$
- L1/MAE: $L = \frac{1}{n}\sum|y - \hat{y}|$

**Label Normalization:**
- Standard (z-score): $\tilde{y} = \frac{y - \mu}{\sigma}$
- MinMax: $\tilde{y} = \frac{y - y_{\min}}{y_{\max} - y_{\min}}$

**Note:** $R^2$ is invariant to linear transformations, allowing direct comparison across normalization schemes.

**Training Configuration:**
| Parameter | Value |
|-----------|-------|
| Optimizer | AdamW |
| Learning Rate | 3e-4 |
| Weight Decay | 0.01 |
| LR Schedule | Cosine Annealing (Î·_min=1e-5) |
| Epochs | 200 |
| Batch Size | 256 |
| Precision | Mixed (FP16) |
| Gradient Clipping | 0.5 |

---

## 5. Theoretical Ceiling via Fisher Information / CRLB

### 5.1 Fisher Information from Spectrum Jacobian

For a spectral model $f(\theta)$ with parameters $\theta = (T_{\text{eff}}, \log g, [M/H], ...)$ and heteroscedastic noise covariance $\Sigma$, the Fisher Information Matrix is:

$$I(\theta) = J^\top \Sigma^{-1} J$$

where $J = \frac{\partial f}{\partial \theta}$ is the Jacobian of the spectrum with respect to parameters.

### 5.2 Marginalized CRLB for log(g)

Using the Schur complement to marginalize over nuisance parameters $\eta = (T_{\text{eff}}, [M/H], ...)$:

$$\text{CRLB}_{g,\text{marg}} = \left(I_{gg} - I_{g\eta} I_{\eta\eta}^{-1} I_{\eta g}\right)^{-1}$$

### 5.3 CRLB to RÂ² Conversion

$$R^2_{\max} = 1 - \frac{\text{CRLB}_{g,\text{marg}}}{\text{Var}(\log g)}$$

This gives the theoretical maximum $R^2$ achievable by any unbiased estimator under the given noise conditions.

### 5.4 Key Results

| Magnitude | SNR | $R^2_{\max}$ (5D) | Interpretation |
|-----------|-----|-------------------|----------------|
| 18.0 | 87.4 | 0.999 | Near-perfect estimation |
| 20.0 | 24.0 | 0.989 | Excellent |
| 21.5 | 7.1 | **0.874** | Good (main comparison point) |
| 22.0 | 4.6 | 0.698 | Moderate |
| 22.5 | 3.0 | 0.265 | Challenging |
| 23.0 | 1.9 | 0.000 | Information cliff |

**Critical insights:**
1. The Schur decay factor (~0.58 for 5D, ~0.69 for 3D) is constant across SNR, indicating that parameter degeneracy is a physics property, not noise-dependent.
2. The 5D ceiling (including chemical abundances Î±_M, C_M) is 1-28% lower than 3D ceiling, with the gap increasing at low SNR.
3. At mag=22.0, the 5D ceiling (0.698) nearly matches ViT performance (0.68), demonstrating that the model approaches physical limits.

---

## 6. Experiments

### 6.1 Baselines

| Model | Type | Configuration |
|-------|------|---------------|
| LightGBM | Tree-based | Raw 4096-dim input |
| Ridge | Linear | Î±=1e5 |
| Template Fitting | Physics | Ï‡Â² minimization |
| 1D CNN | Deep Learning | (to be added) |

### 6.2 Main Results

#### 6.2.1 ViT 1M Performance (Test Set)

| Metric | Value | Condition |
|--------|-------|-----------|
| **RÂ² (test, 10k)** | **0.711** | Best checkpoint (epoch 128) |
| MAE (original space) | 0.372 dex | In log(g) units |
| Architecture | p16_h256_L6_a8 | Sweep-optimized |
| Parameters | ~4.9M | - |

#### 6.2.2 vs Baselines (Same Train/Test Split)

| Model | Data Size | RÂ² (test) | Î” vs ViT | Gap to Ceiling |
|-------|-----------|-----------|----------|----------------|
| **SpecViT** | 1M | **0.711** | - | **0.163** |
| LightGBM | 1M | 0.614 | -0.097 (-14%) | 0.260 |
| Template Fitting | - | 0.404 | -0.307 (-43%) | 0.470 |
| Ridge | 1M | 0.50 | -0.211 (-30%) | 0.374 |
| **Fisher 5D Ceiling** | - | **0.874** | - | **0** |

**Key finding:** ViT outperforms LightGBM by **+16%** and Template Fitting by **+76%** on the same test set.

### 6.3 Per-SNR Performance Comparison

| Magnitude | SNR | ViT RÂ² | LightGBM RÂ² | Ceiling RÂ² | ViT Gap |
|-----------|-----|--------|-------------|------------|---------|
| 18.0 | 87 | ~0.99 | ~0.84 | 0.999 | ~0.01 |
| 20.0 | 24 | 0.90 | 0.87 | 0.989 | 0.09 |
| 21.5 | 7.1 | 0.80 | 0.74 | 0.874 | 0.07 |
| 22.0 | 4.6 | 0.68 | 0.60 | 0.698 | **0.02** |
| 22.5 | 3.0 | 0.52 | 0.42 | 0.265 | ViT>ceiling* |
| 23.0 | 1.9 | ~0.46 | ~0.36 | 0.000 | N/A |

*Note: At mag=22.5, ViT (0.52) > 5D median ceiling (0.265) because the ceiling is computed as median; high-percentile regions still contain extractable information.

**Key observations:**
1. ViT consistently outperforms LightGBM across all SNR regimes
2. At mag=22.0 (SNRâ‰ˆ5), ViT nearly touches the 5D theoretical ceiling (gap=0.02)
3. At low SNR (mag>22.5), all methods approach the information cliff

### 6.4 Scaling Curve

Performance vs dataset size:

| N | ViT RÂ² | LightGBM RÂ² | Ridge RÂ² | ViT vs LightGBM |
|---|--------|-------------|----------|-----------------|
| 50k | 0.434 | 0.488 | 0.442 | ViT < LightGBM |
| **100k** | **0.596** | 0.553 | 0.475 | **First surpass** âœ“ |
| 200k | 0.673 | 0.547 | 0.474 | +23% |
| 500k | 0.709 | 0.574 | 0.490 | +24% |
| **1M** | **0.711** | **0.614** | 0.50 | **+16%** |

**Key findings:**
1. **100k inflection point**: ViT first surpasses LightGBM at 100k samples
2. **Scaling slope**: ViT (50kâ†’1M: +0.277) is **2.2Ã—** that of LightGBM (+0.126)
3. **âš ï¸ Architecture saturation**: 500kâ†’1M only +0.002, current architecture (p16_h256_L6) has reached capacity

### 6.5 Architecture Sweep Results

From 21 sweep runs, we identified the optimal configuration:

| Config | val_RÂ² | Params | Analysis |
|--------|--------|--------|----------|
| **p16_h256_L6_a8** | **0.662** | 4.9M | **Best overall** |
| p8_h128_L8_a4 | 0.612 | 1.67M | Small patch + deep |
| p32_h256_L4_a4 | 0.602 | 3.37M | Large patch + overlap |
| p16_h384_L6_a8 | 0.589 | 10.9M | Overparameterized |

**Key findings:**
- **patch_size=16** is optimal (vs 8/32/64)
- **hidden_size=256 > 384**: Larger is not always better
- **6 layers** is the sweet spot (4 too shallow, 8 needs more epochs)
- **lr=0.0003** works best

### 6.6 Ablations

Tokenization ablation (based on Sweep hlshu8vl, 50k data):

| Configuration | Val RÂ² | Notes |
|---------------|--------|-------|
| **C1D, patch=16** | **0.582 Â± 0.045** | **Best configuration** âœ“ |
| C1D, patch=32 | 0.473 Â± 0.128 | -19% |
| C1D, patch=64 | 0.534 | -8% |
| SW, patch=16 | âš ï¸ Failed | Needs investigation |

**Key findings:**
- **patch_size=16** is optimal, matching 4096-dim input with 256 tokens
- Larger patches (32/64) lose spectral details
- SW (Sliding Window) implementation needs debugging

### 6.7 Error Analysis

**[P2: Optional]** Residual analysis:
- Residual vs Teff
- Residual vs log(g)
- Residual vs [M/H]
- Residual vs SNR

---

## 7. Discussion

### 7.1 Why Scaling Helps

Vision Transformers lack the strong inductive biases of CNNs (locality, translation invariance). With small datasets, the model cannot learn robust spectral feature mappings. At 1M scale, the model enters a "learnable regime" where global self-attention effectively captures long-range spectral dependencies.

### 7.2 Why Gap Remains to CRLB

The gap between SpecViT (RÂ² = 0.698) and Fisher 5D ceiling (RÂ² = 0.874) at mag=21.5 may arise from:

1. **Tokenization information loss**: Patch size 16 may aggregate over fine spectral features
2. **Model capacity**: 6-layer, 256-dim may not be fully saturated
3. **Training strategy**: MSE loss, label normalization choices
4. **Estimator bias**: CRLB assumes unbiased estimators; NN may have slight bias

**However, at mag=22.0 (SNRâ‰ˆ5), the gap shrinks to just 0.02**, indicating that ViT approaches the physical limit under challenging noise conditions. This suggests the remaining gap at higher SNR is primarily due to model/tokenization design rather than fundamental limitations.

### 7.3 Limitations

- **Synthetic-to-real gap**: BOSZ spectra may not capture all real-world effects
- **Parameter coverage**: Limited Teff, [M/H] ranges
- **Single metallicity bin**: Z=0 only
- **Wavelength window**: 7100â€“8850 Ã… (MR arm only)

---

## 8. Conclusion

1. **ViT scales to 1M spectra**: Achieves RÂ² = 0.711 (test) for log(g) regression, outperforming LightGBM by +16% and template fitting by +76%. Data scale is the critical bottleneck for Transformer success in stellar spectroscopy. ViT first surpasses LightGBM at 100k samples with a scaling slope 2.2Ã— that of traditional ML.

2. **ViT approaches physical limits**: At mag=22.0 (SNRâ‰ˆ5), the gap to the 5D Fisher/CRLB ceiling shrinks to just 0.02, demonstrating that SpecViT extracts nearly all available spectral information under challenging noise conditions.

3. **Fisher/CRLB ceiling quantifies headroom**: The RÂ²â€“SNR curve provides a physics-grounded benchmark. At mag=21.5 (SNRâ‰ˆ7), gap = 0.18; the remaining headroom is attributed to tokenization and model capacity rather than irreducible noise.

4. **Architecture matters but not size**: Sweep analysis shows patch_size=16, hidden_size=256, 6 layers is optimal. Larger models (hidden_size=384) perform worse, suggesting data-model capacity matching is crucial.

**Future Work:**
- Scaling curve (1kâ†’1M) to characterize data requirements
- Multi-task learning (Teff, log g, [M/H] jointly)
- Real data transfer (LAMOST, APOGEE)
- Uncertainty quantification
- Physics-informed positional encoding (PIPE)

---

## Figures

### Figure 1: RÂ² vs SNR with Fisher/CRLB Ceiling (Main Figure)
![RÂ² vs SNR](../../logg/scaling/exp/img/r2_vs_snr_ceiling_test_10k_unified_snr.png)

*Caption: Comparison of log(g) prediction RÂ² as a function of SNR (magnitude). The Fisher/CRLB 5D theoretical ceiling (blue circles) represents the maximum achievable performance for any unbiased estimator. SpecViT (yellow diamonds, RÂ²=0.698 overall) approaches the ceiling at medium-high SNR, outperforming LightGBM (green squares, RÂ²=0.614) and template fitting (red triangles, RÂ²=0.404). At mag=22.0 (SNRâ‰ˆ5), ViT nearly touches the ceiling with gap=0.02.*

### Figure 2: SpecViT Pipeline
![SpecViT Pipeline](specvit_pipeline.jpg)

*Caption: End-to-end SpecViT pipeline. (1) Input 4096-dim spectrum; (2) Heteroscedastic noise injection during training; (3) Tokenization/Patch Embedding (patch_size=16 â†’ 256 tokens, supporting C1D or Sliding Window); (4) Add learned positional embedding and [CLS] token; (5) 6-layer Transformer Encoder (hidden=256, heads=8); (6) Regression head outputs log(g) prediction.*

### Figure 3: Scaling Curve
*[P1 - Performance vs dataset size (1kâ†’1M)]*

### Figure 4: Tokenization Ablation
*[P1 - Bar plot of C1D/SW, patch size ablation results]*

### Figure 5: Residual Analysis
*[P2 - Residual maps over (Teff, log g) and (SNR, log g)]*

---

## Tables

### Table 1: Dataset Specification

| Property | Value |
|----------|-------|
| Source | BOSZ synthetic spectra |
| Wavelength | 7100â€“8850 Ã… |
| Dimension | 4096 pixels |
| Teff Range | 3750â€“6000 K |
| log g Range | 1â€“5 |
| [M/H] Range | -0.25â€“0.75 |
| Magnitude | 20.5â€“22.5 |
| Noise | Heteroscedastic, noise_level=1.0 |
| Train/Val/Test | 1,000,000 / 1,000 / 10,000 |

### Table 2: Model Configuration

| Parameter | Value |
|-----------|-------|
| Architecture | ViT-L6-H256 |
| Patch Size | 16 |
| Hidden Size | 256 |
| Layers | 6 |
| Attention Heads | 8 |
| FFN Size | 1024 |
| Parameters | ~4.9M |
| Tokenization | C1D (Conv1D) |
| Position Encoding | Learned |
| Loss | MSE |
| Label Normalization | Standard (z-score) |

### Table 3: Main Results

| Model | Train Size | RÂ² (test) | Î” vs ViT | Gap to Ceiling |
|-------|-----------|-----------|----------|----------------|
| **SpecViT** | 1M | **0.711** | - | **0.163** |
| LightGBM | 1M | 0.614 | -14% | 0.260 |
| Template Fitting | - | 0.404 | -43% | 0.470 |
| Ridge | 1M | 0.50 | -30% | 0.374 |
| **Fisher 5D Ceiling** | - | **0.874** | - | **0** |

### Table 4: Per-SNR Performance

| Magnitude | SNR | ViT RÂ² | LightGBM RÂ² | Ceiling RÂ² | Gap |
|-----------|-----|--------|-------------|------------|-----|
| 20.0 | 24 | 0.90 | 0.87 | 0.989 | 0.09 |
| 21.5 | 7.1 | 0.80 | 0.74 | 0.874 | 0.07 |
| 22.0 | 4.6 | 0.68 | 0.60 | 0.698 | **0.02** |
| 22.5 | 3.0 | 0.52 | 0.42 | 0.265 | - |

---

## Appendix A: RÂ² Scale Invariance Proof

For any linear transformation $y' = ay + b$:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y - \hat{y})^2}{\sum(y - \bar{y})^2}$$

In transformed space:
- $SS_{res}' = \sum((y' - \hat{y}'))^2 = a^2 \sum(y - \hat{y})^2 = a^2 \cdot SS_{res}$
- $SS_{tot}' = \sum(y' - \bar{y}')^2 = a^2 \sum(y - \bar{y})^2 = a^2 \cdot SS_{tot}$

Therefore: $R^2_{orig} = R^2_{norm}$

**Implication:** RÂ² computed in normalized space (standard or minmax) is identical to RÂ² in original log(g) space.

---

## Appendix B: Experiments Checklist

### P0: Must-Have (Core Paper)

| # | Experiment | Status | Artifact |
|---|------------|--------|----------|
| P0.1 | 1M run + Test metrics | âœ… | Table 3 (RÂ²=0.711) |
| P0.2 | LightGBM 1M baseline | âœ… | Table 3 (RÂ²=0.614) |
| P0.3 | Template Fitting baseline | âœ… | Table 3 (RÂ²=0.404) |
| P0.4 | SNR sweep + 5D ceiling | âœ… | Figure 1 |
| P0.5 | Scaling curve (50kâ†’1M) | âœ… | Section 6.4 |
| P0.6 | Architecture sweep | âœ… | Section 6.5 |
| P0.7 | Tokenization ablation | âš ï¸ | C1Dâœ…, SWâŒ |

### P1: Should-Have (Enhancements)

| # | Experiment | Status | Priority |
|---|------------|--------|----------|
| P1.1 | Loss/label norm study | ðŸ”† Running | Medium |
| P1.2 | PE ablation | â³ | Low |
| P1.3 | Multi-task learning | â³ | Low |
| P1.4 | Cross-noise generalization | â³ | Low |

### P2: Nice-to-Have (Future Work)

| # | Experiment | Status |
|---|------------|--------|
| P2.1 | Attention visualization | â³ |
| P2.2 | Pretrain + finetune | â³ |
| P2.3 | Synthetic â†’ real transfer | â³ |
| P2.4 | Uncertainty quantification | â³ |

---

## References

*[To be added]*

---

> **Last Updated:** 2025-12-27 (v2)  
> **Corresponding Experiment Logs:** `logg/vit/`, `logg/scaling/`  
> **Core Figure:** `logg/scaling/exp/img/r2_vs_snr_ceiling_test_10k_unified_snr.png`

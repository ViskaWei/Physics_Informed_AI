\section{Conclusion}
\label{sec:conclusion}

% \todo{Write conclusion covering:
% \begin{itemize}
%   \item Summary of contributions
%   \item Key findings: SpecViT achieves state-of-the-art performance on stellar parameter estimation
%   \item Insights: What the attention patterns reveal about spectral features
%   \item Limitations: Current limitations of the approach
%   \item Future work: Extensions to real observed data, other stellar parameters, transfer learning
% \end{itemize}
% }

We introduced SpecViT, a Vision Transformer (ViT) model for estimating stellar surface gravity (log g) from medium-resolution, one-dimensional spectra. Using one million simulated Subaru Prime Focus Spectrograph (PFS) medium-resolution (MR) spectra with heteroscedastic per-pixel uncertainties, we benchmarked SpecViT against classical baselines, quantified its scaling with training-set size, and compared its noise-dependent performance to a Fisher-information Cram\'er--Rao lower bound (CRLB) ceiling.

\begin{itemize}
\item \textbf{Test-set performance and baseline comparison.} Trained on $10^{6}$ spectra and evaluated on a 10,000-spectrum test set, SpecViT achieves R2=0.711 with MAE=0.372~dex and RMSE=0.64~dex, exceeding LightGBM (R2=0.614), ridge regression (R2=0.50), and template fitting (R2=0.404) under identical splits.

\item \textbf{Scaling with training-set size.} SpecViT is less competitive at small scale ($N=5\times10^{4}$; R2=0.434 vs.\ 0.488 for LightGBM) but improves more rapidly with data, first surpassing LightGBM at $N\sim10^{5}$ (R2=0.596 vs.\ 0.553). From $5\times10^{4}$ to $10^{6}$ spectra, SpecViT gains $\Delta$R2=0.277 compared to $\Delta$R2=0.126 for LightGBM, while the fixed-capacity SpecViT configuration saturates at the largest scales (R2=0.709 at $5\times10^{5}$ to 0.711 at $10^{6}$).

\item \textbf{Robustness across noise regimes.} In magnitude/SNR bins, SpecViT retains an advantage across the range tested, reaching R2=0.90 at SNR$\approx24$, R2=0.80 at SNR$\approx7.1$, and R2=0.68 at SNR$\approx4.6$ (LightGBM: 0.87, 0.74, and 0.60, respectively).

\item \textbf{Closeness to an information-theoretic ceiling.} The Fisher/CRLB analysis provides an SNR-conditioned ceiling for any unbiased log g estimator under the adopted noise model. SpecViT reaches R2=0.80 versus a 5D ceiling of $R2_{\max}=0.874$ at SNR$\approx7.1$ (gap$\approx0.07$) and R2=0.68 versus $R2_{\max}=0.698$ at SNR$\approx4.6$ (gap$\approx0.02$). Residual diagnostics give RMSE=0.64~dex, between $\sigma_{\mathrm{Fisher}}\approx0.43$~dex (mag$\approx21.5$) and $\approx1.11$~dex (mag$\approx22.5$).

\item \textbf{Sensitivity to tokenization.} In a 50k-scale ablation using Conv1D tokenization, 16-pixel patches yield the best results (test R2=$0.554\pm0.042$), while larger patches reduce performance. Under the sweep settings used here, the sliding-window tokenizer did not produce stable completed runs, motivating the Conv1D patch embedding adopted for the main experiments.
\end{itemize}

These benchmarks—and the corresponding Fisher ceilings—are conditioned on the assumed forward model and heteroscedastic noise prescription. The observed saturation with training-set size and the remaining gap to the ceiling at moderate-to-higher SNR indicate that further gains for higher-SNR spectra will require changes in model capacity and/or tokenization rather than additional training spectra within the same setup.

\begin{acknowledgments}
This work is supported by the generosity of Eric and Wendy Schmidt, by recommendation of the Schmidt Futures program.
\end{acknowledgments}

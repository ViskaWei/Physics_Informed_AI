\section{Conclusion}
\label{sec:conclusion}
We introduce \textsc{SpecViT}, a Vision Transformer (ViT) regressor for estimating stellar surface gravity $\log g$ from medium-resolution, one-dimensional spectra with heteroscedastic per-pixel uncertainties, and benchmark it on one million simulated Subaru Prime Focus Spectrograph (PFS) MR spectra against classical baselines and a Fisher/CRLB ceiling. Trained on $10^{6}$ spectra and evaluated on a 10,000-spectrum test set, \textsc{SpecViT} reaches $R^2=0.711$ (MAE $=0.372$~dex; RMSE $=0.64$~dex), exceeding LightGBM ($R^2=0.614$), ridge regression ($R^2=0.50$), and template fitting ($R^2=0.404$). \textsc{SpecViT} scales favorably with data (overtaking LightGBM at $N\sim10^{5}$) but saturates at the largest scales (0.709 $\rightarrow$ 0.711 from $5\times10^{5}$ to $10^{6}$), indicating a transition from data-limited to model-limited performance and motivating scalable-capacity architectures such as mixture-of-experts Transformers. Across magnitude/SNR bins, \textsc{SpecViT} remains robust and approaches the information-theoretic ceiling most closely in the lowest-SNR regime; ablations further support patch-based tokenization (16-pixel patches with a convolutional tokenizer) and on-the-fly heteroscedastic noise injection as practical design choices.

\paragraph{Key findings \& design insights.}
Test-set performance and baselines: with $10^{6}$ training spectra and a 10{,}000-spectrum test set, \textsc{SpecViT} attains $R^2=0.711$, MAE $=0.372$~dex, RMSE $=0.64$~dex, outperforming LightGBM ($R^2=0.614$), ridge ($R^2=0.50$), and template fitting ($R^2=0.404$) under the same split. Scaling with data: at $N=5\times10^{4}$, \textsc{SpecViT} ($R^2=0.434$) trails LightGBM ($0.488$) but surpasses it by $N\sim10^{5}$ ($0.596$ vs.\ $0.553$); the overall gain from $5\times10^{4}$ to $10^{6}$ is $\Delta R^2=0.277$ (vs.\ $0.126$ for LightGBM; $\sim$2.2$\times$ steeper), while the improvement from $5\times10^{5}$ to $10^{6}$ is only $+0.002$ (0.709 $\rightarrow$ 0.711), indicating a shift to a \emph{model-limited} regime that motivates scalable-capacity upgrades (e.g., mixture-of-experts ViT). Noise robustness and Fisher ceiling: in SNR bins, \textsc{SpecViT} reaches $R^2\approx0.90$ at ${\rm SNR}\approx24$, $0.80$ at $\approx7.1$, and $0.68$ at $\approx4.6$ (LightGBM: 0.87/0.74/0.60), approaching the 5D Fisher/CRLB ceiling with gaps $\approx0.07$ at ${\rm SNR}\approx7.1$ ($R^2_{\max}=0.874$) and $\approx0.02$ at ${\rm SNR}\approx4.6$ ($R^2_{\max}=0.698$); residual RMSE $=0.64$~dex lies between $\sigma_{\mathrm{Fisher}}\approx0.43$ (mag~21.5) and $\approx1.11$ (mag~22.5). Architecture/tokenization: 16-pixel Conv1D patch embeddings are consistently strongest in ablations (e.g., at 50k scale, test $R^2=0.554\pm0.042$), larger patches degrade performance, and the sliding-window tokenizer was unstable under sweep settings.

\paragraph{Limitations \& future work.}
Conclusions are conditioned on a synthetic forward model, label distribution, and noise prescription; real surveys introduce systematics (continuum placement, tellurics, sky residuals, LSF/wavelength calibration mismatch) not modeled here, and we focus on a single label and instrument configuration. Future work will (i) pursue synthetic-to-real transfer with robust domain adaptation, (ii) extend to multi-task inference of $T_{\mathrm{eff}}$, $\log g$, and $[\mathrm{M/H}]$ with calibrated uncertainties, (iii) scale capacity with mixture-of-experts Transformers to overcome the observed plateau beyond $N\gtrsim5\times10^{5}$, and (iv) develop diffusion-based generative models for spectra (denoising/likelihood surrogates and physics-informed priors) alongside improved tokenization and positional encoding to target remaining headroom at moderate-to-high SNR.

\begin{acknowledgments}
This work is supported by the generosity of Eric and Wendy Schmidt, by recommendation of the Schmidt Futures program.
\end{acknowledgments}

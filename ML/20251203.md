【2025年12月3日-AI方向】

---

## 一、单选题

### 1️⃣ 迁移学习（Transfer Learning）的适用场景是？

A. 数据量充足但任务与预训练模型任务无关
B. 数据量不足但任务与预训练模型任务相关
C. 数据量不足且任务与预训练模型任务无关
D. 数据量充足且任务与预训练模型任务高度相关

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 迁移学习：复用预训练参数/表征 (\theta_{\text{pre}})，在目标任务上微调
* 目标数据少且任务相关 ⇒ 降低样本复杂度、提升泛化

---

### 2️⃣ 在推理（Inference）阶段相对于训练阶段，通常会被关闭以提高效率的操作是？

A. Layer Normalization
B. Tokenization
C. Dropout
D. 残差连接

【单选】

答案：C

精简解释（≤3 行，偏数学/原理）：

* Dropout 训练期：(h' = m\odot h,; m\sim\text{Bernoulli}(p))
* 推理期关闭以保证确定性与稳定输出（不再随机失活）

---

### 3️⃣ 假设一个深度学习模型的权重初始化自 (N(0, 0.01^2))。随机抽取一个权重，其值落在 ([-0.02, 0.02]) 之间的概率大约是多少？（提示：正态分布 3σ 法则）

A. 68%
B. 95%
C. 34%
D. 99.7%

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* ([-0.02,0.02]=\pm 2\sigma)（(\sigma=0.01)）
* 正态经验法则：(|Z|\le 2 \Rightarrow \approx 95%)

---

### 4️⃣ DPM-Solver 在扩散模型中的主要优势是：

A. 增加模型参数
B. 降低采样步数
C. 提高训练速度
D. 提高模型鲁棒性

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 扩散采样可视作求解 ODE/SDE；DPM-Solver 用高阶数值法逼近
* 更少步数达到相近质量 ⇒ 推理采样更快

---

### 5️⃣ Multi-Head Attention 的输出如何计算？

A. 各头结果拼接后线性投影
B. 各头注意力结果取平均
C. 通过门控机制加权合并
D. 仅保留最大相似度的头

【单选】

答案：A

精简解释（≤3 行，偏数学/原理）：

* (\text{head}_i=\text{softmax}(QK^T/\sqrt{d_k})V)（每头）
* (\text{MHA}=\text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O)

---

### 6️⃣ Jacobi 迭代法和 Gauss–Seidel 迭代法的核心区别在于：

A. Gauss–Seidel 需要矩阵对称
B. Gauss–Seidel 每次迭代使用最新计算的分量
C. Jacobi 收敛更快
D. Jacobi 只能用于对角占优矩阵

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* Jacobi：本轮更新全用上一轮旧值；GS：更新即使用最新值
* 因此 GS 往往更快收敛（但条件不同）

---

### 7️⃣ 在 K-Means 聚类中，若目标函数

[
J=\sum_{k=1}^K \sum_{x\in C_k}|x-\mu_k|^2
]
的值不再变化，以下哪种情况必然成立？

A. 所有样本被分配到最优簇
B. 所有簇中心点已收敛
C. 簇内样本方差为零
D. 簇间距离达到最大

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* K-Means 交替最小化 (J)：更新分配 (C_k) 与中心 (\mu_k)
* (J) 不再变化 ⇒ 分配/中心不再改变 ⇒ (\mu_k) 收敛

---

### 8️⃣ 信息增益 IG 与熵 H 的关系：

A. 两者无关
B. (IG=H(X|Y)-H(X))
C. (IG=H(X,Y))
D. (IG=H(Y)-H(Y|X))

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* 信息增益（互信息）：(IG=I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y))
* 选项 D 符合定义

---

### 9️⃣ 关于向量点积的性质，以下说法正确的是？

A. (|a\cdot b|\le |a|+|b|) 是柯西-施瓦茨不等式的表述
B. (a\cdot (b+c)=a\cdot b+a\cdot c)
C. (a\cdot b=b\cdot a) 仅在正交基下成立
D. 零向量与任意向量的点积恒为 1

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 点积满足线性/分配律：(a\cdot(b+c)=a\cdot b+a\cdot c)
* 柯西-施瓦茨应为：(|a\cdot b|\le |a|,|b|)

---

### 1️⃣0️⃣ 在梯度下降中，学习率 (\alpha) 与梯度 (\Delta L) 是如何更新权重 (\omega) 的？

A. (\omega=\omega/\alpha\Delta L)
B. (\omega=\omega-\alpha\Delta L)
C. (\omega=\omega\times\alpha\Delta L)
D. (\omega=\omega+\alpha\Delta L)

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 最小化 (L(\omega))：(\omega_{t+1}=\omega_t-\alpha\nabla_\omega L)
* 负梯度方向是局部最速下降方向

---

### 1️⃣1️⃣ 下列哪个算法属于监督学习？

A. 线性回归
B. 主成分分析（PCA）
C. K-means 聚类

【单选】

答案：A

精简解释（≤3 行，偏数学/原理）：

* 监督学习：使用标签 (y) 学习映射 (x\mapsto y)（如线性回归 (y\approx Xw)）
* PCA/K-means 不依赖标签，属无监督

---

### 1️⃣2️⃣ 在垃圾邮件分类中，贝叶斯定理可用于：

A. 计算邮件中包含特定关键词的联合概率
B. 根据关键词出现频率更新邮件为垃圾的概率
C. 生成随机噪声以混淆分类结果
D. 直接判定邮件的发送者身份

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* (;P(\text{spam}\mid w)\propto P(w\mid \text{spam})P(\text{spam}))
* 由关键词证据更新后验概率（朴素贝叶斯）

---

### 1️⃣3️⃣ 在逻辑回归中，若正则化参数 (\lambda) 过大，可能导致以下哪种情况？

A. 训练集好，测试集差
B. 训练集差，测试集好
C. 训练集和测试集均表现良好
D. 训练集和测试集上均表现差

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* 正则化：(\min_w L(w)+\lambda|w|^2)（或 (|w|_1)）
* (\lambda) 过大 ⇒ 权重被强压缩 ⇒ 欠拟合 ⇒ 训练/测试都差

---

### 1️⃣4️⃣ 奇异值分解（SVD）对于给定矩阵 (A)，其形式为？

A. (A=QR)
B. (A=Q\Lambda Q^T)
C. (A=LU)
D. (A=U\Sigma V^T)

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* SVD：(A=U\Sigma V^T)，其中 (U,V) 正交，(\Sigma) 为非负对角奇异值
* 是降维/压缩/协同过滤等基础分解

---

### 1️⃣5️⃣ 以下哪项是大模型推理阶段优化的主要目标？

A. 增加模型的参数量
B. 提高模型的训练数据多样性
C. 降低推理时的显存占用和计算延迟
D. 提高模型训练速度

【单选】

答案：C

精简解释（≤3 行，偏数学/原理）：

* 推理优化关注部署成本：显存（权重+KV cache）与延迟/吞吐
* 常见手段：量化、KV cache 优化、并行/融合算子等

---

## 二、多选题

### 1️⃣6️⃣ 极大似然估计法中，似然函数的性质包括：

A. 似然函数在最大值处生成已知观测的可能性最大
B. 似然函数是关于参数的凸函数
C. 似然函数是关于参数的连续函数
D. 似然函数在最小值处生成已知观测的可能性最大

【多选】

答案：A、C

精简解释（≤3 行，偏数学/原理）：

* MLE：(\hat\theta=\arg\max_\theta L(\theta)=\arg\max_\theta p(X|\theta))
* (L(\theta)) 一般可视作关于 (\theta) 的连续函数；不保证凸

---

### 1️⃣7️⃣ 关于迭代法收敛性，正确的说法是？

A. 对于对称正定矩阵，Jacobi 方法收敛
B. 对于对称正定矩阵，Gauss–Seidel 方法收敛
C. 若 (A) 严格对角占优，则 Jacobi 和 Gauss–Seidel 迭代均收敛
D. 若 (A) 是不可分弱严格对角占优矩阵，则 SOR 方法收敛
E. 对于对称正定矩阵，若松弛因子 (0<\omega<2)，则 SOR 方法收敛
F. (A) 是对角线非零的实 H 矩阵，则 SOR 迭代法收敛

【多选】

答案：B、C、E

精简解释（≤3 行，偏数学/原理）：

* 对称正定 (A)：GS 收敛；SOR 在 (0<\omega<2) 时收敛
* 严格对角占优：Jacobi 与 GS 都收敛

---

### 1️⃣8️⃣ 关于伯努利分布（0-1 分布）(X\sim \text{Bernoulli}(p))，以下说法正确的是：

A. 伯努利分布是二项分布的特例（(n=1)）
B. 伯努利分布可以描述多次重复实验的结果
C. (P(X=1)=1-P(X=0))
D. 伯努利分布的期望 (E(X)=p)，方差 (D(X)=p(1-p))

【多选】

答案：A、C、D

精简解释（≤3 行，偏数学/原理）：

* (\text{Bernoulli}(p)\equiv \text{Binomial}(n=1,p))
* (E[X]=p,;\text{Var}(X)=p(1-p))，且 (P(1)=1-P(0))

---

### 1️⃣9️⃣ LoRA 的优点包括：

A. 支持快速任务切换
B. 显著减少微调参数量
C. 保留原模型权重不变
D. 降低推理时显存占用

【多选】

答案：A、B、C

精简解释（≤3 行，偏数学/原理）：

* LoRA：冻结 (W)，仅学低秩增量 (\Delta W=BA)（rank (r\ll d)）
* 微调参数量大幅减少；切换不同 adapter 即快速切任务

---

### 2️⃣0️⃣ 关于常见损失函数及其应用场景的描述，哪些是正确的？

A. 交叉熵损失广泛用于分类任务，能衡量预测分布与真实标签差异
B. MSE 通常用于回归，但在某些情况下也可用于分类/概率回归
C. Hinge Loss 主要用于 SVM，也可扩展到多分类的 margin 学习
D. BLEU、ROUGE 等直接作为训练阶段的优化目标
E. Sigmoid + BCE 可自动解决类别不平衡，无需额外调整

【多选】

答案：A、B、C

精简解释（≤3 行，偏数学/原理）：

* 交叉熵：(\mathcal L=-\sum_i y_i\log \hat y_i)（分类标准配置）
* BLEU/ROUGE多为评测指标且不可微；不平衡通常仍需加权/采样

错题：
下面我按你标的 (1)–(8) **把概念从“能背 + 能用”**的角度讲清楚（尽量用最关键的公式/直觉 + 一个小例子）。

---

# 错题
## (1) 扩散采样 = 解 ODE/SDE；DPM-Solver 是啥？全称是啥？

### 1) 为什么采样像在解 ODE/SDE？

扩散模型通常定义一个“加噪”前向过程（SDE）：
[
d x = f(x,t),dt + g(t), dW_t
]
训练网络学的是 **score** 或 **噪声**（本质上是反向过程需要的“方向”信息）。

反向采样对应 **反向时间 SDE**（从噪声走回数据）：
[
d x = \big[f(x,t) - g(t)^2 \nabla_x \log p_t(x)\big]dt + g(t), d\bar W_t
]
也可以构造一个等价的 **probability flow ODE**（没有随机项）：
[
\frac{dx}{dt}= f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x)
]
所以“采样”就变成：**沿着一个 ODE/SDE 的轨迹积分**（离散化成很多步）。

### 2) DPM-Solver 全称是什么？

* **DPM = Diffusion Probabilistic Model(s)**（扩散概率模型）
* **DPM-Solver**：专门为 DPM 的采样 ODE/离散形式设计的 **高阶数值求解器**（2阶/3阶、多步等）。

### 3) 它为什么能“更少步数”？

传统采样（如 DDPM/ancestral）本质上是**一阶近似**（误差大 → 需要很多步）。
DPM-Solver 用更高阶的数值方法，把每一步的局部误差降下去：

* 一阶：误差 (\mathcal{O}(h^2))（全局 (\mathcal{O}(h))）
* 二阶/三阶：误差更低 ⇒ 同样质量可以用更少步（比如 10–30 步出图，而不是 50–100+）

所以题 4 的正确选项就是：**B. 降低采样步数**。

---

## (2) Jacobi vs Gauss–Seidel：核心区别为什么是“用不用最新分量”？

要解线性方程组 (Ax=b)，把矩阵分裂：
[
A = D + L + U
]
其中 (D) 对角，(L) 下三角，(U) 上三角。

### Jacobi（全用旧值）

[
x^{(k+1)} = D^{-1}\big(b - (L+U)x^{(k)}\big)
]
每个分量 (x_i^{(k+1)}) 都只用上一轮 (x^{(k)})。

### Gauss–Seidel（立刻用新值）

[
x^{(k+1)} = (D+L)^{-1}\big(b - Ux^{(k)}\big)
]
更新 (x_1^{(k+1)}) 后，算 (x_2^{(k+1)}) 会立刻用上 (x_1^{(k+1)})…
**所以它通常更快收敛**（但收敛条件要看矩阵性质）。

因此选项 **B 正确**。

---

## (3) 为什么 (H(Y)-H(Y|X)=H(X)-H(X|Y)) 一样？

关键恒等式（链式法则）：
[
H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)
]

从定义出发：
[
H(Y)-H(Y|X)
= H(Y) - \big(H(X,Y)-H(X)\big)
= H(X)+H(Y)-H(X,Y)
]
同理：
[
H(X)-H(X|Y)=H(X)+H(Y)-H(X,Y)
]
所以二者相等，都是 **互信息**：
[
I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y)=H(X)+H(Y)-H(X,Y)
]
直觉：互信息是 **“知道一个变量后，另一个变量不确定性减少了多少”**，对称的。

---

## (4) 贝叶斯在垃圾邮件分类里怎么用？

贝叶斯定理：
[
P(\text{spam}\mid w)=\frac{P(w\mid \text{spam})P(\text{spam})}{P(w)}
]
其中 (w) 可以是“出现某关键词/特征”。

朴素贝叶斯（词条件独立假设）常写成：
[
P(\text{spam}\mid w_1,\dots,w_m)\propto P(\text{spam})\prod_{j=1}^m P(w_j\mid \text{spam})
]
所以：**看到关键词的证据 → 更新邮件是 spam 的后验概率**，对应选项 **B**。

小例子（直觉）：
若 “free” 在 spam 中很常见：(P(\text{free}\mid spam)) 大，而在正常邮件里小：(P(\text{free}\mid ham)) 小 ⇒ 后验 (P(spam\mid free)) 会显著升高。

---

## (5) “似然函数关于参数是凸函数”为啥错？给个例子

题目说的是 **似然 (L(\theta))**（不是负对数似然）。

给一个最简单的反例：伯努利样本 (n=2)，观测为 ({1,0})。
似然：
[
L(p)=p(1-p)=p-p^2,\quad p\in(0,1)
]
二阶导数：
[
L''(p)=-2<0
]
说明它是 **严格凹**，不是凸。
所以“似然函数一定凸”是错的。

补充一句：很多优化里我们优化的是 **负对数似然 NLL**，它在很多模型（如 logistic 回归）里是凸的；但这不等价于“似然函数凸”。

---

## (6) 迭代法收敛性那题：为什么选 B、C、E？

这类题背一个判据就够：
**迭代法收敛 ⇔ 迭代矩阵谱半径 (\rho(M)<1)**。

### B：SPD（对称正定） ⇒ Gauss–Seidel 收敛 ✅

这是经典结论。

### C：严格对角占优 ⇒ Jacobi & GS 都收敛 ✅

严格对角占优是 Jacobi/GS 最常见的“保证收敛”条件。

### E：SPD 且 (0<\omega<2) ⇒ SOR 收敛 ✅

SOR 是 GS 的松弛推广；SPD + 合适 (\omega)（0 到 2 之间）可保证收敛。

### A 为什么不一定对（SPD 不保证 Jacobi）

给你一个**能记住的反例构造**：令 (n=3)，
[
A=(1-\alpha)I+\alpha \mathbf{1}\mathbf{1}^T,\quad 0<\alpha<1
]
它是 SPD（特征值：(1-\alpha>0)，(1+\alpha(n-1)>0)）。
但 Jacobi 的迭代矩阵对 (n\ge 3) 可能有
[
\rho(M)=\alpha(n-1)
]
取 (\alpha=0.6, n=3)，(\rho=1.2>1) ⇒ **Jacobi 发散**。
所以 A 不能选。

> 你会看到：**GS 在 SPD 上稳很多**，Jacobi 还需要额外结构（如对角占优）。

---

## (7) “伯努利分布可以描述多次重复实验的结果” 为啥不选？

* 单次试验（成/败）是 **Bernoulli(p)**。
* 多次重复试验的“**成功次数总和**”是 **Binomial(n,p)**：
  [
  S=\sum_{i=1}^n X_i \sim \text{Binomial}(n,p)
  ]

题干里的“描述多次重复实验的结果”通常指“最终统计结果（成功次数）”，所以更准确是二项分布 ⇒ **不选 B**。

---

## (8) BLEU / ROUGE 是啥？为啥不能直接当训练损失？类别不平衡怎么处理？

### 1) BLEU / ROUGE 是什么？

它们是**生成任务的评测指标**：

* **BLEU**：偏机器翻译，基于 n-gram 精确匹配（precision + brevity penalty）。
* **ROUGE**：偏摘要，常用 ROUGE-1/2/L（召回导向的 n-gram / 最长公共子序列）。

### 2) 为什么通常不直接作为训练目标？

因为它们对输出 token 是**离散、不可微**的，没法直接反传梯度。
训练一般用可微目标（如交叉熵），如果要“优化 BLEU/ROUGE”，通常用：

* 强化学习（REINFORCE）、最小风险训练、或用可微近似。

### 3) “Sigmoid + BCE 能自动解决类别不平衡”为什么错？怎么调？

BCE（不加权）：
[
\mathcal{L}=-\big[y\log p + (1-y)\log(1-p)\big]
]
类别极不平衡时，多数类主导梯度，模型容易学成“全预测多数类”。

常用解决法（背这 4 个就够）：

1. **加权 BCE（class weight）**
   [
   \mathcal{L}=-\big[w_+, y\log p + w_- (1-y)\log(1-p)\big]
   ]
   一般取 (w_+ \propto \frac{N}{2N_+}), (w_- \propto \frac{N}{2N_-})。

2. **Focal Loss（抑制易样本）**
   [
   \mathcal{L}=-(1-p)^\gamma y\log p - p^\gamma(1-y)\log(1-p)
   ]
   难样本权重大，适合极端不平衡。

3. **重采样**

* 过采样少数类 / 欠采样多数类（或用 balanced batch）

4. **阈值/校准**

* 训练后调 decision threshold（不一定用 0.5），配合 PR 曲线/F1 最优点。

---

如果你愿意，我可以把这些内容压成 **一页“机考速背卡”**（每条 2–3 行 + 关键公式），尤其是 (1)(2)(3)(4)(6)(8) 这几块最容易在选择题里反复考。你想要“速背版”还是“带一个小练习题版”？

# ML 学习进度追踪表

> **目标**: 2026/1/8 华为 AI 考试  
> **更新时间**: 2025-12-29  
> **倒计时**: 10 天

---

## 📊 每日练习记录

| 日期 | 选择题 | 错题 | 编程1 | 编程2 | 状态 |
|------|--------|------|-------|-------|------|
| 2025-08-27 | - | - | - | - | 🔴 空 |
| 2025-08-28 | - | - | - | - | 🔴 空 |
| 2025-09-03 | - | - | - | - | 🔴 空 |
| 2025-09-04 | - | - | - | - | 🔴 空 |
| 2025-09-05 | - | - | - | - | 🔴 空 |
| 2025-09-10 | - | - | - | - | 🔴 空 |
| 2025-09-12 | - | - | - | - | 🔴 空 |
| 2025-09-17 | - | - | - | - | 🔴 空 |
| 2025-09-18 | - | - | - | - | 🔴 空 |
| 2025-09-24 | - | - | - | - | 🔴 空 |
| 2025-09-28 | - | - | - | - | 🔴 空 |
| 2025-10-10 | - | - | - | - | 🔴 空 |
| 2025-10-10us | - | - | - | - | 🔴 空 |
| 2025-10-15 | - | - | - | - | 🔴 空 |
| 2025-10-17 | - | - | - | - | 🔴 空 |
| 2025-10-22 | - | - | - | - | 🔴 空 |
| 2025-10-23 | - | - | - | - | 🔴 空 |
| 2025-10-29 | - | - | - | - | 🔴 空 |
| 2025-11-05 | - | - | - | - | 🟡 部分 |
| 2025-11-06 | - | - | - | - | 🟡 部分 |
| 2025-11-12 | - | - | - | - | 🟡 部分 |
| 2025-11-19 | - | - | - | - | 🟡 部分 |
| 2025-11-20 | ✅ | ✅ | - | - | 🟢 完成 |
| 2025-12-03 | ✅ | ✅ | - | - | 🟢 完成 |
| 2025-12-17 | ✅ | ✅ | - | - | 🟡 部分 |

---

## 🎯 ML 知识点掌握度

### 1. 深度学习基础

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 梯度消失/爆炸 | 4 | ⬜ | 链式法则累乘导致梯度指数衰减/膨胀 |
| 反向传播 | 3 | ⬜ | $\frac{\partial L}{\partial w}$ 链式法则 |
| 激活函数 (ReLU/Sigmoid/Tanh) | 2 | ⬜ | ReLU: $\max(0,x)$，导数 $x>0$ 时为1 |
| 权重初始化 (Xavier/He) | 1 | ⬜ | 3σ法则: ±2σ≈95% |
| Early Stopping | 1 | ⬜ | 验证集性能不再提升时停止 |
| Dropout | 1 | ⬜ | 训练时随机失活，推理时关闭 |

### 2. Transformer & 注意力机制

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| MHA/GQA/MQA/MLA | 4 | ⬜ | MQA共享KV；GQA分组共享；MLA低秩投影 |
| Scaled Dot-Product | 2 | ⬜ | $\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ |
| 位置编码 | 1 | ⬜ | Transformer不感知顺序，需显式加入 |
| LayerNorm | 2 | ⬜ | 稳定训练，缓解内部协变量偏移 |
| Causal Mask | 1 | ⬜ | 下三角mask防止看到未来token |

### 3. 大模型 (LLM)

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| BERT vs GPT | 3 | ⬜ | BERT=Encoder+MLM；GPT=Decoder+AR |
| RLHF/DPO | 2 | ⬜ | PPO最大化奖励+KL约束；DPO跳过奖励模型 |
| LoRA | 1 | ⬜ | 冻结主体，只训练低秩增量 $\Delta W=BA$ |
| Scaling Laws | 1 | ⬜ | Loss ~ power-law(N, D, C) |
| Tokenizer (BPE/WordPiece) | 1 | ⬜ | 子词分词解决OOV问题 |
| Activation Checkpointing | 2 | ⬜ | 用计算换显存，只存检查点 |
| KV Cache | 1 | ⬜ | 推理时缓存K/V减少重复计算 |

### 4. 概率与统计

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| MLE 最大似然 | 3 | ⬜ | $\hat\theta=\arg\max p(X|\theta)$ |
| 贝叶斯定理 | 2 | ⬜ | $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$ |
| 方差/协方差 | 3 | ⬜ | $Cov=0 \Rightarrow Var(X+Y)=Var(X)+Var(Y)$ |
| 泊松过程 | 1 | ⬜ | 间隔服从指数分布，计数服从泊松分布 |
| 伯努利/二项分布 | 2 | ⬜ | Bernoulli(p)是Binomial(1,p) |
| 马尔科夫性 | 1 | ⬜ | 当前状态只依赖前一状态 |
| 信息熵/互信息 | 1 | ⬜ | $I(X;Y)=H(Y)-H(Y|X)$ |

### 5. 线性代数

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| SVD 奇异值分解 | 3 | ⬜ | $A=U\Sigma V^T$，降维/压缩 |
| 特征值/对角化 | 2 | ⬜ | 实对称矩阵可正交对角化 |
| 满秩/可逆 | 1 | ⬜ | $\det(A)\neq 0 \Leftrightarrow$ 可逆 |
| 线性方程组解 | 1 | ⬜ | 自由变量数 = n - rank(A) |
| 迭代法 (Jacobi/GS/SOR) | 2 | ⬜ | GS用最新值，通常更快收敛 |

### 6. 机器学习经典

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 岭回归/正则化 | 2 | ⬜ | λ↑ → 偏差↑方差↓（Bias-Variance Trade-off） |
| 最小二乘法 | 2 | ⬜ | $b=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$ |
| 逻辑回归/交叉熵 | 2 | ⬜ | $L=-[y\log\hat{y}+(1-y)\log(1-\hat{y})]$ |
| K-Means | 1 | ⬜ | 交替优化分配和中心点 |
| PCA | 1 | ⬜ | 最大化投影方差的线性降维 |
| 随机森林 | 1 | ⬜ | Bagging + 特征随机子集 |
| 朴素贝叶斯 | 1 | ⬜ | 假设特征条件独立 |
| 迁移学习 | 1 | ⬜ | 数据少+任务相关时复用预训练 |

### 7. 优化器

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| SGD | 1 | ⬜ | $\theta_{t+1}=\theta_t-\eta g_t$ |
| Momentum | 1 | ⬜ | 一阶动量平滑，减少震荡 |
| AdaGrad | 1 | ⬜ | 累计二阶矩，学习率单调递减 |
| RMSProp | 1 | ⬜ | EMA替代累加，解决AdaGrad衰减问题 |
| Adam | 2 | ⬜ | Momentum + RMSProp |

### 8. 评估指标

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| Precision/Recall | 2 | ⬜ | P=TP/(TP+FP)，R=TP/(TP+FN) |
| F1 分数 | 2 | ⬜ | $F1=\frac{2PR}{P+R}$ |
| ROC/AUC | 1 | ⬜ | TPR vs FPR 曲线下面积 |
| MSE/MAE | 1 | ⬜ | MSE对异常值更敏感 |

### 9. CNN

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 卷积输出尺寸 | 2 | ⬜ | $H_{out}=\frac{H+2P-K}{S}+1$ |
| 参数量计算 | 1 | ⬜ | $(K \times K \times C_{in} + 1) \times C_{out}$ |
| 池化层 | 1 | ⬜ | 降采样，保留主要特征 |

### 10. 扩散模型

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| DPM-Solver | 1 | ⬜ | 高阶数值法减少采样步数 |
| 前向/反向过程 | 1 | ⬜ | 加噪SDE → 反向ODE/SDE |

---

## 📝 错题本（按知识点分类）

### 梯度消失 vs 数据标准化
- **题目特征**: "某一层学习速度明显慢于其他层"
- **易错点**: 误选"数据标准化不充分"
- **正确理解**: 梯度消失是"单层/局部"问题；数据标准化影响"全局"

### 正交对角化
- **题目特征**: 实对称矩阵性质
- **核心结论**: $S=S^T \Leftrightarrow$ 可正交对角化 $\Leftrightarrow$ 存在正交特征向量基

### 似然函数凸性
- **易错点**: 似然函数关于参数是凸函数
- **反例**: $L(p)=p(1-p)$，二阶导<0，是凹函数

### 伯努利 vs 二项
- **易错点**: 伯努利可描述多次实验
- **正确理解**: Bernoulli=单次；多次成功次数=Binomial

### BLEU/ROUGE
- **易错点**: 可直接作为训练损失
- **正确理解**: 离散不可微，只能做评测指标

---

## 🔥 冲刺复习计划 (10天)

| 天数 | 日期 | 复习主题 | 重点 |
|------|------|----------|------|
| D-10 | 12/29 | 深度学习基础 | 梯度消失、反向传播、激活函数 |
| D-9 | 12/30 | Transformer | MHA/GQA/MQA、Attention公式 |
| D-8 | 12/31 | 大模型 | BERT/GPT、RLHF、LoRA |
| D-7 | 1/1 | 概率统计 | MLE、贝叶斯、方差公式 |
| D-6 | 1/2 | 线性代数 | SVD、特征值、迭代法 |
| D-5 | 1/3 | ML经典算法 | 岭回归、最小二乘、K-Means |
| D-4 | 1/4 | 优化器+评估指标 | Adam、F1、ROC |
| D-3 | 1/5 | CNN+扩散模型 | 尺寸公式、DPM-Solver |
| D-2 | 1/6 | 错题复习 | 所有错题本内容 |
| D-1 | 1/7 | 模拟测试 | 完整模拟一套题 |

---

## ✅ 掌握度说明

- ⬜ 未复习
- 🟨 已复习，需巩固
- ✅ 已掌握

---

## 📌 快速链接

- [20251105.md](./20251105.md) - AI方向题
- [20251106.md](./20251106.md) - 留学生AI方向题
- [20251112.md](./20251112.md) - AI方向题
- [20251119.md](./20251119.md) - AI方向题
- [20251120.md](./20251120.md) - 留学生AI方向题 + 错题讲解
- [20251203.md](./20251203.md) - AI方向题 + 错题讲解
- [20251217.md](./20251217.md) - AI方向题 + 错题讲解

# ML 学习进度追踪表

> **目标**: 2026/1/8 华为 AI 考试  
> **更新时间**: 2025-12-30  
> **倒计时**: 10 天

---

## 📊 每日练习记录

| 日期 | 选择题1刷 | 编程1 | 编程2 | 状态 |
|------|---------------|-------|-------|------|
| 2025-08-27 | - | - | - | 🔴 空 |
| 2025-08-28 | - | - | - | 🔴 空 |
| 2025-09-03 | - | - | - | 🔴 空 |
| 2025-09-04 | - | - | - | 🔴 空 |
| 2025-09-05 | - | - | - | 🔴 空 |
| 2025-09-10 | - | - | - | 🔴 空 |
| 2025-09-12 | - | - | - | 🔴 空 |
| 2025-09-17 | - | - | - | 🔴 空 |
| 2025-09-18 | - | - | - | 🔴 空 |
| 2025-09-24 | - | - | - | 🔴 空 |
| 2025-09-28 | - | - | - | 🔴 空 |
| 2025-10-10 | - | - | - | 🔴 空 |
| 2025-10-10us | - | - | - | 🔴 空 |
| 2025-10-15 | - | - | - | 🔴 空 |
| 2025-10-17 | - | - | - | 🔴 空 |
| 2025-10-22 | - | - | - | 🔴 空 |
| 2025-10-23 | - | - | - | 🔴 空 |
| 2025-10-29 | - | - | - | 🔴 空 |
| 2025-11-05 | ✅1/1 | - | - | 🟡 部分 |
| 2025-11-06 | ✅1/1 | - | - | 🟡 部分 |
| 2025-11-12 | ✅1/1 | - | - | 🟡 部分 |
| 2025-11-19 | ✅1/1 | - | - | 🟡 部分 |
| 2025-11-20 | ✅12/30 | - | - | 🟡 部分 |
| 2025-12-03 | ✅12/30 | - | - | 🟡 部分 |
| 2025-12-17 | ✅12/29 | - | - | 🟡 部分 |

---

## 🎯 ML 知识点掌握度

### 1. 深度学习基础

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 梯度消失/爆炸 | 4 | ⬜ | 链式法则累乘导致梯度指数衰减/膨胀 |
| 反向传播 | 3 | ⬜ | $\frac{\partial L}{\partial w}$ 链式法则 |
| 激活函数 (ReLU/Sigmoid/Tanh) | 2 | ⬜ | ReLU: $\max(0,x)$，导数 $x>0$ 时为1 |
| 权重初始化 (Xavier/He) | 1 | ⬜ | 3σ法则: ±2σ≈95% |
| Early Stopping | 1 | ⬜ | 验证集性能不再提升时停止 |
| Dropout | 1 | ⬜ | 训练时随机失活，推理时关闭 |

### 2. Transformer & 注意力机制

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| MHA/GQA/MQA/MLA | 4 | ⬜ | MQA共享KV；GQA分组共享；MLA低秩投影 |
| Scaled Dot-Product | 2 | ⬜ | $\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ |
| 位置编码 | 1 | ⬜ | Transformer不感知顺序，需显式加入 |
| LayerNorm | 2 | ⬜ | 稳定训练，缓解内部协变量偏移 |
| Causal Mask | 1 | ⬜ | 下三角mask防止看到未来token |

### 3. 大模型 (LLM)

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| BERT vs GPT | 3 | ⬜ | BERT=Encoder+MLM；GPT=Decoder+AR |
| RLHF/DPO | 2 | ⬜ | PPO最大化奖励+KL约束；DPO跳过奖励模型 |
| LoRA | 1 | ⬜ | 冻结主体，只训练低秩增量 $\Delta W=BA$ |
| Scaling Laws | 1 | ⬜ | Loss ~ power-law(N, D, C) |
| Tokenizer (BPE/WordPiece) | 1 | ⬜ | 子词分词解决OOV问题 |
| Activation Checkpointing | 2 | ⬜ | 用计算换显存，只存检查点 |
| KV Cache | 1 | ⬜ | 推理时缓存K/V减少重复计算 |

### 4. 概率与统计

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| MLE 最大似然 | 3 | ⬜ | $\hat\theta=\arg\max p(X|\theta)$ |
| 贝叶斯定理 | 2 | ⬜ | $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$ |
| 方差/协方差 | 3 | ⬜ | $Cov=0 \Rightarrow Var(X+Y)=Var(X)+Var(Y)$ |
| 泊松过程 | 1 | ⬜ | 间隔服从指数分布，计数服从泊松分布 |
| 伯努利/二项分布 | 2 | ⬜ | Bernoulli(p)是Binomial(1,p) |
| 马尔科夫性 | 1 | ⬜ | 当前状态只依赖前一状态 |
| 信息熵/互信息 | 1 | ⬜ | $I(X;Y)=H(Y)-H(Y|X)$ |

### 5. 线性代数

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| SVD 奇异值分解 | 3 | ⬜ | $A=U\Sigma V^T$，降维/压缩 |
| 特征值/对角化 | 2 | ⬜ | 实对称矩阵可正交对角化 |
| 满秩/可逆 | 1 | ⬜ | $\det(A)\neq 0 \Leftrightarrow$ 可逆 |
| 线性方程组解 | 1 | ⬜ | 自由变量数 = n - rank(A) |
| 迭代法 (Jacobi/GS/SOR) | 2 | ⬜ | GS用最新值，通常更快收敛 |

### 6. 机器学习经典

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 岭回归/正则化 | 2 | ⬜ | λ↑ → 偏差↑方差↓（Bias-Variance Trade-off） |
| 最小二乘法 | 2 | ⬜ | $b=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$ |
| 逻辑回归/交叉熵 | 2 | ⬜ | $L=-[y\log\hat{y}+(1-y)\log(1-\hat{y})]$ |
| K-Means | 1 | ⬜ | 交替优化分配和中心点 |
| PCA | 1 | ⬜ | 最大化投影方差的线性降维 |
| 随机森林 | 1 | ⬜ | Bagging + 特征随机子集 |
| 朴素贝叶斯 | 1 | ⬜ | 假设特征条件独立 |
| 迁移学习 | 1 | ⬜ | 数据少+任务相关时复用预训练 |

### 7. 优化器

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| SGD | 1 | ⬜ | $\theta_{t+1}=\theta_t-\eta g_t$ |
| Momentum | 1 | ⬜ | 一阶动量平滑，减少震荡 |
| AdaGrad | 1 | ⬜ | 累计二阶矩，学习率单调递减 |
| RMSProp | 1 | ⬜ | EMA替代累加，解决AdaGrad衰减问题 |
| Adam | 2 | ⬜ | Momentum + RMSProp |

### 8. 评估指标

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| Precision/Recall | 2 | ⬜ | P=TP/(TP+FP)，R=TP/(TP+FN) |
| F1 分数 | 2 | ⬜ | $F1=\frac{2PR}{P+R}$ |
| ROC/AUC | 1 | ⬜ | TPR vs FPR 曲线下面积 |
| MSE/MAE | 1 | ⬜ | MSE对异常值更敏感 |

### 9. CNN

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| 卷积输出尺寸 | 2 | ⬜ | $H_{out}=\frac{H+2P-K}{S}+1$ |
| 参数量计算 | 1 | ⬜ | $(K \times K \times C_{in} + 1) \times C_{out}$ |
| 池化层 | 1 | ⬜ | 降采样，保留主要特征 |

### 10. 扩散模型

| 知识点 | 出现次数 | 掌握度 | 核心公式/概念 |
|--------|----------|--------|---------------|
| DPM-Solver | 1 | ⬜ | 高阶数值法减少采样步数 |
| 前向/反向过程 | 1 | ⬜ | 加噪SDE → 反向ODE/SDE |

---

## 📝 错题本（按知识点分类）

### 梯度消失 vs 数据标准化
- **题目特征**: "某一层学习速度明显慢于其他层"
- **易错点**: 误选"数据标准化不充分"
- **正确理解**: 梯度消失是"单层/局部"问题；数据标准化影响"全局"

### 正交对角化
- **题目特征**: 实对称矩阵性质
- **核心结论**: $S=S^T \Leftrightarrow$ 可正交对角化 $\Leftrightarrow$ 存在正交特征向量基

### 似然函数凸性
- **易错点**: 似然函数关于参数是凸函数
- **反例**: $L(p)=p(1-p)$，二阶导<0，是凹函数

### 伯努利 vs 二项
- **易错点**: 伯努利可描述多次实验
- **正确理解**: Bernoulli=单次；多次成功次数=Binomial

### BLEU/ROUGE
- **易错点**: 可直接作为训练损失
- **正确理解**: 离散不可微，只能做评测指标

### CNN 输出尺寸公式
- **题目特征**: 计算卷积层输出尺寸/通道数
- **核心公式**: $H_{out}=\left\lfloor\frac{H+2P-K}{S}\right\rfloor+1$；输出通道数=卷积核个数
- **易错点**: 忘记 padding；把输出通道数误写为输入通道数

### 不相关 ≠ 独立
- **易错点**: 以为 $E(XY)=E(X)E(Y)$ 推出独立
- **正确理解**: 只说明 $\mathrm{Cov}(X,Y)=0$；仍可强依赖（如 $Y=X^2$）；但有 $\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)$

### 优化器常见误解
- **误区**: Adam 保证全局最优；SGD 很少用；AdaGrad 后期仍快
- **纠正**: Adam 不保证全局最优；SGD/SGDM 仍常用且泛化好；AdaGrad 学习率单调递减，后期可能停滞

### BPE 第 1 轮合并（并列最高频）
- **关键点**: `es` 与 `st` 频次并列最高时任选其一合并；`we` 频次更低
- **易错点**: 忽略并列第一（tie），错误选择非最高频对

### EM 与 MLE（含隐变量/缺失数据）
- **一句话**: 先用当前参数计算隐变量后验的期望（E 步），再在“补全数据”上最大化（M 步）
- **结论**: 每步不下降地提高对数似然

### MLE 的渐近正态
- **公式**: $\sqrt{n}\big(\hat\theta-\theta_0\big)\xrightarrow{d}\mathcal{N}\!\left(0,\ I(\theta_0)^{-1}\right)$
- **直觉**: 峰更尖（信息量大）→ 方差更小

---

## 🔥 冲刺复习计划 (10天)

| 天数 | 日期 | 复习主题 | 重点 |
|------|------|----------|------|
| D-10 | 12/29 | 深度学习基础 | 梯度消失、反向传播、激活函数 |
| D-9 | 12/30 | Transformer | MHA/GQA/MQA、Attention公式 |
| D-8 | 12/31 | 大模型 | BERT/GPT、RLHF、LoRA |
| D-7 | 1/1 | 概率统计 | MLE、贝叶斯、方差公式 |
| D-6 | 1/2 | 线性代数 | SVD、特征值、迭代法 |
| D-5 | 1/3 | ML经典算法 | 岭回归、最小二乘、K-Means |
| D-4 | 1/4 | 优化器+评估指标 | Adam、F1、ROC |
| D-3 | 1/5 | CNN+扩散模型 | 尺寸公式、DPM-Solver |
| D-2 | 1/6 | 错题复习 | 所有错题本内容 |
| D-1 | 1/7 | 模拟测试 | 完整模拟一套题 |

---

## 🔥 薄弱环节追踪

### 2025-11-06 薄弱环节 Top-3

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 生成与评测 | 1/1 | 0% | Beam search 长度偏差（样本少） |
| 2 | 线性代数 | 2/3 | 33% | 换基变换 Q^TAQ, Q^{-1}AP |
| 3 | 概率统计 | 1/5 | 80% | 交叉熵 ⇔ MLE |

### 2025-11-12 薄弱环节 Top-3

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 注意力与Transformer | 2/4 | 50% | MHA/GQA区分 + LayerNorm维度 |
| 2 | 线性代数 | 1/4 | 75% | 余弦相似度与归一化 |
| 3 | - | - | - | 其他知识点全对 |

### 2025-11-05 薄弱环节 Top-3

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 深度学习基础 | 2/4 | 50% | GELU特性、Checkpointing原理 |
| 2 | 评估指标 | 1/2 | 50% | F1 计算（样本少） |
| 3 | 概率统计 | 1/4 | 75% | 贝叶斯公式（答案存疑） |

### 2025-11-19 薄弱环节 Top-3

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 优化与数值方法 | 1/1 | 0% | 截断误差 vs 舍入误差 |
| 2 | 线性代数 | 2/3 | 33% | 增广矩阵、代数余子式 |
| 3 | 注意力与Transformer | 1/2 | 50% | MHA/MQA/GQA/MLA 区分 |

---

## ✅ 掌握度说明

- ⬜ 未复习
- 🟨 已复习，需巩固
- ✅ 已掌握

---

## 📌 快速链接

- [20251105.md](./20251105.md) - AI方向题
- [20251106.md](./20251106.md) - 留学生AI方向题
- [20251112.md](./20251112.md) - AI方向题
- [20251119.md](./20251119.md) - AI方向题
- [20251120.md](./20251120.md) - 留学生AI方向题 + 错题讲解
- [20251203.md](./20251203.md) - AI方向题 + 错题讲解
- [20251217.md](./20251217.md) - AI方向题 + 错题讲解

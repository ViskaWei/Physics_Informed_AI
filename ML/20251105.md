# 2025-11-05 AI 方向题

## 一、单选题

### 1. 设 A, P 均为 3 阶矩阵，$P^T$ 为 $P$ 的转置矩阵，且 $P^TAP=\begin{pmatrix}1&0&0\0&1&0\0&0&2\end{pmatrix}$，若 $P=(\alpha_1,\alpha_2,\alpha_3),\ Q=(\alpha_1+\alpha_2,\alpha_2,\alpha_3)$，则 $Q^TAQ$ 为（）

* A. $\begin{pmatrix}2&0&0\0&1&0\0&0&2\end{pmatrix}$
* B. $\begin{pmatrix}2&1&0\1&1&0\0&0&2\end{pmatrix}$
* C. $\begin{pmatrix}1&1&0\1&2&0\0&0&2\end{pmatrix}$
* D. $\begin{pmatrix}1&0&0\0&2&0\0&0&2\end{pmatrix}$

**答案：B**

---

### 2. 设 $P(A)=0.3,\ P(B|A)=0.6,\ P(B|\neg A)=0.2$，则 $P(A|B)$ 的值为

* A. 0.36
* B. 0.56
* C. 0.72
* D. 0.18

**答案：A**

$$\frac{0.6\times 0.3}{0.6\times 0.3+0.2\times 0.7}=0.36$$

---

### 3. 已知函数 $f(x)=x^2$，用中心差分公式 $f'(x)\approx\dfrac{f(x+h)-f(x-h)}{2h}$，取步长 $h=0.1$，计算 $f'(1)$ 数值结果为：

* A. 1.8
* B. 2.2
* C. 2.0
* D. 2.4

**答案：C**

$$f'(1)\approx \frac{f(1.1)-f(0.9)}{2\times 0.1}=\frac{1.21-0.81}{0.2}=2.0$$

---

### 4. 设 $\hat{\theta}=\hat{\theta}(X_1,X_2\dots\dots X_n)$ 是基于随机样本 $X_1,X_2\dots\dots X_n$ 构造的未知参数 $\theta$ 的一个估计量，若 $\hat{\theta}$ 满足以下条件（）时，则称 $\hat{\theta}$ 是 $\theta$ 的无偏估计

* A. $\hat{\theta}=\theta$
* B. $E(\hat{\theta}-\theta)=\theta$
* C. $E(\hat{\theta})=\theta$
* D. $P(\hat{\theta}=\theta)=1$

**答案：C**

---

### 5. 在回归问题中，以下哪种损失函数对异常值（outliers）最敏感？

* A. 平均绝对误差（MAE）
* B. 均方误差（MSE）
* C. 对数损失（Log Loss）
* D. Huber损失

**答案：B**

---

### 6. 使用算法计算数列 $I_n=\int_0^1 \frac{x^n}{x+5}dx,\ n=0,1,\dots,100$，如果初始值 $I_0^*$ 的绝对误差为 0.01，那么以下哪种情形可以称该算法是数值稳定的？

* A. $I_n^*$ 的绝对误差均不超过 0.01
* B. $I_n^*$ 的绝对误差单调不增加
* C. $I_n^*$ 的绝对误差有上界

**答案：C**

---

### 7. 在反向传播过程中，如果发现某个特定层的学习速度显著慢于其他层，以下选项中，最可能的原因是？

* A. 该层的参数数量较少
* B. 该层的梯度消失
* C. 数据标准化不充分
* D. 学习率设置过低

**答案：B**

---

### 8. 在一个二分类问题中，精确率（Precision）为 0.8，召回率（Recall）为 0.5，F1 分数最接近如下哪个选项？

* A. 0.62
* B. 0.31
* C. 0.4
* D. 0.7

**答案：A**

$$F1=\frac{2PR}{P+R}=\frac{2\times 0.8\times 0.5}{0.8+0.5}\approx 0.62$$

---

### 9. 某二分类模型混淆矩阵为：TP=80，TN=100，FP=20，FN=50。其 F1 分数约为？

* A. 0.62
* B. 0.54
* C. 0.7
* D. 0.78

**答案：A**

---

### 10. A 为 2 阶矩阵，B 为 2 阶矩阵，且已知 $\det(A)=2,\ \det(B)=3$，则下列结论正确的是（）

* A. $\det(A^TB)=5$（$A^T$ 为 $A$ 的转置矩阵）
* B. $\det(AB)=6$
* C. $\det(2A)=4$
* D. $\det(A+B)=5$

**答案：B**

$$\det(AB)=\det(A)\det(B)=2\times 3=6$$

---

### 11. 在多层感知器（MLP）中，隐藏层通过一系列非线性变换（由激活函数实现）对输入数据进行变化操作，这样的主要作用是？

* A. 直接映射输入到输出，无需中间处理
* B. 学习输入特征之间的复杂关系，并进行非线性变换
* C. 用于减少模型的训练时间
* D. 确保模型的预测结果为线性可分

**答案：B**

---

### 12. 假设输入的向量为 $a=[0,1]$，$b=[2,0]$，其中 $W_k=\begin{bmatrix}0&1\1&0\end{bmatrix}$，$W_q=\begin{bmatrix}1&1\1&2\end{bmatrix}$，$W_v=\begin{bmatrix}2&0\0&1\end{bmatrix}$，温度因子 $d_k=1$，已知 $e^1\approx 2.718$，$e^2\approx 7.389$，$e^3\approx 20.086$，$e^4\approx 54.598$，计算 self-attention 后的a，b的输出

* A. $a=[0, 1]$；$b=[2, 0]$
* B. $a=[16, 1]$；$b=[16, 2]$
* C. $a=[2.9242, 0, 2689]$；$b=[0.4768, 0.8807]$
* D. $a=[3.8104, 0, 0474]$；$b=[3, 5232, 0.1192]$

**答案：D**

---

### 13. 在 Transformer 模型中，自注意力机制（Self-Attention）的核心作用是

* A. 捕捉序列中不同位置元素之间的依赖关系
* B. 替代卷积操作提取局部特征
* C. 降低模型的计算复杂度
* D. 减少参数数量

**答案：A**

---

### 14. 假设一个大模型 Agent 在解决数学推理任务时采用 “反思 - 修正” 机制，其核心流程为：生成初步答案 $A_1$；通过独立模块生成反思信号 $R$；根据 $R$ 修正答案 $A_1\to A_2$。若训练时采用强化学习（RL）框架，则以下哪项最可能是奖励函数的设计目标？

* A. 最大化 $R$ 与 $A_1$ 的语义相似度
* B. 最大化 $A_2$ 与真实答案 $G$ 的匹配度，同时最小化 $|A_1-A_2|$
* C. 最小化 $A_1$ 与 $A_2$ 的计算开销差异
* D. 最大化 $A_1$ 与真实答案 $G$ 的匹配度

**答案：B**

---

### 15. 某种电子设备的寿命（单位：小时）服从参数为 $\lambda=0.01$ 的指数分布，即其概率密度函数为 $f(t)=0.01e^{-0.01t}$，其中 $t\ge 0$。以下陈述正确的是：

* A. 设备失效率（瞬时故障率）随时间增长而降低。
* B. 设备已经工作100小时，它在未来10小时内失效的条件概率小于一个新设备在前10小时内失效的概率。
* C. 设备失效率（瞬时故障率）随时间增长而增加。
* D. 设备已经工作100小时，它在未来10小时内失效的条件概率等于一个新设备在前10小时内失效的概率。

**答案：D**

---

## 二、多选题

### 16. 以下哪些操作不会改变 Transformer 的位置编码信息？

* A. 交换两个 token
* B. 对输入嵌入加常数向量
* C. 将序列整体左移 1 个 token
* D. 对输入嵌入乘常数矩阵

**答案：B、D**

* ❌ A：交换 token 会改变位置对应关系
* ✅ B：平移嵌入不改变位置编码本身
* ❌ C：整体左移会改变每个 token 的位置索引
* ✅ D：线性变换不改变位置编码信息本身（位置仍由编码给出）

---

### 17. 关于激活函数 GELU 与 ReLU，下列描述正确的是（可多选）

* A. GELU 近似保留输入分布的均值，使深网络更易训练
* B. 在绝大多数现代 Transformer 变体中，GELU 已被 SwiGLU 全面替代
* C. GELU 相较于 ReLU 无需额外随机性即可引入“软门控”
* D. 在相同学习率下，GELU 的梯度上界大于 ReLU

**答案：A、C**

* ✅ A：平滑门控，训练更稳定
* ❌ B：并非“全面替代”
* ✅ C：GELU 提供“软门控”效果
* ❌ D：ReLU 梯度上界为 1；题干给出 GELU 梯度上界略小于 1

---

### 18. 在深度学习中，权重初始化是一个重要步骤。假设某个网络层的权重被初始化为来自均值为 0、标准差为 sigma 的高斯分布（正态分布）。关于这个初始化策略，以下哪些说法在概率论上是正确的？

* A. 权重的取值范围被严格限制在 $[-3\sigma,3\sigma]$ 之内
* B. 增加标准差 sigma 会使得初始化的权重值更倾向于远离 0
* C. 大约 68% 的权重值会落在 $[-\sigma,\sigma]$ 的区间内
* D. 初始化权重的期望值为 0

**答案：B、C、D**

* ❌ A：正态分布不被“严格限制”在有限区间内
* ✅ B：$\sigma$ 越大，分布越分散
* ✅ C：经验法则：约 68% 落在 $\pm\sigma$
* ✅ D：均值为 0 ⇒ 期望为 0

---

### 19. 在机器学习中，以下哪些方法可以用于处理类别不平衡问题？

* A. 对少数类样本进行过采样（Oversampling）
* B. 使用 AUC-ROC 曲线作为评价指标
* C. 使用 F1 分数作为评价指标
* D. 对多数类样本进行欠采样（Undersampling）

**答案：A、B、C、D**

* ✅ A：提升少数类占比
* ✅ B：阈值无关、对不平衡更稳
* ✅ C：关注 Precision/Recall 的折中
* ✅ D：降低多数类占比

---

### 20. 梯度检查点 (Gradient Checkpointing) 与 ZeRO-Offload 的主要区别体现在（可多选）

* A. 两者可叠加以进一步扩大可训练模型规模
* B. 前者通过丢弃中间激活再正向重算，减少显存；后者将部分参数 / 优化器状态搬到 CPU 或 NVMe
* C. 两者都会增加前向推理时延
* D. 前者仅影响前向计算图；后者改动优化器加载激活值的方式（例如 CPU）

**答案：A、B**

* ✅ A：可叠加同时省显存/省显存占用路径
* ✅ B：Checkpointing=丢激活重算；ZeRO-Offload=参数/优化器状态外搬
* ❌ C：题目给出的正确项不包含该表述
* ❌ D：题目给出的正确项不包含该表述

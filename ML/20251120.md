# 2025-11-20 留学生AI方向题

## 一、单选题

### 1. 根据2020年 Kaplan 等人的 Tranformer “Scaling Laws”，若在给定计算预算下想最小化语言模型的训练损失，应当

* A. 同时按近似幂律比例增大模型参数数，训练token数和训练步数
* B. 增大词向量维度（Embedding Dimension）
* C. 增大模型参数数量（Model Size）
* D. 增大批大小（Batch Size）

**答案：A**

$$\text{Loss}\sim \text{power-law}(N,\ D,\ C)\ \Rightarrow\ \text{按幂律比例共同扩展更高效}$$

---

### 2. 设 S=R 为实对称矩阵，下列每项说法与“S正交对角化”等价？

* A. S 满足 S=1
* B. S 的特征向量可以组成一组正交基
* C. S 不可约
* D. S 的所有特征值都是正数

**答案：B**

$$S=Q\Lambda Q^T,\ Q\ \text{正交}\ \Leftrightarrow\ \text{存在一组正交特征向量基}$$

---

### 3. 假设输入图片的维度是 224×224×3，卷积神经网络的第一层中有5个卷积核，每个卷积核尺寸为7×7，具有1填充且步骤为1，并给出样例的输出的速度是多少

* A. 218×218×5
* B. 218×218×3
* C. 220×220×5
* D. 220×220×3

**答案：C**

$$H_{\text{out}}=\frac{H+2P-K}{S}+1=\frac{224+2\times 1-7}{1}+1=220,\ \text{通道}=5$$

---

### 4. 已知函数 f(x)=x^2，用中心差分公式，取步长 h=0.1，计算 f‘(1) 数值结果为

* A. 1.8
* B. 2.0
* C. 2.4
* D. 2.2

**答案：B**

$$f'(1)\approx \frac{f(1.1)-f(0.9)}{2h}=\frac{1.21-0.81}{0.2}=2.0$$

---

### 5. 给定数据：x=[1,2,3],y=[1,4,9]。用最小二乘法拟合直线 y=a+bx，则系数b为？

* A. 2.5
* B. 3.5
* C. 4.0
* D. 3.0

**答案：C**

$$b=\frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2}=4.0$$

---

### 6. 某购物APP推荐算法团队希望通过线性回归模型分析用户上个月平均每日在线时间x(单位：小时)对上个月

* x(小时) / Y(美元)
* 1.0 / 65
* 2.0 / 70
* 3.0 / 75
* 4.0 / 80
* 5。0 / 85

假设采用简单线性回归模型: y=w0+w1x，使用最小二乘法求解模型最优参数，下列哪一组参数是正确的

* A. w0=55.0,w1=6.0
* B. w0=58.0,w1=5.5
* C. w0=62.0,w1=4.5
* D. w0=60.0,w1=5.0

**答案：D**

$$y=5x+60\Rightarrow (w_0,w_1)=(60,5)$$

---

### 7. 如果随机变量X和Y的期望满足E(XY)=E(X)E(Y)，则下列哪项成立

* A. Var(X-Y)=Var(X)-Var(Y)
* B. Var(XY)=Var(X)Var(Y)
* C. Var(X^2-Y^2)=Var(X^2)+Var(Y^2)
* D. Var(X+Y)=Var(X)+Var(Y)

**答案：D**

$$\mathrm{Cov}(X,Y)=E(XY)-E(X)E(Y)=0\Rightarrow \mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)$$

---

### 8. 已知随机变量X和Y的协方差Cov(X,Y)=8，且 Var(X)=16,Var(Y)=25，则他们的相关系数 P_{X,Y} 为 0

* A. 0.4
* B. 0.2
* C. 0.8
* D. 0.6

**答案：A**

$$\rho_{X,Y}=\frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y)}}=\frac{8}{\sqrt{16\cdot 25}}=0.4$$

---

### 9. 在Transformer模型中，自注意力机制(Self-Attention)的核心作用是

* A. 替代卷积操作提取局部特征
* B. 减少参数数量
* C. 降低模型的计算复杂度
* D. 捕捉序列中不同位置元素之间的依赖关系

**答案：D**

$$\mathrm{Attn}(Q,K,V)=\mathrm{softmax}!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\ \Rightarrow\ \text{全局依赖建模}$$

---

### 10. 在推荐系统中，用户-物品评分矩阵 A∈R^{m×n} 通过SVD压缩为 A=UV（保留k个奇异值，k 远小于m和n的条件）后最小存储空间是？

* A. O(k iog nm)【缺失：请人工确认】
* B. O(knm)
* C. O((k+m+n)^2)
* D. O(k(m+n))

**答案：D**

$$\text{存 }U_k(m\times k)+\Sigma_k(k)+V_k(n\times k)\Rightarrow mk+nk+k=O(k(m+n))$$

---

### 11. 在回归问题中，以下哪种损失函数对异常值(outiers)最敏感?

* A. 平均绝对误差(MAE)
* B. Huber损失
* C. 均方误差(MSE)
* D. 对数损失(Log Loss)

**答案：C**

$$\text{MSE}=\frac1n\sum (y-\hat y)^2\ \Rightarrow\ \text{大残差平方放大}$$

---

### 12. 在一个二分类问题中，精确率(Precision)为0.8，召回率(Recall)为0.5，F1分数最接近如下哪个选项

* A. 0.4
* B. 0.62
* C. 0.7
* D. 0.31

**答案：B**

$$F1=\frac{2PR}{P+R}=\frac{2\times 0.8\times 0.5}{0.8+0.5}\approx 0.615\approx 0.62$$

---

### 13. 从一个盒子中随机抽取一个球，球有编号 1-5，其中编号1、2、3 是红球，编号4、5是蓝球、设事件 A 为 “抽到红球”，事件 B 为 “抽到奇数编号”，那么 P(A∩B)为？

* A. 4/5
* B. 1/5
* C. 2/5
* D. 3/5

**答案：C**

$$A\cap B={1,3},\ P(A\cap B)=\frac{2}{5}$$

---

### 14. 在反向传播过程中，如果发现某个特定层的学习速度显善慢于其他层，以下选项中，最可能的原因是？

* A. 该层的参数数量较少
* B. 该层的梯度消失
* C. 数据标准化不充分
* D. 学习率设置过低

**答案：B**

$$|\nabla W_\ell|\approx 0\Rightarrow \Delta W_\ell\approx 0\Rightarrow \text{该层更新慢}$$

---

### 15. 在训练超大规模语言模型时，为了在显存受限的情况下仍能使用较深的网络并保持梯度正确，常用的内存优化技术是

* A. 权重衰减(Weifht Decay)
* B. 梯度裁剪(Gradient Clipping)
* C. 随机失活(Dropout)
* D. 激活检查点(Activation Checkpointing)

**答案：D**

$$\text{用计算换显存：只存检查点激活，反传时重算中间激活}$$

---

## 二、多选题

### 16. 以下哪些方法可以用于缓解Transformer模型中的规度消失问题?

* A. 使用门控机制(Mamba，类似GRU、LSTM)控制信息流动失
* B. 使用残差连接(Residual Connection)，通过跳跃连接缓解梯度消失
* C. 采用层归一化(Layer Normalization)稳定训练过程
* D. 增加模型的层数，以增强模型的可表达性

**答案：B、C**

* ❌ A：不是 Transformer 体系中缓解梯度消失的典型手段
* ✅ B：残差 $x+\mathrm{SubLayer}(x)$ 让梯度可跨层直通
* ✅ C：LayerNorm 稳定数值范围，梯度更稳定
* ❌ D：加深通常更易加重梯度消失

---

### 17. 关于 MLE(Maximum Likelihood Estimation，最大似然估计)的说法，正确的是

* A. MLE 估计量总是无偏的
* B. EM 算法用于含隐变量的 MLE
* C. MLE 计算时常求其对数似然函数
* D. 当样本量 n 趋于无穷，MLE 服从正态分布

**答案：B、C、D**

* ❌ A：MLE 一般仅“渐近无偏”，有限样本可有偏
* ✅ B：EM 用于含隐变量/缺失数据的 MLE
* ✅ C：最大化 $\log L(\theta)$（乘积变和，便于求导）
* ✅ D：在正则条件下渐近正态：$$\hat\theta_{\text{MLE}}\overset{d}{\to}N(\theta_0,I^{-1}(\theta_0))$$

---

### 18. 假设某数据集中包括 100 个数据其最大值为 100 ，最小值为 1、那么，以下有关数据集基本统计量的定义中，错误的有

* A. 该数据集的众数一定严格大于1
* B. 该数据集的标准差一定严格大于1
* C. 该数据集的均值一定严格大于1
* D. 该数据集的中位数一定严格大于1

**答案：A、D**

* ❌ A：可构造 99 个 1、1 个 100 → 众数为 1
* ✅ B：至少含 1 和 100，离均值平方离差下界使 Std > 1
* ✅ C：存在至少一个 >1 且全体 ≥1 → 均值严格 >1
* ❌ D：同样构造下中位数可为 1

---

### 19. 利用 BPE算法在字节级语料上训练子词词表。给定如下 初始字符级切分与词频，请计算第一轮合并后可能的词表。

语料如下 (为结束符):

* 单词(字符序列) / 频次
  l o w / 5
  l o w e r / 2
  n e w e s t / 6
  w i d e s t / 3

* A.
  l o w
  l o w e r
  n e w e s t
  w i d e s t

* B.
  l ow
  l ow e r
  n e w e s t
  w i d e s t

* C.
  l o w
  l o we r
  n e we s t
  w i d e s t

* D.
  l o w
  l o w e r
  n e we s t
  w i d es t

**答案：C、D**

* ❌ A：未进行“最高频符号对”的合并
* ❌ B：对应合并 w e（频次更低，不是最高频对）
* ✅ C：第一轮可从最高频对中任选其一合并 → 该词表可能出现
* ✅ D：同理，选择另一最高频对合并 → 该词表也可能出现

---

### 20. 在深度学习模型训练过程中，选择合适的优化器对于加速收敛和提高模型性能至关重要。以下关于几种常见优化及其特性的描述

* A. 在大多数情况下，使用Adam优化器可以保证找到全局最优解，因为它能够动态调整学习率并结合了二阶矩估计、因此在所有类型的
* B. SGD(随机梯度下降)优化器虽然简单，但在处理非凸优化问题时比Adam更容易陷入局部最优解，因此在实际运用中很少使用SGD作为优化器
* C. RMSProp优化器通过指数加权平均的方式平滑梯度，从而解决了AdaGrd中学习率单调递减的问题，适用于处理非平稳目标函数
* D. Adam优化器结合了动量(Momentum)和RMSProp的优点，通过自适应地调整每个参数的学习率来加速收敛，并且通常不需要手动调节学习率
* E. AdaGrad优化器为每个参数分配不同的学习率，并随着参数更新次数的增加逐渐减小学习率，这使得它特别适合于稀疏数据场景下的训练，但可能【缺失：请人工确认】

**答案：C、D、E**

* ❌ A：Adam 不能保证全局最优，非凸下仍可能停在局部/鞍点
* ❌ B：SGD 仍非常常用，泛化上常有优势，“很少使用”不对
* ✅ C：RMSProp 用指数加权平方梯度，缓解 AdaGrad 学习率过快衰减
* ✅ D：Adam=Momentum+RMSProp 思路（自适应步长、收敛更快/更稳）
* ✅ E：AdaGrad 对稀疏特征友好，但学习率累积衰减可能致后期停滞


#错题
下面我按你 5 个点逐个“从概念→公式→反例/直觉”讲清楚（尽量像你要背机考那种）。

---

## （1）“S 正交对角化”到底是什么意思？为什么等价于“特征向量能组成正交基”？

### 1）一句话直觉

**正交对角化 = 换一个“旋转/反射后的坐标系”，让线性变换只剩下“各坐标轴方向的缩放”。**
“正交”代表坐标变换不拉伸不扭曲（保持长度和角度），就是纯旋转/反射。

### 2）严格定义

矩阵 (S\in\mathbb R^{n\times n}) **可正交对角化** (\iff) 存在**正交矩阵** (Q)（(Q^\top Q=I)）和对角矩阵 (\Lambda)，使得
[
S = Q\Lambda Q^\top \quad\Longleftrightarrow\quad Q^\top S Q=\Lambda .
]
其中 (Q) 的列向量是新坐标系的基（正交归一基）。

### 3）为什么等价于“特征向量组成一组正交基”（题目选 B）

假设 (Q=[q_1,\dots,q_n])，且 (S=Q\Lambda Q^\top)。把它左乘右乘展开，你会得到：
[
Sq_i = \lambda_i q_i.
]
也就是说 **(Q) 的每一列都是 (S) 的特征向量**，而且 (Q) 是正交矩阵意味着这些特征向量两两正交且范数为 1 ——所以它们构成**一组正交（规范）基**。
反过来，如果你已经有一组**单位正交**特征向量 ({q_i})，把它们拼成 (Q)，把对应特征值放进 (\Lambda)，就必然有 (S=Q\Lambda Q^\top)。
所以：
[
\boxed{S\ \text{可正交对角化} \iff \text{存在一组单位正交特征向量基}}
]

### 4）为什么“实对称矩阵”在概念上特别关键（谱定理）

**谱定理：**
[
\boxed{S=S^\top\ (\text{实对称}) \iff S\ \text{可正交对角化}}
]
直觉是：对称矩阵对应“能量/二次型”这类几何对象，它不会把正交方向“搅在一起”，所以能找到一组互相垂直的主方向（特征向量）来描述它。

### 5）其他选项为什么不等价（给你一眼反例）

* A（页面里像是 “(S^2=I)” 或 “(S=I)” 这类）：**不等价**。
  反例：(S=\mathrm{diag}(2,3)) 是实对称、可正交对角化，但 (S^2\neq I)，也不是 (S=I)。
* C “不可约”：这是代数结构概念，跟能不能正交对角化没直接等价关系。
* D “特征值都为正”：这只说明是正定/半正定，不是正交对角化的充要条件。
  反例：(S=\mathrm{diag}(1,-1)) 仍然实对称、可正交对角化，但有负特征值。

---

## （2）CNN 输出尺寸公式 (H_\text{out}=\frac{H+2P-K}{S}+1) 怎么来的？Padding 在哪儿？

先用 **1D** 想最清楚（2D 就对高、宽各做一次）。

* 输入长度：(H)
* 两边 padding：每边补 (P) 个 → 有效长度变成 (;H+2P)
* 卷积核长度：(K)
* stride：(S)

卷积核从最左边开始滑，每次右移 (S)。
**它能放下的最后一个位置**，其左端最多到：
[
(H+2P)-K
]
所以起始位置从 (0) 到 ((H+2P-K))，步长 (S)，一共多少个起始点？
[
#\text{positions} = \left\lfloor \frac{H+2P-K}{S}\right\rfloor + 1
]
这就是
[
\boxed{H_\text{out}=\left\lfloor \frac{H+2P-K}{S}\right\rfloor + 1}
]
（机考很多时候省略 floor，但真实实现是取整。）

**你的题：** (H=224, P=1, K=7, S=1)
[
H_\text{out}=\frac{224+2\cdot 1-7}{1}+1=220
]
输出通道数 = 卷积核个数 = 5，所以输出是 (220\times 220\times 5)。

---

## （3）已知 (E(XY)=E(X)E(Y))，为什么只有 D 必然成立？其他错在哪里？

关键一步：
[
\mathrm{Cov}(X,Y)=E(XY)-E(X)E(Y)=0
]
也就是 **“不相关(uncorrelated)”**，但注意：**不相关 ≠ 独立**（更强）。

### D 为什么对

方差恒等式：
[
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)+2\mathrm{Cov}(X,Y)
]
而 (\mathrm{Cov}=0)，所以
[
\boxed{\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)}
]

### A 错在哪里

[
\mathrm{Var}(X-Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)-2\mathrm{Cov}(X,Y)
]
(\mathrm{Cov}=0) 时变成 (\mathrm{Var}(X-Y)=\mathrm{Var}(X)+\mathrm{Var}(Y))，**不是** (\mathrm{Var}(X)-\mathrm{Var}(Y))。
反例：令 (X,Y) 独立且 (\mathrm{Var}(X)=\mathrm{Var}(Y)=1)，则 (\mathrm{Var}(X-Y)=2\neq 0)。

### B 错在哪里（这是常见坑）

就算 (X,Y) 独立（当然满足 (E(XY)=E(X)E(Y))），一般也有：
[
\mathrm{Var}(XY)=\mathrm{Var}(X)\mathrm{Var}(Y)+\mathrm{Var}(X),E(Y)^2+\mathrm{Var}(Y),E(X)^2
]
只有当 (E(X)=E(Y)=0) 才可能退化成 (\mathrm{Var}(X)\mathrm{Var}(Y))。
反例：(X,Y\sim\text{Bernoulli}(1/2)) 独立：
(E(X)=E(Y)=1/2)，(\mathrm{Var}(X)=\mathrm{Var}(Y)=1/4)。
(XY=1) 的概率是 (1/4)，所以 (\mathrm{Var}(XY)=\frac14(1-\frac14)=\frac{3}{16}=0.1875)，
而 (\mathrm{Var}(X)\mathrm{Var}(Y)=\frac{1}{16}=0.0625)。不相等。

### C 错在哪里（“不相关”只控制一阶协方差）

[
\mathrm{Var}(X^2-Y^2)=\mathrm{Var}(X^2)+\mathrm{Var}(Y^2)-2\mathrm{Cov}(X^2,Y^2)
]
题设只给 (\mathrm{Cov}(X,Y)=0)，**并不推出** (\mathrm{Cov}(X^2,Y^2)=0)。
经典反例：令 (X\sim U[-1,1])，(Y=X^2)。
则 (E(X)=0)，(E(XY)=E(X^3)=0=E(X)E(Y))（满足题设），但 (X^2) 和 (Y^2=X^4) 显然强相关，所以 C 不必成立。

---

## （4）“某一层学习速度明显慢”为什么更像梯度消失(B)而不是数据标准化不充分(C)？

先把“学习速度”翻译成数学：对第 (\ell) 层参数 (W_\ell)，一次更新大致是
[
\Delta W_\ell = -\eta \frac{\partial L}{\partial W_\ell}
]
**学得慢 = 该层梯度范数小**（(|\partial L/\partial W_\ell|) 很小）。

### 为什么 B（梯度消失）能“只影响某一层/某几层”

梯度是链式法则乘出来的：
[
\frac{\partial L}{\partial W_\ell}
= \frac{\partial L}{\partial a_\ell}\cdot \frac{\partial a_\ell}{\partial W_\ell}
]
只要在这一层附近出现 **饱和/截断**，就会让这层梯度特别小，例如：

* sigmoid/tanh 在饱和区导数接近 0
* ReLU 大量“死掉”（输入长期 <0）导致导数为 0
* 初始化导致某层输出尺度异常，使得该层后面 softmax/归一化进入极端区间
  这些都可能让**某一层**明显慢，而不是所有层一起慢。

### 为什么 C（数据标准化不充分）通常不是“只一层慢”的最可能原因

数据标准化（尤其是输入特征尺度问题）更像是**整体条件数变差**：

* 训练更不稳定、收敛更慢、梯度尺度整体失衡
* 影响通常在**靠近输入的前几层**最明显，但不太会表现为“只有某一层特别慢，其它层完全正常”。

所以机考的“最可能原因”通常选 **B：该层梯度消失**。
（顺带一提：你页面上默认勾选可能不可靠，题库“答案整理区”给的通常才是标准答案。）

---

## （5）优化器公式 + 记忆法（把 20 题那几个一次背牢）

设 (g_t=\nabla_\theta L(\theta_t))。

### SGD（最朴素）

[
\boxed{\theta_{t+1}=\theta_t-\eta g_t}
]
记忆：**“直下坡”**（只看当前梯度）

### Momentum（动量：梯度做一阶平滑）

常见写法（速度 (v)）：
[
\boxed{v_t=\mu v_{t-1}+g_t,\quad \theta_{t+1}=\theta_t-\eta v_t}
]
记忆：**“滚球惯性”**（方向更稳、抖动更小）

### AdaGrad（累计二阶矩：对稀疏特征友好，但越走越慢）

[
\boxed{r_t=r_{t-1}+g_t\odot g_t,\quad \theta_{t+1}=\theta_t-\eta\frac{g_t}{\sqrt{r_t}+\epsilon}}
]
记忆：**“全历史累加”**（(r_t) 只增不减 → 学习率单调变小 → 后期可能停滞）
这就是 20 题 E 的核心点。

### RMSProp（AdaGrad 的“滑动窗口版”：用 EMA 忘掉远古梯度）

[
\boxed{r_t=\rho r_{t-1}+(1-\rho)g_t\odot g_t,\quad \theta_{t+1}=\theta_t-\eta\frac{g_t}{\sqrt{r_t}+\epsilon}}
]
记忆：**“只记最近的 RMS(均方根)”**
对应 20 题 C：缓解 AdaGrad 学习率衰减过快。

### Adam（= Momentum + RMSProp：一阶矩 + 二阶矩）

[
\boxed{
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\quad
v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2
}
]
偏置修正：
[
\hat m_t=\frac{m_t}{1-\beta_1^t},\quad \hat v_t=\frac{v_t}{1-\beta_2^t}
]
更新：
[
\boxed{\theta_{t+1}=\theta_t-\eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}}
]
记忆：**“A-dam = Adaptive Moment(一阶+二阶)”**
所以 20 题 D：Adam = Momentum(一阶) + RMSProp(二阶 EMA)。

### 为什么 20 题 A 错（Adam 不保证全局最优）

深度网络是**非凸**，哪怕自适应学习率也可能停在局部极小/鞍点；“保证全局最优”几乎从不成立（除非特殊凸问题+额外条件）。所以 A 错。

### 为什么 20 题 B 错（SGD 依然常用）

SGD/SGD+Momentum 在大模型训练里非常常见，且经常**泛化更好**；“很少使用”是错误描述。

---

如果你愿意，我也可以把这 5 个点压成一张“机考速背卡片”（每点 3 行 + 一个核心公式 + 一个反例），方便你直接记忆。你想要“更短背诵版”还是“再来一个具体数值例子版”？

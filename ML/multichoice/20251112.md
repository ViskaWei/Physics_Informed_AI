# 2025-11-12 AI 方向题

## 一、单选题

### 1. A/B测试是工业界常用的一种评估功能效果的方法。假设你想测试一个新的推荐算法（B版本）是否比旧算法（A版本）有更高的点击率。在收集到足够数据后，你通常会使用什么统计学概念来判断B版本是否“显著地”优于A版本？

* A. 计算 p 值（p-value）并与显著性水平（如0.05）比较
* B. 计算两个版本点击率的协方差
* C. 计算两个版本点击率的相关系数
* D. 使用最大似然估计来重新拟合模型

**答案：A**

$$p\text{-value}<\alpha\ (如\ 0.05)\Rightarrow \text{差异显著}$$

---

### 2. ReLU 激活函数的导数为：

* A. $$\text{ReLU}'(x)=\begin{cases}1,&\text{if }x\neq 0\0,&\text{if }x=0\end{cases}$$
* B. $$\text{ReLU}'(x)=\begin{cases}1,&\text{if }x>0\0,&\text{if }x<0\end{cases}$$
* C. $$\text{ReLU}'(x)=\begin{cases}1,&\text{if }x\ge 0\0,&\text{if }x<0\end{cases}$$
* D. $$\text{ReLU}'(x)=\begin{cases}1,&\text{if }x>0\0,&\text{if }x\le 0\end{cases}$$

**答案：B**

---

### 3. 贝叶斯定理的核心是用于计算什么类型的概率？

* A. 边缘概率
* B. 先验概率
* C. 条件概率
* D. 联合概率

**答案：C**

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

---

### 4. BERT 模型与 GPT 模型的主要区别是？

* A. BERT 不支持微调，GPT 支持微调
* B. BERT 使用 Transformer 编码器，GPT 使用 Transformer 解码器
* C. BERT 是生成模型，GPT 是预训练模型
* D. BERT 是单向的，GPT 是双向的

**答案：B**

---

### 5. 以下关于自注意力机制中多头注意力（Multi-Head Attention MHA）、多查询注意力机制（Multi-Query Attention MQA）、分组查询注意力机制（Grouped Query Attention GQA）、多头潜在注意力（Multi-Head Latent Attention MLA）描述错误的是？

* A. GQA是对多头注意力（MHA）和多头潜在注意力（MLA）的折中优化方案
* B. MHA核心思想是将输入映射到多个子空间，分别计算注意力权重并聚合结果，从而增强模型对复杂模式的捕捉能力。
* C. MQA核心思想是共享键（Key）和值（Value）的投影参数，仅对查询（Query）使用独立投影参数。
* D. MLA核心思想是通过低秩投影压缩查询（Q）、键（K）、值（V）的维度，并在注意力计算中解耦内容与位置信息，从而减少计算复杂度，同时保留长距离依赖建模能力。

**答案：A**

---

### 6. 设矩阵 $$B=\begin{bmatrix}2&1\0&2\end{bmatrix}$$，下列关于 B 的说法正确的是：

* A. 特征值为 2, 2，且 B 可对角化
* B. 特征值为 2, 2，且 B 不可对角化
* C. 特征值为 1, 3，且 B 可对角化
* D. 特征值为 1, 3，且 B 不可对角化

**答案：B**

---

### 7. 在逻辑回归中，若样本 $x^{(i)}$ 的标签 $y^{(i)}=1$ 且 $h_{\theta}(x^{(i)})=0.1$，则该样本的交叉熵损失贡献为？

* A. $-\log(0.1)$
* B. $\log(0.1)$
* C. $\log(0.9)$
* D. $-\log(0.9)$

**答案：A**

---

### 8. 给定样本矩阵 $X\in\mathbb{R}^{n\times d}$（行对应样本，列对应特征）。若要在不丢失信息的前提下压缩表示，使得重构误差（Frobenius 范数）最小，应采用

* A. LU 分解
* B. 奇异值分解（SVD）截断
* C. Cholesky 分解
* D. QR 分解

**答案：B**

---

### 9. 在马尔科夫模型中，当前状态的概率仅依赖于前一个状态的假设称为？

* A. 独立同分布假设
* B. 马尔科夫性
* C. 平稳性假设
* D. 贝叶斯定理

**答案：B**

---

### 10. 在推荐系统中，用户偏好用向量 $\mathbf{u},\mathbf{v}\in\mathbb{R}^n$ 表示。若采用余弦相似度衡量用户相似性，其值与以下哪种操作有关？

* A. 向量归一化（单位化）
* B. 向量缩放
* C. 向量平移（加常数）
* D. 向量同时取反

**答案：A**

---

### 11. 在Transformer架构的每个子层（如自注意力层、前馈网络层）之后，通常会应用层归一化（Layer Normalization）。关于LayerNorm在Transformer中的作用和计算方式，以下哪项描述是正确的？

* A. 在Transformer的“预归一化”（Pre-LN）结构中，LayerNorm被应用在残差连接之后、非线性激活之前。
* B. LayerNorm的主要目的是稳定模型训练过程，通过归一化使每一层的输入分布保持相对稳定，缓解内部协变量偏移（Internal Covariate Shift）
* C. LayerNorm与Batch Normalization（BN）一样，其归一化统计量（均值和方差）在推理阶段依赖于当前推理批次的样本。
* D. LayerNorm在每个样本的所有token维度上进行归一化，即对序列长度方向（sequence length）求均值和方差。

**答案：B**

---

### 12. 关于 MHA（multi-head attention）、GQA（Grouped-Query Attention）、MLA（Multi-Head Latent Attention）的区别，错误的是

* A. GQA 的参数量与头数无关
* B. MLA（多查询注意力）的所有头共享同一组 K/V
* C. GQA 是 MHA 和 MLA 的折中方案，分组共享 K/V
* D. MHA 的计算成本最高，但表达能力最强

**答案：A**

---

### 13. 某事件在独立重复试验中首次成功所需的试验次数服从

* A. 泊松分布
* B. 指数分布
* C. 二项分布
* D. 几何分布

**答案：D**

---

### 14. 下列哪一个指标可以用来衡量数据的 “离散程度”？

* A. 众数
* B. 标准差
* C. 均值
* D. 中位数

**答案：B**

---

### 15. 一个卷积神经网络的某个卷积层，接收一个尺寸为 $(32,32,3)$ 的输入特征图（高度、宽度、通道数），该卷积层的配置如下：16个卷积核，每个卷积核的尺寸为 $5\times 5$，步长（Stride）为2，填充方式为“VALID”（即无填充），该卷积带Biases为True。请问这个卷积层包含多少个可训练的参数？

* A. 3136
* B. 1200
* C. 1216
* D. 416

**答案：C**

$$\text{params}=(5\cdot 5\cdot 3+1)\cdot 16=1216$$

---

## 二、多选题

### 16. 矩阵 $$A=\begin{bmatrix}1&2\3&4\end{bmatrix}$$，向量 $$\mathbf{b}=\begin{bmatrix}5\6\end{bmatrix}$$。以下选项中正确的有

* A. A 的逆矩阵为 $$A^{-1}=\begin{bmatrix}-2&1\1.5&-0.5\end{bmatrix}$$
* B. A 的行列式 $$\det(A)=-2$$
* C. A 的秩为 1
* D. 方程组 $A\mathbf{x}=\mathbf{b}$ 的解是 $$\mathbf{x}=\begin{bmatrix}4\-1\end{bmatrix}$$

**答案：A、B**

* ✅ A：可直接用 $A^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&-b\-c&a\end{bmatrix}$
* ✅ B：$\det(A)=1\cdot 4-2\cdot 3=-2$
* ❌ C：$\det(A)\neq 0\Rightarrow \text{rank}=2$
* ❌ D：代入检验：$A\begin{bmatrix}4\-1\end{bmatrix}=\begin{bmatrix}2\8\end{bmatrix}\neq \begin{bmatrix}5\6\end{bmatrix}$

---

### 17. 下列属于多项式插值方法的是?

* A. Newton插值法
* B. B样条插值
* C. Hermite插值
* D. Lagrange插值法

**答案：A、C、D**

* ✅ A：多项式插值的一种递推构造
* ❌ B：样条插值为“分段”低次多项式
* ✅ C：利用函数值+导数信息的多项式插值
* ✅ D：用基函数构造全局插值多项式

---

### 18. 以下关于文档切分（Chunking）策略的说法，哪些是正确的？

* A. 基于 “滑动窗口 + 重叠” 的切分可以缓解固定长度切分带来的边界截断问题
* B. 使用句子级切分（sentences）时，必须保证每个 chunk 的 token 数严格相同
* C. 基于语义递归切分（Recursive Character Split）会先按段落、再按句子、再按字符逐步降级，直到满足长度限制
* D. 固定长度切分（如每 512 token）不会破坏句子或段落的语义完整性

**答案：A、C**

* ✅ A：重叠可保留跨边界上下文
* ❌ B：句子切分通常只保证不超上限，不要求严格相同
* ✅ C：递归切分常见策略就是“段落→句子→字符”降级
* ❌ D：固定长度很容易切断句子/段落

---

### 19. 下列关于BERT与GPT两种Transformer模型在预训练目标、架构设计及适用场景等方面的说法，哪些是正确的？

* A. BERT输入时加入token type嵌入用于区分句子对，GPT省略了token type嵌入，仅依赖位置与词嵌入。
* B. BERT的双向上下文表示更适合句子级分类、序列标注等理解类任务，GPT的单向生成能力更适合续写与对话生成。
* C. GPT原生支持从左到右的文本生成与填空续写，BERT由于掩码策略在端到端生成任务上能力有限。
* D. BERT采用掩码语言模型（Masked LM）+下一句预测（Next Sentence Prediction），GPT采用自回归语言模型（Auto-Regressive LM）

**答案：A、B、C、D**

* ✅ A：BERT常用 token type；GPT常无此项
* ✅ B：双向理解 vs 单向生成
* ✅ C：GPT天然自回归；BERT端到端生成不占优势
* ✅ D：预训练目标差异核心点

---

### 20. 在大规模预训练过程中，为了缓解梯度噪声、加速收敛并提升最终泛化能力，以下哪些做法被证实有效？

* A. 使用 AdamW 配合 Decoupled Weight Decay
* B. 在学习率预热（warm-up）结束后采用 Cosine Annealing 带 Restarts
* C. 采用 Gradient Accumulation 维持大批量等效，但在优化器中使用小批量梯度均值
* D. 将批大小从 2k 提升到 32k，并按线性规则放大学习率，同时配合更长的 warmup 与合适的 KL / 权重衰减与梯度裁剪

**答案：A、B、C、D**

* ✅ A：AdamW（解耦权重衰减）是大模型常用配置
* ✅ B：warmup+cosine(+restarts)常见稳定方案
* ✅ C：梯度累积实现“大batch等效”
* ✅ D：大batch+线性放大学习率等经验法则常用（配合warmup/裁剪等）


## ✅ 错题整理（结构化）

> **整理日期**: 2026-01-01  
> **总题数**: 20 | **错题数**: 3 | **正确率**: 85%

---

### Q10: 余弦相似度与向量归一化

**原题**：
> 在推荐系统中，用户偏好用向量 $\mathbf{u},\mathbf{v}\in\mathbb{R}^n$ 表示。若采用余弦相似度衡量用户相似性，其值与以下哪种操作有关？
> * A. 向量归一化（单位化）
> * B. 向量缩放
> * C. 向量平移（加常数）
> * D. 向量同时取反

**正确答案**：A

**你选了**：C

**易错点**：
误以为"平移"与余弦相似度有关。实际上平移改变了向量方向，会改变余弦值。

**核心公式/结论**：
$$
\cos(\theta)=\frac{\mathbf{u}\cdot\mathbf{v}}{|\mathbf{u}||\mathbf{v}|}
$$

- 余弦相似度**本质是单位化后的点积**
- 向量缩放（乘常数）不改变方向 → 不改变余弦值
- 向量平移（加常数）改变方向 → **改变余弦值**
- 向量同时取反 → 方向完全相反 → 余弦值变号

**一眼记住**：
> 余弦 = 方向相似度；归一化是计算的必经步骤；平移≠不变

---

### Q11: LayerNorm 归一化维度

**原题**：
> 在Transformer架构的每个子层（如自注意力层、前馈网络层）之后，通常会应用层归一化（Layer Normalization）。关于LayerNorm在Transformer中的作用和计算方式，以下哪项描述是正确的？
> * A. 在Transformer的"预归一化"（Pre-LN）结构中，LayerNorm被应用在残差连接之后、非线性激活之前。
> * B. LayerNorm的主要目的是稳定模型训练过程，通过归一化使每一层的输入分布保持相对稳定，缓解内部协变量偏移（Internal Covariate Shift）
> * C. LayerNorm与Batch Normalization（BN）一样，其归一化统计量（均值和方差）在推理阶段依赖于当前推理批次的样本。
> * D. LayerNorm在每个样本的所有token维度上进行归一化，即对序列长度方向（sequence length）求均值和方差。

**正确答案**：B

**你选了**：D

**易错点**：
混淆了 LayerNorm 的归一化维度。LayerNorm 是在 **embedding 维度（特征维度）** 上归一化，不是在 sequence length 维度。

**核心公式/结论**：
$$
\mu=\frac{1}{d}\sum_{i=1}^d x_i, \quad \sigma^2=\frac{1}{d}\sum_{i=1}^d(x_i-\mu)^2
$$

其中 $d$ 是 **embedding 维度**（如 768），不是序列长度。

| 归一化方法 | 归一化维度 | 依赖batch? |
|-----------|-----------|-----------|
| BatchNorm | batch维度 | ✅ 是 |
| LayerNorm | 特征维度 | ❌ 否 |

**一眼记住**：
> LayerNorm = 对 embedding 维度归一化 ≠ sequence length；与 batch 无关

---

### Q12: MHA/GQA/MLA 参数量

**原题**：
> 关于 MHA（multi-head attention）、GQA（Grouped-Query Attention）、MLA（Multi-Head Latent Attention）的区别，错误的是
> * A. GQA 的参数量与头数无关
> * B. MLA（多查询注意力）的所有头共享同一组 K/V
> * C. GQA 是 MHA 和 MLA 的折中方案，分组共享 K/V
> * D. MHA 的计算成本最高，但表达能力最强

**正确答案**：A（错误的选项）

**你选了**：C

**易错点**：
题目问的是"**错误的是**"，C 选项本身是正确描述（GQA 确实是分组共享 K/V 的折中方案）。
真正错误的是 A：GQA 的参数量**与分组数/头数有关**，只是比 MHA 少。

**核心公式/结论**：

| 机制 | K/V 共享方式 | 参数量 |
|------|-------------|--------|
| MHA | 每头独立 K/V | 最多 |
| GQA | 分组共享 K/V | 中等（与分组数相关） |
| MQA | 所有头共享同一组 K/V | 最少 |
| MLA | 低秩投影压缩 Q/K/V | 压缩版 |

**一眼记住**：
> 看清"问错误的是"！GQA 参数量 = f(分组数) ≠ 与头数无关

---

## 📊 知识点统计（2025-11-12）

| Tag | 总题数 | 错题数 | 正确率 |
|-----|--------|--------|--------|
| 概率统计 | 5 | 0 | 100% |
| 深度学习基础 | 3 | 0 | 100% |
| 大模型 | 4 | 0 | 100% |
| 注意力与Transformer | 4 | 2 | 50% |
| 线性代数 | 4 | 1 | 75% |
| 经典机器学习 | 1 | 0 | 100% |
| CNN | 1 | 0 | 100% |
| 优化与数值方法 | 1 | 0 | 100% |

---

## 🔥 薄弱环节 Top-3（2025-11-12）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 注意力与Transformer | 2/4 | 50% | MHA/GQA区分 + LayerNorm维度 |
| 2 | 线性代数 | 1/4 | 75% | 余弦相似度与归一化 |
| 3 | - | - | - | 其他知识点全对 |


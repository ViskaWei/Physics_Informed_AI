# 2025-10-10（留学生）- AI岗题

## 一、单选题

### 1. 一个三状态马尔科夫链的转移矩阵为

$$
\left[\begin{array}{ccc}
0.1 & 0.9 & 0 \
0 & 0.2 & 0.8 \
0.5 & 0 & 0.5
\end{array}\right]
$$
从状态2出发，两步到达状态3的概率是()

* A. 0.4
* B. 0.64
* C. 0.16
* D. 0.56

**答案：D**

$$
(P^2)*{2,3}=\sum*{k=1}^3 P_{2k}P_{k3}=0\cdot 0+0.2\cdot 0.8+0.8\cdot 0.5=0.56
$$

---

### 2. 设序列输入长度为 $s=256$，隐藏层维度 $d=1024$，注意力头数 $h=16$，则多头注意力中 $QKY$ 投影的总参数量为

* A. 3*s*d*h
* B. d*d*h
* C. 3*d*d*h
* D. 3*d*d

**答案：D**

$$
W_Q,W_K,W_V\in\mathbb{R}^{d\times d}\Rightarrow #\text{params}=3d^2\ (\text{与 }h\text{ 无关})
$$

---

### 3. 设 $A$ 是 $n$ 阶矩阵$(n\ge 22)$，且 $|A|=4$，则 $|(A^*)^*|=()$

注：$A^*$ 表示矩阵 $A$ 的伴随矩阵(也称为余子矩阵的转置)，有 $AA^*=A^*A=|A|l$

* A. $4^{n-1}$
* B. $4^{n}$
* C. $4^{(n-1)^2}$
* D. ${4^{(n)}}^2$

**答案：C**

$$
|A^*|=|A|^{n-1}\Rightarrow |(A^*)^*|=|A^*|^{n-1}=|A|^{(n-1)^2}=4^{(n-1)^2}
$$

---

### 4. 对点 $(x,P(x))\in{(0,1),(1,3),(3,9)}$ 用拉格朗日法构造插值函数 $P(x)$,则 $P(2)$ 的值为

提示：拉格朗日插值多项式为 $p_k(x)=\Pi_i \frac{x-x_1}{x_k-x_1}$

* A. 6
* B. $P(x)$ 的取值不唯一
* C. $\frac{17}{3}$
* D. $\frac{37}{7}$

**答案：C**

拉格朗日插值三点代入 $x=2$ 得：
$$
P(2)=\frac{17}{3}
$$

---

### 5. 在神经网络中，以下激活函数可以缓解梯度消失同题，并且在隐藏层中被广泛便用？

* A. 阶跃函数
* B. ReLu
* C. Sigmoid
* D. Tanh

**答案：B**

ReLU 在正区间梯度恒为 1，可缓解梯度消失且计算简单。

---

### 6. 某工厂生产的产品次品率为 0.05，质检员随机抽查一件产品，发现是次品，已知质检员检测次品的正确率为 0.98 ，检测出次品的正确率为 0.99 。产品实际为次品的概率是多少？

* A. 0.912
* B. 0.838
* C. 0.932
* D. 0.867

**答案：A**

贝叶斯公式（按答案区给出的“检验正确率0.5%”）：
$$
P(\text{次品}|\text{检出次品})\approx \frac{0.98\cdot 0.05}{0.98\cdot 0.05+0.005\cdot 0.95}\approx 0.912
$$

---

### 7. 关于线性回归算法的下列说法中，哪一项是正确的?

* A. 岭回归 (Ridge Regression) 在损失函数中引入 $L1$ 正则项，可将部分系数压编至零，从而实现特征选择。
* B. 普通最小二乘法 (OLS) 通过最小化残差的绝对值之和来估计参数，该问题存在闭式解，可通过矩阵运算直接求解。
* C. 多重共线性会导致特征之间高度线性相关，虽然模型在训练集上的预测精度可能仍然较高，但会显著增大回归系数的方案。
* D. 线性回归的 $R^2$ 值越高，表明模型对新样本的预测能力越强；因此，应尽可能通过增加特征来提升 $R^2$，从而提高模型泛化性能。

**答案：C**

多重共线性会显著增大系数估计的方差，降低稳定性（预测无偏性不保证稳定）。

---

### 8. 抛掷两枚均匀硬币，事件 $A$ 为“至少一枚硬币正面”，事件 $B$ 为“两枚硬币都是正面”，那么 $P(B|A)$ 为

* A. $\frac{1}{3}$
* B. $\frac{1}{4}$
* C. $\frac{1}{2}$
* D. $\frac{3}{4}$

**答案：A**

$$
A={(正,正),(正,反),(反,正)},\ B={(正,正)}\Rightarrow P(B|A)=\frac{1}{3}
$$

---

### 9. 对于 $3\times224\times224$ 输入，卷积核 $7\times7$、stride=2、padding=3、out_channeis=64 输出特征图的空间尺寸为

* A. 128×128
* B. 112×112
* C. 256×256
* D. 224×224

**答案：B**

$$
\left\lfloor\frac{224-7+2\times 3}{2}\right\rfloor+1=\left\lfloor\frac{223}{2}\right\rfloor+1=112
$$

---

### 10. 若词表大小为 50k，输入序列长度为 512，Embedding 层维度为 768，则 Embedding 参数量为

* A. 512×768
* B. 50k×768
* C. 50k×512
* D. 50k+768

**答案：B**

$$
#\text{params}=\text{vocab}\times d=50000\times 768
$$

---

### 11. 若输入维度 $d=768$ ，FFN 中间层维度为 3072，则 FFN 的参数量为

* A. 2*768*3072
* B. 768*3072
* C. 768*3072+3072*768
* D. (768*3072+3072)+(3072*768+768)

**答案：D**

两层线性层含偏置：
$$
(768\cdot 3072+3072)+(3072\cdot 768+768)
$$

---

### 12. 在高斯混合模型 (GMM) 中，EM 算法的 E 步主要计算：

* A. 后验概率
* B. 先验概率
* C. 参数更新
* D. 特征函数

**答案：A**

E 步计算每个样本属于各高斯分量的后验概率（责任值）。

---

### 13. 已知 $x=1.250\pm0.040,\ y=8.000\pm0.200$，计算：$z=xy$ 的绝对误差限。

* A. 0.578
* B. 0.562
* C. 0.560
* D. 0.570

**答案：A**

$$
\Delta z\approx |y|\Delta x+|x|\Delta y=8\cdot 0.04+1.25\cdot 0.2=0.57
$$
最接近选项为 0.578。

---

### 14. 矩阵

$$
A=\left[\begin{array}{ccc}
4 & 1\
1 & 4
\end{array}\right]
$$
的特征值是()

* A. 3 和 5
* B. 4 和 4
* C. 0 和 8
* D. 2 和 6

**答案：A**

$$
\lambda^2-8\lambda+15=0\Rightarrow \lambda=3,5
$$

---

### 15. 某工厂两条生产线 A (合格率90%，产量60%)和 B (合格率80%，产量40%)。随机抽取一件不合格品，其来自 A 线的概率为？

* A. 3/7
* B. 4/7
* C. 6/7
* D. 5/7

**答案：A**

$$
P(A|\text{不合格})=\frac{0.1\cdot 0.6}{0.1\cdot 0.6+0.2\cdot 0.4}=\frac{0.06}{0.14}=\frac{3}{7}
$$

---

## 二、多选题

### 16. 关于大模型的文本语义相似度计算，下列说法正确的是

* A. 可通过计算两文本的嵌入向量 (Embedding) 余弦相似度实现
* B. 所有大模型的嵌入向量维度必须统一才能计算相似度
* C. 微调时增加同义句对数据可提升相似度计算精度
* D. 模型对语义相似但表达方式差异大的文本识别能力有限

**答案：A、C**

* ✅ A：嵌入向量余弦相似度可用
* ❌ B：不同模型嵌入维度不必统一
* ✅ C：加入同义句对可提升精度
* ❌ D：答案区给出“不普遍成立”

---

### 17. 在随机森林 (Random Forest) 中，以下哪些策路可以提高模型的泛化能力?

* A. 使用 Bagging 方法
* B. 增加特征随机选择的维度 (max_features)
* C. 降低每棵树的深度 (max_depth)
* D. 增加树的数量 (n_estimators)

**答案：A、C、D**

* ✅ A：Bagging 提升泛化
* ❌ B：答案区给出“可能增加过拟合风险”
* ✅ C：降低树深减少过拟合
* ✅ D：增加树数量提升稳定性

---

### 18. 对实矩阵 $X\in\mathbb{R}^{n\times d}$ 的奇异值分解 $X=U\Sigma V^T$，下列说法正确的是：

* A. $U$ 的前 $r(=col(X))$ 列向量给出了 $X$ 列空间的一组正交基
* B. $U、V$ 均为正交矩阵
* C. $\Sigma$ 的对角元素按非递增顺序排列
* D. $rank(X)$ 等于 $\Sigma$ 的非零对角元素个数
* E. $\Sigma$ 的非零奇异值等于 $XX^T$ 的非零特征值的平方根

**答案：A、B、C、D、E**

SVD 的基本性质（答案区：这些都是 SVD 的基本性质）。

---

### 19. 以下哪些算法属于无监督学习?

* A. 随机森林
* B. 主成分分析 (PCA)
* C. 高斯混合模型 (GMM)
* D. K-Means

**答案：B、C、D**

* ❌ A：随机森林为监督学习
* ✅ B/C/D：无监督方法

---

### 20. 以下关于最大似然估计 (MLE) 的说法中，些是正确的?

* A. MLE 能保证估计量是无偏的。
* B. MLE 的估计量可能不是唯一解。
* C. MLE 通常要求样本数据是独立同分布的
* D. MLE 可以通过数值优化方法求解复杂模型的参数

**答案：B、C、D**

* ❌ A：答案区给出“MLE 不一定无偏”
* ✅ B/C/D：可能多解、常用 i.i.d 假设、可用数值优化求解

---

## 📊 知识点标注表

| 题号 | 知识点 | 是否错题 |
|------|--------|----------|
| Q1 | 概率统计（马尔科夫链） | ❌ |
| Q2 | 注意力与Transformer（参数量） | ✅ |
| Q3 | 线性代数（伴随矩阵） | ❌ |
| Q4 | 优化与数值方法（插值） | ❌ |
| Q5 | 深度学习基础（激活函数） | ✅ |
| Q6 | 概率统计（贝叶斯） | ✅ |
| Q7 | 经典机器学习（正则化） | ✅ |
| Q8 | 概率统计（条件概率） | ✅ |
| Q9 | CNN（输出尺寸） | ✅ |
| Q10 | 大模型(LLM)（Embedding） | ✅ |
| Q11 | 注意力与Transformer（FFN参数量） | ✅ |
| Q12 | 经典机器学习（GMM/EM） | ❌ |
| Q13 | 优化与数值方法（误差传播） | ✅ |
| Q14 | 线性代数（特征值） | ✅ |
| Q15 | 概率统计（贝叶斯） | ✅ |
| Q16 | 大模型(LLM)（Embedding相似度） | ✅ |
| Q17 | 经典机器学习（随机森林） | ✅ |
| Q18 | 线性代数（SVD） | ✅ |
| Q19 | 经典机器学习（无监督学习） | ✅ |
| Q20 | 概率统计（MLE） | ✅ |

**统计**: 20 题，4 错，正确率 80%

---

## 🔥 薄弱环节 Top-3（20251010us）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 优化与数值方法 | 1/2 | 50% | 拉格朗日插值 |
| 2 | 线性代数 | 1/3 | 67% | 伴随矩阵 |
| 3 | 经典机器学习 | 1/4 | 75% | GMM/EM后验概率 |

---

## 📚 核心知识点速查

### 概率统计
| 知识点 | 核心公式 |
|--------|----------|
| 马尔科夫转移矩阵 | $P_{ij} = P(X_{t+1}=j \mid X_t=i)$，行→列（from→to），每行和为1 |
| 两步转移概率 | $(P^2)_{ij} = \sum_k P_{ik} P_{kj}$ |
| 贝叶斯公式 | $P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)}$ |
| MLE 公式 | $\hat\theta_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p(x_i \mid \theta)$ |

### 线性代数
| 知识点 | 核心公式 |
|--------|----------|
| 伴随矩阵行列式 | $\|A^*\| = \|A\|^{n-1}$ |
| 二次伴随矩阵 | $\|(A^*)^*\| = \|A\|^{(n-1)^2}$ |
| 特征值求解 | $\det(\lambda I - A) = 0$ |

### 优化与数值方法
| 知识点 | 核心公式 |
|--------|----------|
| 拉格朗日插值 | 3点 → 唯一二次多项式 $P(x) = ax^2 + bx + c$ |
| 绝对误差传播 | $\Delta z \approx \|y\|\Delta x + \|x\|\Delta y$ （乘法） |

### 经典机器学习
| 知识点 | 核心公式 |
|--------|----------|
| GMM E步（后验概率） | $\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}$ |
| 先验 vs 后验 | Prior = $P(\theta)$，Posterior = $P(\theta \mid D)$ |

---

## ✅ 错题整理（结构化）

### Q1: 马尔科夫链两步转移概率

**原题**：
> 一个三状态马尔科夫链的转移矩阵为
> $$\begin{bmatrix} 0.1 & 0.9 & 0 \\ 0 & 0.2 & 0.8 \\ 0.5 & 0 & 0.5 \end{bmatrix}$$
> 从状态2出发，两步到达状态3的概率是()
> A. 0.4  B. 0.64  C. 0.16  D. 0.56

**正确答案**：D (0.56)

**易错点**：
记反"行到列"的方向，或漏掉某条路径

**核心公式/结论**：
$$
(P^2)_{2,3} = \sum_{k=1}^3 P_{2k} P_{k3} = 0 \cdot 0 + 0.2 \cdot 0.8 + 0.8 \cdot 0.5 = 0.56
$$

**一眼记住**：
- 行 = from（出发），列 = to（到达）
- 验证：每行和 = 1 ⇒ 行是条件
- 两步：$P^2$ 的第 $(i,j)$ 元素 = 所有"i→k→j"路径概率之和

---

### Q3: 伴随矩阵的行列式

**原题**：
> 设 $A$ 是 $n$ 阶矩阵$(n\ge 2)$，且 $|A|=4$，则 $|(A^*)^*|=()$
> A. $4^{n-1}$  B. $4^{n}$  C. $4^{(n-1)^2}$  D. $4^{n^2}$

**正确答案**：C ($4^{(n-1)^2}$)

**易错点**：
忘记伴随矩阵的行列式公式，或误算指数

**核心公式/结论**：
$$
|A^*| = |A|^{n-1} \quad \Rightarrow \quad |(A^*)^*| = (|A^*|)^{n-1} = |A|^{(n-1)^2}
$$

**一眼记住**：
"伴随一次：det 升 $(n-1)$ 次方；伴随两次：$(n-1)^2$"

---

### Q4: 拉格朗日插值

**原题**：
> 对点 $(x,P(x)) \in \{(0,1),(1,3),(3,9)\}$ 用拉格朗日法构造插值函数 $P(x)$，则 $P(2)$ 的值为
> A. 6  B. $P(x)$ 的取值不唯一  C. $\frac{17}{3}$  D. $\frac{37}{7}$

**正确答案**：C ($\frac{17}{3}$)

**易错点**：
用拉格朗日公式硬算容易出错；3点插值唯一确定二次多项式

**核心公式/结论**：
设 $P(x) = ax^2 + bx + c$，代入三点：
- $x=0 \Rightarrow c=1$
- $x=1 \Rightarrow a+b=2$
- $x=3 \Rightarrow 9a+3b=8$

解得：$a=\frac{1}{3}, b=\frac{5}{3}, c=1$

$$P(2) = \frac{4}{3} + \frac{10}{3} + 1 = \frac{17}{3}$$

**一眼记住**：
"3点 → 唯一二次；先用 $x=0$ 秒拿 $c$，剩下解 $a,b$"

---

### Q12: EM算法 E步计算后验概率

**原题**：
> 在高斯混合模型 (GMM) 中，EM 算法的 E 步主要计算：
> A. 后验概率  B. 先验概率  C. 参数更新  D. 特征函数

**正确答案**：A（后验概率）

**易错点**：
混淆先验/后验的定义，或误以为 E 步更新参数

**核心公式/结论**：
$$
\gamma_{ik} = P(z_i=k \mid x_i, \theta) = \frac{\pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
$$

**一眼记住**：
- Prior = pre（看数据前）
- Posterior = post（看数据后，条件于观测）
- E步：算后验（责任值）；M步：更新参数

---

<details>
<summary>📎 追问详情（原始解析）</summary>

## （1）马尔可夫"从行到列"怎么记

最常用约定是：

> **转移矩阵第 (i) 行第 (j) 列：$P_{ij}=P(X_{t+1}=j\mid X_t=i)$**
> **行 = 当前/出发（from），列 = 下一步/到达（to）**。

### 3 个"秒杀记忆法"

1. **看谁"和为 1"**

* 你这题矩阵每一行都加起来是 1 ⇒ **行是条件（已知当前状态）** ⇒ 从行到列。

2. **把概率当"质量流"**（物理直觉）

* 当前状态 $i$ 的"概率质量"会被这一行分配到各个下一状态 $j$。
* 所以**一行就是"怎么把 1 份质量分出去"**，自然要和为 1。

3. **用分布向量乘法自检**（永远不会记反）
   令当前分布是行向量 $\pi_t$，则
   $$
   \pi_{t+1}=\pi_t P
   $$
   如果你从状态 2 确定出发，$\pi_t=e_2^\top=[0,1,0]$，那么
   $$
   \pi_{t+1}=e_2^\top P=\text{"第2行"}
   $$
   这就直接说明：**第2行描述"从2出发下一步到哪儿"**。

> 你题里两步到 3：$(P^2)_{2,3}=\sum_k P_{2k}P_{k3}$ 也正是"2→k→3"的路径加和，仍是 **行到列** 的写法。

---

## （3）伴随矩阵 $(A^*)^*$ 的行列式怎么做（答案 C）

题干给：$|A|=4$，求 $|(A^*)^*|$。

### 核心结论（背下来就够）

1. 伴随矩阵（adjugate）满足（当 $A$ 可逆时）：
   $$
   \operatorname{adj}(A)=A^* = |A| A^{-1}
   $$
2. **行列式会"升幂"**：
   $$
   |A^*|=|A|^{n-1}
   $$
   再做一次伴随：
   $$
   |(A^*)^*|=(|A^*|)^{n-1}=(|A|^{n-1})^{n-1}=|A|^{(n-1)^2}
   $$
   代入 $|A|=4$：
   $$
   |(A^*)^*|=4^{(n-1)^2}
   $$

### 直觉（为什么是 $n-1$ 次方）

* $A^*$ 的每个元素是一个 $(n-1)\times(n-1)$ 余子式（本质上含 $|A|$ 的"$(n-1)$ 阶体积"信息）。
* 所以整体行列式会带来 **(n-1) 次幂**：$|A^*|=|A|^{n-1}$。
* 再做一次 ⇒ $(n-1)^2$。

---

## （4）拉格朗日插值 $P(2)$（答案 C：$\frac{17}{3}$）

三点：$(0,1),(1,3),(3,9)$。**最快算法**：直接设二次多项式
$$
P(x)=ax^2+bx+c
$$
代点：

* $x=0\Rightarrow c=1$
* $x=1\Rightarrow a+b+1=3\Rightarrow a+b=2$
* $x=3\Rightarrow 9a+3b+1=9\Rightarrow 9a+3b=8$

解：

* $b=2-a$
* $9a+3(2-a)=8\Rightarrow 6a=2\Rightarrow a=\frac{1}{3}$
* $b=2-\frac{1}{3}=\frac{5}{3}$

算 $P(2)$：
$$
P(2)=4a+2b+c=\frac{4}{3}+\frac{10}{3}+1=\frac{14}{3}+1=\frac{17}{3}
$$

---

## （12）先验/后验分别是什么？EM 的 E 步为什么是"后验概率"？

### 先验 vs 后验（最短定义）

* **先验（Prior）**：看到数据前，你对某个事件/参数的信念
  $$P(\theta) \quad\text{或}\quad P(z=k)$$
* **后验（Posterior）**：看到数据后，更新后的信念
  $$P(\theta\mid D)\quad\text{或}\quad P(z=k\mid x)$$

**记忆法：** prior = **pre**（之前），posterior = **post**（之后）。

### 物理直觉（"测量前/测量后"）

像量子测量/贝叶斯更新一样：

* 测量前：你只有"混合比例/经验" ⇒ 先验
* 测量后：你拿到观测 $x$（证据）⇒ 用证据更新谁更像"产生它的原因" ⇒ 后验

### EM（GMM）里 E 步的"后验概率/责任值"

在 GMM 中，隐变量 $z_i$ 表示样本 $x_i$ 来自哪个高斯分量 $k$。E 步算的是：
$$
\gamma_{ik}=P(z_i=k\mid x_i,\theta)
=\frac{\pi_k \mathcal{N}(x_i\mid\mu_k,\Sigma_k)}
{\sum_{j}\pi_j \mathcal{N}(x_i\mid\mu_j,\Sigma_j)}
$$

* $\pi_k$ 是 **先验**（混合权重：不看 $x_i$ 时，来自第 $k$ 类的概率）
* $\gamma_{ik}$ 是 **后验**（看了 $x_i$ 后，它来自第 $k$ 类的概率）

### MLE 公式

给数据 $D=\{x_i\}_{i=1}^n$，最大似然估计：
$$
\hat\theta_{\text{MLE}}=\arg\max_{\theta} p(D\mid \theta)
=\arg\max_{\theta} \prod_{i=1}^n p(x_i\mid \theta)
$$
通常取对数（把乘法变加法）：
$$
\hat\theta_{\text{MLE}}=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)
$$

</details>

---


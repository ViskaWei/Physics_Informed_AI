# 2025-12-17 AI 方向题

## 一、单选题

### 1. 岭回归 λ 较大时，偏差与方差？

- A. 偏差↑，方差↓
- B. 偏差↓，方差↑
- C. 偏差↑，方差↑
- D. 偏差↓，方差↓

**答案：A**

$$\min_w \|y-Xw\|^2+\lambda\|w\|^2, \quad \lambda\uparrow \Rightarrow \text{Bias}\uparrow,\ \text{Var}\downarrow$$

---

### 2. 伯努利分布 $X\sim\text{Bernoulli}(p)$，样本 $\{1,0,1,1\}$ 的 MLE？

- A. 样本中位数
- B. 0.75
- C. 0.5
- D. 0.25

**答案：B**

$$\hat{p}_{\text{MLE}} = \frac{1}{n}\sum x_i = \frac{3}{4}$$

---

### 3. 最大似然估计中，为什么取对数？

- A. 增加模型复杂度
- B. 使参数估计更准确
- C. 引入先验分布
- D. 乘法→加法，简化计算

**答案：D**

$$\log\prod_i p(x_i|\theta) = \sum_i\log p(x_i|\theta)$$

---

### 4. 大量负样本误判为正样本，说明？

- A. Accuracy 偏高
- B. Precision 偏高
- C. Recall 偏低
- D. Precision 偏低

**答案：D**

$$\text{Precision} = \frac{TP}{TP+FP}, \quad FP\uparrow \Rightarrow \text{Precision}\downarrow$$

---

### 5. $\mathbb{R}^2$ 中哪个是子空间？

- A. $\{(x,y)\mid y=0\}$
- B. 两条坐标轴的并集
- C. $\{(x,y)\mid x+y=1\}$
- D. $\{(x,y)\mid x+1=2y\}$

**答案：A** — 子空间：含零向量 + 加法/数乘封闭

---

### 6. 反向传播用于？

- A. 计算梯度
- B. 前向计算
- C. 更新权重
- D. 初始化参数

**答案：A** — $\frac{\partial L}{\partial w}$（链式法则）

---

### 7. 什么是早停法（Early Stopping）？

- A. 初始化时停止训练
- B. 为减少计算成本提前停止
- C. 训练初期防梯度爆炸
- D. 验证集性能不再提升时停止

**答案：D** — 防止过拟合

---

### 8. Scaled Dot-Product Attention 除以 $\sqrt{d_k}$ 的目的？

- A. 引入位置信息
- B. 缩小 softmax 输入方差，避免梯度饱和
- C. 提高模型容量
- D. 减少计算量

**答案：B**

$$\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)$$

---

### 9. 满秩矩阵，哪项一定成立？

- A. 零空间含非零向量
- B. $\det(A)=0$
- C. 可逆
- D. 特征值全为 0

**答案：C**

$$\det(A)\neq0 \iff A^{-1}\ \text{存在}$$

---

### 10. 若事件 $A$ 与自身独立，则 $P(A)=$？

- A. 0.5
- B. 1
- C. 0
- D. 0 或 1

**答案：D**

$$P(A) = P(A)^2 \Rightarrow P(A)\in\{0,1\}$$

---

### 11. 向量组 $\{(1,0,1),(0,1,1),(1,1,2)\}$ 的线性关系？

- A. 线性无关，秩 3
- B. 线性无关，秩 2
- C. 线性相关，秩 2
- D. 线性相关，秩 1

**答案：C**

$$(1,1,2) = (1,0,1) + (0,1,1)$$

---

### 12. Transformer 位置编码的作用？

- A. 提供位置信息
- B. 增加非线性
- C. 优化注意力权重
- D. 压缩序列长度

**答案：A** — Transformer 本身不感知顺序

---

### 13. 最大似然估计的核心思想？

- A. 最大化观测数据概率
- B. 最小化平方误差
- C. 最大化后验概率
- D. 矩匹配

**答案：A**

$$\hat{\theta} = \arg\max_\theta p(X|\theta)$$

---

### 14. 交叉熵损失主要用于？

- A. 回归
- B. 降维
- C. 分类
- D. 聚类

**答案：C**

$$L = -\sum y\log\hat{y}$$

---

### 15. 下列哪项不是泊松过程特征？

- A. 时间间隔服从指数分布
- B. 不相交区间事件独立
- C. 时间间隔服从正态分布
- D. 事件数服从泊松分布

**答案：C**

---

## 二、多选题

### 16. 自注意力机制（MHA / GQA / MQA）

- A. GQA 中多个 Query 头共享 KV，必须串行计算
- B. MHA→GQA/MQA 主要为减少推理阶段 KV 缓存显存
- C. 单头内 $d_q=d_k$，但 $d_v$ 可以不同
- D. 除以 $\sqrt{d_k}$ 防止 softmax 梯度饱和

**答案：B、C、D**

- ❌ A：GQA 仍可并行，只是共享 KV
- ✅ B：KV cache 压缩（推理友好）
- ✅ C：点积要求 $d_q=d_k$，$d_v$ 不受限
- ✅ D：$\text{Attention}=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

---

### 17. 多模态大模型（BLIP-2、LLaVA、GPT-4V）

- A. LLaVA 端到端预训练，完全移除视觉编码器
- B. 视觉适配层参数量远小于 LLM 主体，类似 PEFT
- C. GPT-4V 将视觉信息 token 化与文本一同送入 Transformer
- D. BLIP-2 采用冻结视觉编码器 + Q-former 对齐表示空间

**答案：B、C、D**

- ❌ A：LLaVA 仍使用视觉编码器
- ✅ B：Projector / Q-former ≈ 轻量适配层
- ✅ C：统一 Transformer 建模多模态 token
- ✅ D：BLIP-2 核心设计

---

### 18. 矩阵分解在推荐系统中的应用

- A. PCA 用于用户–物品特征降维去噪
- B. NMF 适合处理非负评分数据
- C. QR 分解用于求解线性方程和最小二乘
- D. SVD 是协同过滤经典方法

**答案：A、B、C、D**

$$R \approx U\Sigma V^\top \quad (\text{SVD})$$

---

### 19. 单位矩阵 $I_n$ 的性质

- A. 所有特征值均为 1
- B. 与同形矩阵相乘保持不变
- C. 秩等于 $n$
- D. $\det(I_n)=0$

**答案：A、B、C**

$$I_n A = A I_n = A, \quad \text{rank}(I_n)=n, \quad \det(I_n)=1$$

---

### 20. 连续型随机变量

- A. PDF 为偶函数 → 分布关于 0 对称
- B. CDF 一定连续
- C. $P(X=a)=0$
- D. PDF 某些点可以大于 1

**答案：A、B、C、D**

$$P(X=a) = \int_a^a f_X(x)\,dx = 0, \quad \int_{-\infty}^{\infty} f_X(x)\,dx = 1$$

---

> 📝 **待整理**: 错题整理（结构化）章节待补充

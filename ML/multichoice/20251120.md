# 2025-11-20 留学生AI方向题

## 一、单选题

### 1. 根据2020年 Kaplan 等人的 Tranformer “Scaling Laws”，若在给定计算预算下想最小化语言模型的训练损失，应当

* A. 同时按近似幂律比例增大模型参数数，训练token数和训练步数
* B. 增大词向量维度（Embedding Dimension）
* C. 增大模型参数数量（Model Size）
* D. 增大批大小（Batch Size）

**答案：A**

$$\text{Loss}\sim \text{power-law}(N,\ D,\ C)\ \Rightarrow\ \text{按幂律比例共同扩展更高效}$$

---

### 2. 设 S=R 为实对称矩阵，下列每项说法与“S正交对角化”等价？

* A. S 满足 S=1
* B. S 的特征向量可以组成一组正交基
* C. S 不可约
* D. S 的所有特征值都是正数

**答案：B**

$$S=Q\Lambda Q^T,\ Q\ \text{正交}\ \Leftrightarrow\ \text{存在一组正交特征向量基}$$

---

### 3. 假设输入图片的维度是 224×224×3，卷积神经网络的第一层中有5个卷积核，每个卷积核尺寸为7×7，具有1填充且步骤为1，并给出样例的输出的速度是多少

* A. 218×218×5
* B. 218×218×3
* C. 220×220×5
* D. 220×220×3

**答案：C**

$$H_{\text{out}}=\frac{H+2P-K}{S}+1=\frac{224+2\times 1-7}{1}+1=220,\ \text{通道}=5$$

---

### 4. 已知函数 f(x)=x^2，用中心差分公式，取步长 h=0.1，计算 f‘(1) 数值结果为

* A. 1.8
* B. 2.0
* C. 2.4
* D. 2.2

**答案：B**

$$f'(1)\approx \frac{f(1.1)-f(0.9)}{2h}=\frac{1.21-0.81}{0.2}=2.0$$

---

### 5. 给定数据：x=[1,2,3],y=[1,4,9]。用最小二乘法拟合直线 y=a+bx，则系数b为？

* A. 2.5
* B. 3.5
* C. 4.0
* D. 3.0

**答案：C**

$$b=\frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2}=4.0$$

---

### 6. 某购物APP推荐算法团队希望通过线性回归模型分析用户上个月平均每日在线时间x(单位：小时)对上个月

* x(小时) / Y(美元)
* 1.0 / 65
* 2.0 / 70
* 3.0 / 75
* 4.0 / 80
* 5。0 / 85

假设采用简单线性回归模型: y=w0+w1x，使用最小二乘法求解模型最优参数，下列哪一组参数是正确的

* A. w0=55.0,w1=6.0
* B. w0=58.0,w1=5.5
* C. w0=62.0,w1=4.5
* D. w0=60.0,w1=5.0

**答案：D**

$$y=5x+60\Rightarrow (w_0,w_1)=(60,5)$$

---

### 7. 如果随机变量X和Y的期望满足E(XY)=E(X)E(Y)，则下列哪项成立

* A. Var(X-Y)=Var(X)-Var(Y)
* B. Var(XY)=Var(X)Var(Y)
* C. Var(X^2-Y^2)=Var(X^2)+Var(Y^2)
* D. Var(X+Y)=Var(X)+Var(Y)

**答案：D**

$$\mathrm{Cov}(X,Y)=E(XY)-E(X)E(Y)=0\Rightarrow \mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)$$

---

### 8. 已知随机变量X和Y的协方差Cov(X,Y)=8，且 Var(X)=16,Var(Y)=25，则他们的相关系数 P_{X,Y} 为 0

* A. 0.4
* B. 0.2
* C. 0.8
* D. 0.6

**答案：A**

$$\rho_{X,Y}=\frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y)}}=\frac{8}{\sqrt{16\cdot 25}}=0.4$$

---

### 9. 在Transformer模型中，自注意力机制(Self-Attention)的核心作用是

* A. 替代卷积操作提取局部特征
* B. 减少参数数量
* C. 降低模型的计算复杂度
* D. 捕捉序列中不同位置元素之间的依赖关系

**答案：D**

$$\mathrm{Attn}(Q,K,V)=\mathrm{softmax}!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\ \Rightarrow\ \text{全局依赖建模}$$

---

### 10. 在推荐系统中，用户-物品评分矩阵 A∈R^{m×n} 通过SVD压缩为 A=UV（保留k个奇异值，k 远小于m和n的条件）后最小存储空间是？

* A. O(k iog nm)【缺失：请人工确认】
* B. O(knm)
* C. O((k+m+n)^2)
* D. O(k(m+n))

**答案：D**

$$\text{存 }U_k(m\times k)+\Sigma_k(k)+V_k(n\times k)\Rightarrow mk+nk+k=O(k(m+n))$$

---

### 11. 在回归问题中，以下哪种损失函数对异常值(outiers)最敏感?

* A. 平均绝对误差(MAE)
* B. Huber损失
* C. 均方误差(MSE)
* D. 对数损失(Log Loss)

**答案：C**

$$\text{MSE}=\frac1n\sum (y-\hat y)^2\ \Rightarrow\ \text{大残差平方放大}$$

---

### 12. 在一个二分类问题中，精确率(Precision)为0.8，召回率(Recall)为0.5，F1分数最接近如下哪个选项

* A. 0.4
* B. 0.62
* C. 0.7
* D. 0.31

**答案：B**

$$F1=\frac{2PR}{P+R}=\frac{2\times 0.8\times 0.5}{0.8+0.5}\approx 0.615\approx 0.62$$

---

### 13. 从一个盒子中随机抽取一个球，球有编号 1-5，其中编号1、2、3 是红球，编号4、5是蓝球、设事件 A 为 “抽到红球”，事件 B 为 “抽到奇数编号”，那么 P(A∩B)为？

* A. 4/5
* B. 1/5
* C. 2/5
* D. 3/5

**答案：C**

$$A\cap B={1,3},\ P(A\cap B)=\frac{2}{5}$$

---

### 14. 在反向传播过程中，如果发现某个特定层的学习速度显善慢于其他层，以下选项中，最可能的原因是？

* A. 该层的参数数量较少
* B. 该层的梯度消失
* C. 数据标准化不充分
* D. 学习率设置过低

**答案：B**

$$|\nabla W_\ell|\approx 0\Rightarrow \Delta W_\ell\approx 0\Rightarrow \text{该层更新慢}$$

---

### 15. 在训练超大规模语言模型时，为了在显存受限的情况下仍能使用较深的网络并保持梯度正确，常用的内存优化技术是

* A. 权重衰减(Weifht Decay)
* B. 梯度裁剪(Gradient Clipping)
* C. 随机失活(Dropout)
* D. 激活检查点(Activation Checkpointing)

**答案：D**

$$\text{用计算换显存：只存检查点激活，反传时重算中间激活}$$

---

## 二、多选题

### 16. 以下哪些方法可以用于缓解Transformer模型中的规度消失问题?

* A. 使用门控机制(Mamba，类似GRU、LSTM)控制信息流动失
* B. 使用残差连接(Residual Connection)，通过跳跃连接缓解梯度消失
* C. 采用层归一化(Layer Normalization)稳定训练过程
* D. 增加模型的层数，以增强模型的可表达性

**答案：B、C**

* ❌ A：不是 Transformer 体系中缓解梯度消失的典型手段
* ✅ B：残差 $x+\mathrm{SubLayer}(x)$ 让梯度可跨层直通
* ✅ C：LayerNorm 稳定数值范围，梯度更稳定
* ❌ D：加深通常更易加重梯度消失

---

### 17. 关于 MLE(Maximum Likelihood Estimation，最大似然估计)的说法，正确的是

* A. MLE 估计量总是无偏的
* B. EM 算法用于含隐变量的 MLE
* C. MLE 计算时常求其对数似然函数
* D. 当样本量 n 趋于无穷，MLE 服从正态分布

**答案：B、C、D**

* ❌ A：MLE 一般仅“渐近无偏”，有限样本可有偏
* ✅ B：EM 用于含隐变量/缺失数据的 MLE
* ✅ C：最大化 $\log L(\theta)$（乘积变和，便于求导）
* ✅ D：在正则条件下渐近正态：$$\hat\theta_{\text{MLE}}\overset{d}{\to}N(\theta_0,I^{-1}(\theta_0))$$

---

### 18. 假设某数据集中包括 100 个数据其最大值为 100 ，最小值为 1、那么，以下有关数据集基本统计量的定义中，错误的有

* A. 该数据集的众数一定严格大于1
* B. 该数据集的标准差一定严格大于1
* C. 该数据集的均值一定严格大于1
* D. 该数据集的中位数一定严格大于1

**答案：A、D**

* ❌ A：可构造 99 个 1、1 个 100 → 众数为 1
* ✅ B：至少含 1 和 100，离均值平方离差下界使 Std > 1
* ✅ C：存在至少一个 >1 且全体 ≥1 → 均值严格 >1
* ❌ D：同样构造下中位数可为 1

---

### 19. 利用 BPE算法在字节级语料上训练子词词表。给定如下 初始字符级切分与词频，请计算第一轮合并后可能的词表。

语料如下 (为结束符):

* 单词(字符序列) / 频次
  l o w / 5
  l o w e r / 2
  n e w e s t / 6
  w i d e s t / 3

* A.
  l o w
  l o w e r
  n e w e s t
  w i d e s t

* B.
  l ow
  l ow e r
  n e w e s t
  w i d e s t

* C.
  l o w
  l o we r
  n e we s t
  w i d e s t

* D.
  l o w
  l o w e r
  n e we s t
  w i d es t

**答案：C、D**

* ❌ A：未进行“最高频符号对”的合并
* ❌ B：对应合并 w e（频次更低，不是最高频对）
* ✅ C：第一轮可从最高频对中任选其一合并 → 该词表可能出现
* ✅ D：同理，选择另一最高频对合并 → 该词表也可能出现

---

### 20. 在深度学习模型训练过程中，选择合适的优化器对于加速收敛和提高模型性能至关重要。以下关于几种常见优化及其特性的描述

* A. 在大多数情况下，使用Adam优化器可以保证找到全局最优解，因为它能够动态调整学习率并结合了二阶矩估计、因此在所有类型的
* B. SGD(随机梯度下降)优化器虽然简单，但在处理非凸优化问题时比Adam更容易陷入局部最优解，因此在实际运用中很少使用SGD作为优化器
* C. RMSProp优化器通过指数加权平均的方式平滑梯度，从而解决了AdaGrd中学习率单调递减的问题，适用于处理非平稳目标函数
* D. Adam优化器结合了动量(Momentum)和RMSProp的优点，通过自适应地调整每个参数的学习率来加速收敛，并且通常不需要手动调节学习率
* E. AdaGrad优化器为每个参数分配不同的学习率，并随着参数更新次数的增加逐渐减小学习率，这使得它特别适合于稀疏数据场景下的训练，但可能【缺失：请人工确认】

**答案：C、D、E**

* ❌ A：Adam 不能保证全局最优，非凸下仍可能停在局部/鞍点
* ❌ B：SGD 仍非常常用，泛化上常有优势，“很少使用”不对
* ✅ C：RMSProp 用指数加权平方梯度，缓解 AdaGrad 学习率过快衰减
* ✅ D：Adam=Momentum+RMSProp 思路（自适应步长、收敛更快/更稳）
* ✅ E：AdaGrad 对稀疏特征友好，但学习率累积衰减可能致后期停滞

---

> 📝 **待整理**: 错题整理（结构化）章节待补充

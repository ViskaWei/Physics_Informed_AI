# 2025-11-06 留学生AI方向题

## 一、单选题

### 1. 在反向传播过程中，如果发现某个特定层的学习速度显看慢于其他层，以下选项中，最可能的原因是?

- A. 数据标准化不充分
- B. 该层的梯度消失
- C. 该层的参致数量较少
- D. 学习率设置过低

**答案：B**

$$\Delta w=-\eta\frac{\partial L}{\partial w},\ \frac{\partial L}{\partial w}\approx 0 \Rightarrow \Delta w\approx 0$$

---

### 2. 在经典的束搜索(beam searh)中，生成的结果往往偏向于短序列，这种长度偏差的根本原因是

- A. 序列越短，候选序列累计得分越不容易被稀释
- B. Beam width较小时生成的序列偏向于短序列
- C. 在解码前对所有序列长度做了统一的长度惩罚
- D. Transformer类的模型本身更偏好短序列

**答案：A**

$$\log P(y_{1:T})=\sum_{t=1}^{T}\log P(y_t\mid y_{<t},x),\ T\uparrow \Rightarrow \text{sum更负}$$

---

### 3. 训练集1000 000条样本，使用mini-batch SGD,batch_size=100，epochs=50，每个 epoch 内迭代次数为

- A. 50000
- B. 5000000
- C. 1000000
- D. 10000

**答案：D**

$$\text{iters/epoch}=\frac{1{,}000{,}000}{100}=10{,}000$$

---

### 4. 幂法(Power Method) 主要用于求解

- A. 矩阵的奇异值分解
- B. 矩阵的最小特征值
- C. 矩阵的主特征值(模最大的特征值)
- D. 矩阵的行列式

**答案：C**

$$x_{k+1}=\frac{Ax_k}{\|Ax_k\|}\Rightarrow x_k\to v_{\max}$$

---

### 5. 假设一个大模型Agent在解决数学推理任务时采用"反思-修正"机制，其核心流程为：
1.生成初步答案 A1;
2.通过独立模块生成反思信号 R ;
3.根据修正答案 A1→A2。
若训练时采用强化学习(RL)框架，则以下哪项最可能是奖励函数的设计目标?

- A. 最大化 R 与 A1 的语义相似度
- B. 最大化 A2 与真实答案 G 的匹配度，同时最小化 |A1−A2|
- C. 最大化 A1 与真实答案 G 的匹配度
- D. 最小化 A1 与 A2 的计算开销差异

**答案：B**

$$r=\text{match}(A_2,G)-\lambda|A_1-A_2|$$

---

### 6. 若attention计算中 QK^T 的分数矩阵为
$$\left[\begin{array}{ll}2 & 4 \\1 & 3\end{array}\right]$$
应用causal mask和softmax后的结果为哪一个(忽略缩放)?
注:causal mask矩阵为
$$\left[\begin{array}{ll}0 & -inf \\0 & 0\end{array}\right]$$
，注意力得分叠加 '0' 表示有效，叠加 '-inf' 经 softmax 后表示无效

- A. $$\left[\begin{array}{ll}1/3 & 2/3 \\1 & 0\end{array}\right]$$
- B. $$\left[\begin{array}{cc}1 & 0 \\\frac{e^{1}}{e^{1}+e^{3}} & \frac{e^{3}}{e^{1}+e^{3}}\end{array}\right]$$
- C. $$\left[\begin{array}{ll}1 & 0 \\0.25 & 0.75\end{array}\right]$$
- D. $$\left[\begin{array}{cc}\frac{e^{2}}{e^{2}+e^{4}} & \frac{e^{4}}{e^{2}+e^{4}} \\1 &0\end{array}\right]$$

**答案：B**

第一行变为 $[2,-\infty]$ → softmax 得 $[1,0]$；第二行 softmax 得 $\left[\frac{e}{e+e^3},\frac{e^3}{e+e^3}\right]$。

---

### 7. 在 集成学习的 Bagging 方法 中，以下哪一项最准确地描述了随机森林(Random Forest)的"特征随机选择"机制?

- A. 每棵树在分裂时仅使用单个特征进行分裂
- B. 每棵树在分裂时随机选择一部分特征进行分裂
- C. 每棵树在分裂时固定选择前几个特征进行分裂
- D. 每棵树在分裂时随机选择所有特征进行分裂

**答案：B**

每个节点在随机抽取的特征子集中选最优划分（mtry）。

---

### 8. 在多层感知器(MLP)中，隐藏层通过一系列非线性变换(由激活函数实现)对输入数据进行变化操作，这样的主要作用是什么?

- A. 确保模型的预测结果为线性可分
- B. 用于减少模型的训练时间
- C. 学习输入特征之间的复杂关系，并进行非线性变换
- D. 直接映射输入到输出，无需中间处理

**答案：C**

非线性激活让模型能拟合复杂函数族。

---

### 9. 设 A,P 均为 3 阶矩阵，P^T 为 P 的转置矩阵,且
$$P^{T} A P=\left(\begin{array}{lll}
1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 2\end{array}\right)$$
若 $$P=(\alpha_1,\alpha_2,\alpha_3),\ Q=(\alpha_1+\alpha_2,\alpha_2,\alpha_3)$$
则 $$Q^TAQ$$ 为 ()

- A. $$\left(\begin{array}{lll}1 & 1 & 0 \\1 & 2 & 0 \\0 & 0 & 2\end{array}\right)$$
- B. $$\left(\begin{array}{lll}2 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 2\end{array}\right)$$
- C. $$\left(\begin{array}{lll}2 & 1 & 0 \\1 & 1 & 0 \\0 & 0 & 2\end{array}\right)$$
- D. $$\left(\begin{array}{lll}1 & 0 & 0 \\0 & 2 & 0 \\0 & 0 & 2\end{array}\right)$$

**答案：C**

---

### 10. 对于可逆马氏链，下列描述正确的是

- A. 转移矩阵所有特征值为非负实数
- B. 满足细致平衡方程 $$\pi_{i} P_{i j}=\pi_{j} P_{j i}$$
- C. 必为非周期链
- D. 转移矩阵 P 必对称

**答案：B**

可逆链充要条件：存在平稳分布 $\pi$ 满足细致平衡。

---

### 11. 在训练一个多分类的深度学习模型时(例如图像识别)，我们通常使用Softmax作为输出层的激活函数，并选择交叉(Cr0ss-Entropy)作为损失函数。从概率论和论的角度看，最小化交叉熵损失函数在本质上等价于什么?

- A. 确保模型每一层的输出都服从标准正态分布。
- B. 最小化模型参数的L2范数，以达到正则化效果。
- C. 最大化模型预测分布与均匀分布之间的KL散度。
- D. 执行最大似然估计(MLE)，即最大化在给定输入下，观测到真实标签的条件概率。

**答案：D**

$$\min \text{CE} \equiv \min(-\log p_\theta(y|x)) \equiv \max \log p_\theta(y|x)$$

---

### 12. 如果随机变量 X 和 Y 的期望满足 E(XY)=E(X)E(Y) ，则下式哪项成立

- A. Var(X+Y)=Var(X)+Var(Y)
- B. Var(X-Y)=Var(X)-Var(Y)
- C. Var(XY)=Var(X)Var(Y)
- D. Var(X²+Y²)=Var(X²)+Var(Y²)

**答案：A**

由 $E(XY)=E(X)E(Y)\Rightarrow \text{Cov}(X,Y)=0$。

---

### 13. 、从一个盒子中随机抽取一个球，球有编号 1~5，其中编号 1、2、3 是红球，编号 4、5 是蓝球。设事件  为"抽到红球"，事件 B 为"抽到奇数编号”，那么 P(A∩B)为?

- A. $$\frac{3}{5}$$
- B. $$\frac{4}{5}$$
- C. $$\frac{2}{5}$$
- D. $$\frac{1}{5}$$

**答案：C**

红且奇数：{1,3} 共2个；总共5个球。

---

### 14. 函数 f(x)=sin(x) 在点 x0=0 处二阶泰勒展开式为:

- A. f(x)=x
- B. f(x)=cos(x)
- C. f(x)=1+x
- D. f(x)=x²

**答案：A**

$$\sin x = x-\frac{x^3}{6}+\cdots$$

---

### 15. 设线性变换 T: Rn→Rm 在标准基下的矩阵为 A。若将 Rn 的基换成可逆矩阵 P 的列向量，将 Rm 的基换成可逆矩阵 Q 的列向量，则 T 在新基下的矩阵表示为

- A. $$Q^{-1}AP$$
- B. $$QAP$$
- C. $$QAP^{-1}$$
- D. $$Q^{-1}AP^{-1}$$

**答案：A**

$$x=P\tilde x,\ y=Ax,\ y=Q\tilde y\Rightarrow \tilde y=Q^{-1}AP\tilde x$$

---

## 二、多选题

### 16. 在机器学习中，以下哪些方法可以用于处理类别不平衡问题?

- A. 对少数类样本进行过采样(Oversampling)
- B. 使用F1分数作为评价指标
- C. 使用AUC-ROC曲线作为评价指标
- D. 对多数类样本进行欠采样(Undersampling)

**答案：A、B、C、D**

- ✅ A：直接提高少数类占比
- ✅ B：F1 更关注少数类的 Precision/Recall 权衡
- ✅ C：AUC-ROC 对阈值/比例更稳健
- ✅ D：降低多数类占比

---

### 17. 在构建一个基于Transformer的大语言模型时，会涉及到多种概率和统计相关的技术。以下哪些选项是正确的应用?

- A. 词嵌入 (Word Embeddings) 的过程，可以看作是将一个服从离散分布的词汇表，映射到一个高维连续向量空间中。
- B. Layer Normalization 通过计算一个层内所有神经元输出的均值和方差来进行归一化，目的是稳定训练过程中的数据分布。
- C. Softmax函数被用在注意力机制和最后的输出层，将原始分数 (logits) 转换为一个合法的概率分布。
- D. 在训练初期，模型权重通常从一个特定的概率分布 (如 Xavier 或 He 初始化，它们均基于均匀或正态分布)中进行采样，以帮助缓解梯度消失或梯度爆炸问题

**答案：A、B、C、D**

- ✅ A：离散ID → 连续向量空间
- ✅ B：按样本/特征维归一化稳定训练
- ✅ C：logits → 概率分布（注意力/分类）
- ✅ D：合适初始化缓解梯度消失/爆炸

---

### 18. 在深度学习领域，softmax函数的作用有哪些

- A. 通常用于多分类模型中
- B. 用于损失函数的计算，特别是在交叉熵损失函数中
- C. 标准化模型输出，使其成为概率分布
- D. 作为激活函数，一般用于分类网络最后一层

**答案：A、B、C、D**

- ✅ A：多分类常用输出层
- ✅ B：与交叉熵/NLL配套
- ✅ C：归一化为概率分布
- ✅ D：常作最后一层激活

---

### 19. 关于 RLHE(Reinforcement Learning from Human Feedback)及其常见替代技术，下列说法正确的是(可多选)

- A. RLHF 中常用 PPO 对策略进行强化学习更新，最大化奖励模型给出的标量奖励的期望，并通过KL正则约束策略与参考模型的偏差
- B. DPO(Direct Preference Optimization)跳过显式训练奖励模型，直接最小化人类偏好对比损失
- C. 自回归语言模型在进行RLHF 更新后，必须重新进行教师强迫(teacher forcing)微调，才能保证采样稳定
- D. RLAIF(RL from AI Feedback)用另一个冻结大模型产生偏好数据，可减少人工标注成本

**答案：A、B、D**

- ✅ A：PPO + KL 约束是典型 RLHF 配置
- ✅ B：DPO 直接用偏好对比损失训练
- ❌ C：并非“必须”流程
- ✅ D：用 AI 反馈降低人工标注成本

---

### 20. 在大型语言模型(LLM)的文本处理流程中，有关词嵌入(Embedding)和分词器(Tokenizer)的描述，下列哪些选项是正确的?

- A. 在 Transformer 架构中、位置编码 (Positional Encoding) 被添加到词嵌入向量中，以使模型能够学习到输入序列中词语的位置信息，这对于理解句子结构至关重要。
- B. 在预训练阶段，BERT 使用的 Wordpiece 分词器能够将输入文本切分成子词单元 (subwordtokens)，以解决未登录词 (OOV,Out-Of-Vyocabulary) 问题。
- C. 无论是基于 Transformer 架构的编码器还是解码器模型，在进行文本处理时，首先需要通过一个特定的分词器将输入文本转换为对应的 token ID 序列，然后再传递给模型进行后续处理。
- D. 【缺失：请人工确认】

**答案：A、B、C**

- ✅ A：位置编码 + 词向量提供序列位置信息
- ✅ B：WordPiece 子词化缓解 OOV
- ✅ C：文本 → tokenizer → token IDs → 模型
- ❌ D：网页未提供该选项内容，无法判断


## ✅ 错题整理（结构化）

> **整理日期**: 2026-01-01  
> **统计**: 20 题，4 错，正确率 80%

---

### Q2: Beam Search 长度偏差的根本原因

**原题**：
> 在经典的束搜索(beam search)中，生成的结果往往偏向于短序列，这种长度偏差的根本原因是
> - A. 序列越短，候选序列累计得分越不容易被稀释
> - B. Beam width较小时生成的序列偏向于短序列
> - C. 在解码前对所有序列长度做了统一的长度惩罚
> - D. Transformer类的模型本身更偏好短序列

**正确答案**：A

**易错点**：
误选 C。C 描述的是**修复长度偏差的方法**（length normalization），而不是根本原因。

**核心公式/结论**：
$$
\log P(y_{1:T}\mid x)=\sum_{t=1}^{T}\log P(y_t\mid y_{<t},x)
$$
因为 $P(\cdot)\le 1$，所以 $\log P(\cdot)\le 0$。**步数越多，累加越多负数，分数更低** → 偏向短句。

**一眼记住**：
- Beam search 默认用**累加对数概率**打分 → 越长越负 → 偏短
- 长度惩罚 $\frac{1}{T^\alpha}$ 是**修复手段**，不是原因

---

### Q9: 矩阵换基变换 $Q^TAQ$

**原题**：
> 设 $P^TAP=\mathrm{diag}(1,1,2)$，$P=(\alpha_1,\alpha_2,\alpha_3)$，$Q=(\alpha_1+\alpha_2,\alpha_2,\alpha_3)$，求 $Q^TAQ$

**正确答案**：C
$$
Q^TAQ=\begin{pmatrix}2&1&0\\1&1&0\\0&0&2\end{pmatrix}
$$

**易错点**：
不知道怎么算，被矩阵形式绕晕。

**核心公式/结论**：
关键洞察：**在 $\{\alpha_1,\alpha_2,\alpha_3\}$ 基下，二次型由对角阵给出**：
$$
u^TAu = x_1^2\cdot\lambda_1 + x_2^2\cdot\lambda_2 + x_3^2\cdot\lambda_3
$$

$(Q^TAQ)_{ij}=q_i^TAq_j$ 就是**带权点积**：
$$
q_i^TAq_j = \lambda_1 x_{i1}x_{j1}+\lambda_2 x_{i2}x_{j2}+\lambda_3 x_{i3}x_{j3}
$$

**一眼记住**：
- $q_1=\alpha_1+\alpha_2$ 在 $\alpha$ 基下坐标 $(1,1,0)$
- $q_2=\alpha_2$ 坐标 $(0,1,0)$，$q_3=\alpha_3$ 坐标 $(0,0,1)$
- 逐项算带权点积即可

<details>
<summary>详细计算</summary>

- $q_1^TAq_1 = 1\cdot1^2+1\cdot1^2+2\cdot0^2=2$
- $q_1^TAq_2 = 1\cdot(1\cdot0)+1\cdot(1\cdot1)+2\cdot(0\cdot0)=1$
- $q_1^TAq_3 = 0$
- $q_2^TAq_2 = 1$
- $q_2^TAq_3 = 0$
- $q_3^TAq_3 = 2$

</details>

---

### Q11: 交叉熵损失本质上等价于什么

**原题**：
> 使用Softmax + 交叉熵作为损失函数，从概率论角度看，最小化交叉熵损失函数在本质上等价于什么？
> - C. 最大化模型预测分布与均匀分布之间的KL散度
> - D. 执行最大似然估计(MLE)

**正确答案**：D

**易错点**：
误选 C。C 说的是"最大化与均匀分布的 KL 散度"，这是**完全不同的目标**。

**核心公式/结论**：
$$
\min \text{CE} = \min\left(-\log q_\theta(y|x)\right) \equiv \max \log q_\theta(y|x) = \text{MLE}
$$

交叉熵与 KL 散度的关系：
$$
H(p,q_\theta)=H(p)+\mathrm{KL}(p\|q_\theta)
$$
因为 $H(p)$ 与 $\theta$ 无关，**最小化交叉熵 ⇔ 最小化 KL(p||q)**。

**一眼记住**：
- 交叉熵 = NLL = -log likelihood
- **最小化 CE ⇔ 最大化似然（MLE）**
- C 选项说"最大化"某个 KL，方向完全反了

---

### Q15: 线性变换换基后的矩阵表示

**原题**：
> 线性变换 $T: \mathbb{R}^n\to\mathbb{R}^m$ 在标准基下矩阵为 $A$。将 $\mathbb{R}^n$ 的基换成 $P$ 的列向量，$\mathbb{R}^m$ 的基换成 $Q$ 的列向量，则 $T$ 在新基下的矩阵表示为？

**正确答案**：A. $Q^{-1}AP$

**易错点**：
不理解换基公式的推导。

**核心公式/结论**：
$$
\begin{aligned}
&\text{旧坐标} \to \text{新坐标}: x = P\tilde{x},\ y = Q\tilde{y} \\
&y = Ax = A(P\tilde{x}) \\
&Q\tilde{y} = AP\tilde{x} \\
&\tilde{y} = Q^{-1}AP\tilde{x}
\end{aligned}
$$

**一眼记住**：
- **新基矩阵 = $Q^{-1}AP$**
- $P$ 管输入空间换基（右乘），$Q^{-1}$ 管输出空间换基（左乘）
- 特例：同一空间换基 $P=Q$ → $P^{-1}AP$（相似变换）

---

## 📊 知识点统计（2025-11-06）

| 题号 | Tag1 | Tag2 | 是否错题 |
|------|------|------|----------|
| Q1 | 深度学习基础 | - | ✅ |
| Q2 | 生成与评测 | - | ❌ |
| Q3 | 深度学习基础 | - | ✅ |
| Q4 | 线性代数 | 优化与数值方法 | ✅ |
| Q5 | 强化学习与对齐 | - | ✅ |
| Q6 | 注意力与Transformer | - | ✅ |
| Q7 | 经典机器学习 | - | ✅ |
| Q8 | 深度学习基础 | - | ✅ |
| Q9 | 线性代数 | - | ❌ |
| Q10 | 概率统计 | - | ✅ |
| Q11 | 概率统计 | 深度学习基础 | ❌ |
| Q12 | 概率统计 | - | ✅ |
| Q13 | 概率统计 | - | ✅ |
| Q14 | 其他(数学分析) | - | ✅ |
| Q15 | 线性代数 | - | ❌ |
| Q16 | 经典机器学习 | - | ✅ |
| Q17 | 深度学习基础 | 注意力与Transformer | ✅ |
| Q18 | 深度学习基础 | - | ✅ |
| Q19 | 强化学习与对齐 | - | ✅ |
| Q20 | 注意力与Transformer | - | ✅ |

### 按知识点汇总

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 线性代数 | 3 | 2 | 33% |
| 生成与评测 | 1 | 1 | 0% |
| 概率统计 | 5 | 1 | 80% |
| 深度学习基础 | 6 | 0 | 100% |
| 注意力与Transformer | 3 | 0 | 100% |
| 经典机器学习 | 2 | 0 | 100% |
| 强化学习与对齐 | 2 | 0 | 100% |
| 其他(数学分析) | 1 | 0 | 100% |

### 🔥 薄弱环节 Top-3（2025-11-06）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 生成与评测 | 1/1 | 0% | 样本少 |
| 2 | 线性代数 | 2/3 | 33% | 换基变换 |
| 3 | 概率统计 | 1/5 | 80% | 交叉熵/MLE |


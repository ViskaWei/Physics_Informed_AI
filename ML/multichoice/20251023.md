## ✅ 错题整理（结构化）

> 题源：CodeFun2000 #P4276（2025年10月23日-AI方向(留学生)）
> 说明：当前输入未包含“你选了什么/错题区/多轮追问”，因此“是否错题/追问”暂为待补充。

---

## Q1: 聚类任务正确说法

**原题**：
> 1、关于机器学习聚类任务的下列说法中，哪一项是正确的?
>
> A. DBSCAN 要求所有点必须属于某一簇，因此不能处理含有噪声的数据集，否则无法收敛到稳定结果  
> B. 谱聚类通过图拉普拉斯矩阵的特征分解降维，并在低维空间中使用 K-means 进行聚类。  
> C. 层次聚类通过迭代优化目标函数来寻找最优聚类划分，其结果依赖于初始聚类中心的选择。  
> D. K-means 聚类使用中位数作为簇中心，因此对噪声和离群点具有较强鲁棒性。

**正确答案**：B

**易错点**：
- DBSCAN 允许噪声点；层次聚类不依赖“初始中心”；K-means 用均值不鲁棒（中位数更像 K-medoids）。

**核心公式/结论**：
- 谱聚类：先对图拉普拉斯矩阵做特征分解得到嵌入，再在嵌入空间做 K-means（概念性流程）。

**一眼记住**：待补充

**追问（待补充）**：待补充

---

## Q2: L1/L2 正则化

**原题**：
> 2、关于 L1 正则化(Lasso) 和 L2 正则化(Ridge) 的描述，下列说法正确的是  
> A. 当训练数据存在多重共线性时，L1 正则化的效果通常优于 L2 正则化  
> B. L2 正则化会导致参数值变得更大，从而增强模型对输入特征的敏感性  
> C. L1 正则化会使模型参数更倾向于稀疏化，即更多参数变为 0

**正确答案**：C

**易错点**：
- L2 是“收缩”不是变大；L1 会促使部分系数变为 0（稀疏）。

**核心公式/结论**：
$$
\min_w \; \mathcal{L}(w) + \lambda \|w\|_1 \quad \Rightarrow \quad w \text{ 更稀疏}
$$

**一眼记住**：待补充  
**追问（待补充）**：待补充

---

## Q3: SVD 维度与性质

**原题**：
> 3、矩阵 A 是一个 m×n 的实矩阵，其奇异值分解为 A=U\Sigma V^T，其中 U、\Sigma、V 分别为相应维度的矩阵。下列关于这三个矩阵的说法，正确的是()  
> A. 若 A 是可逆方阵，则 \Sigma 的对角元素均为 0  
> B. U 是 m×n 的正交矩阵，且其列向量是 A^TA 的特征向量  
> C. \Sigma 是 m×n 的对角矩阵，对角元素非负且按降序排列  
> D. V 是 m×m 的正交矩阵，且其列向量是 A^TA 的特征向量

**正确答案**：C

**易错点**：
- 可逆 ⇒ 奇异值非零；U 的列向量对应 AA^T；V 对应 A^TA，且 V 是 n×n。

**核心公式/结论**：
$$
A = U\Sigma V^T,\quad \Sigma=\mathrm{diag}(\sigma_1\ge \sigma_2\ge\cdots\ge 0)
$$

**一眼记住**：待补充  
**追问（待补充）**：待补充

---

## Q4: Transformer 里 LayerNorm 的关键原因

**原题**：
> 4、在 Transformer 的架构中…相比于 Batch Normalization 选择 LayerNorm 最关键的原因是？  
> A. LayerNorm 的计算量远小于 BatchNorm，可以显著提升训练速度  
> B. LayerNorm 计算归一化统计量独立于 batch 其他样本，对变长序列/变 batch size 更鲁棒  
> C. BatchNorm 只能用于卷积层，而 LayerNorm 可以用于全连接层  
> D. LayerNorm 能够更好处理 OOV 单词

**正确答案**：B

**易错点**：
- LN 不依赖 batch 统计量；BN 不是只能用于卷积；OOV 与归一化无关。

**核心公式/结论**：
- LN：对单样本特征维做归一化（概念结论）。

**一眼记住**：待补充  
**追问（待补充）**：待补充

---

## Q5: 贝叶斯后验

**原题**：
> 5、已知从人群中随机抽取一名男性，该男性年龄在 30–40 岁之间的概率为 0.25…且两段年龄男性比例都为 0.4…用贝叶斯估计该男性属于 30–40 岁的后验概率为：  
> A. 0.25  B. 0.4  C. 0.5  D. 0.1

**正确答案**：A（0.25）

**易错点**：
- 两类里“男性比例”相同会抵消，后验回到先验 0.25。

**核心公式/结论**：
$$
P(30\text{–}40\mid M)=\frac{0.25\times 0.4}{0.25\times 0.4+0.75\times 0.4}=0.25
$$

**一眼记住**：待补充  
**追问（待补充）**：待补充

---

## Q6: 矩阵 2-范数与 Frobenius 范数

**原题**：
> 6、关于矩阵的 2-范数 \|A\|_2 和 Frobenius 范数 \|A\|_F，下列说法正确的是  
> A. 二者都等于矩阵元素绝对值之和  
> B. \|A\|_2 是最大特征值，\|A\|_F 是特征值平方和  
> C. 对任意矩阵，\|A\|_2=\|A\|_F  
> D. \|A\|_2 是最大奇异值，\|A\|_F 是奇异值平方和的平方根

**正确答案**：D

**核心公式/结论**：
$$
\|A\|_2=\sigma_{\max},\qquad 
\|A\|_F=\sqrt{\sum_i \sigma_i^2}
$$

---

## Q7: 相似矩阵与行列式

**原题**：
> 7、已知 3 阶矩阵 A 与 B 相似,且 A 满足 |A+E|=|A-2E|=|2A+3E|=0，则 |B+3E|=()  
> A. 20  B. 15  C. 0  D. 50  （注: E 为单位矩阵）

**正确答案**：B（15）

**核心公式/结论**：
- 由 |A+E|=0 得 -1 为特征值；|A-2E|=0 得 2 为特征值；|2A+3E|=0 得 -\tfrac32 为特征值。  
- 相似 ⇒ 特征值相同，且
$$
|B+3E|=\prod_i(\lambda_i+3)=(2)(5)\left(\tfrac32\right)=15
$$

---

## Q8: 连续两个 6 的期望次数

**原题**：
> 8、一个均匀 6 面骰子，平均投掷多少次能投出连续的两个 6 点  
> A. 36  B. 42  C. 48  D. 37

**正确答案**：B（42）

**核心公式/结论**：
- 设 p=\tfrac16, q=\tfrac56，状态方程（页面给出）：
$$
E_0=1+pE_1+qE_0,\qquad E_1=1+qE_0 \Rightarrow E_0=42
$$

---

## Q9: Causal mask (L=3)

**原题**：
> 9、若输入序列长度为 3，Causal Mask 的矩阵形式是  
> A. 全 0 矩阵  
> B. \begin{bmatrix}0&0&0\\-\infty&0&0\\-\infty&-\infty&0\end{bmatrix}  
> C. \begin{bmatrix}0&-\infty&0\\0&0&-\infty\\-\infty&0&0\end{bmatrix}  
> D. \begin{bmatrix}0&-\infty&-\infty\\0&0&-\infty\\0&0&0\end{bmatrix}

**正确答案**：D

**核心公式/结论**：
$$
\text{mask}_{ij}=
\begin{cases}
0,& j\le i\\
-\infty,& j>i
\end{cases}
$$

---

## Q10: 由 \mu 的置信区间推 P(X\le 0) 的置信区间

**原题**：
> 10、假设总体 X\sim N(\mu,1)，其均值 \mu 的 95% 置信区间为 [0.31,1.56]，则概率 P(x\le 0) 的 95% 置信区间为  
> A. [1-\Phi(1.56),\,1-\Phi(0.31)]  
> B. [0.31,1.56]  
> C. [0,\Phi(1.56)]  
> D. [\Phi(0.31),\Phi(1.56)]

**正确答案**：A

**核心公式/结论**：
$$
P(X\le 0)=\Phi(-\mu)=1-\Phi(\mu)
$$

---

## Q11: LoRA 参数占比估算

**原题**：
> 11、使用 LoRA rank=64 对 LLAMA 7B（hidden size=4096，32 层）微调，仅对 attention 的 query & value 矩阵加 LoRA，可训练参数量占原模型总量约  
> A. 1.00%  B. 0.05%  C. 0.50%  D. 0.10%

**正确答案**：C（0.50%）

**核心公式/结论**：
$$
\Delta N = r(d_{\text{in}}+d_{\text{out}})=64(4096+4096)=524{,}288
$$
- 对 Q、V 两个矩阵、32 层合计约 33.6M，占 7B 约 0.48%\approx 0.50%。

---

## Q12: Swish 极限

**原题**：
> 12、Swish: f(x)=x\cdot\sigma(\beta x)，当 \beta\to\infty 时逼近哪个函数  
> A. tanh  B. 线性  C. sigmoid  D. ReLU

**正确答案**：D（ReLU）

**核心公式/结论**：
$$
\sigma(\beta x)\to \mathbf{1}_{x>0}\quad\Rightarrow\quad x\sigma(\beta x)\to \max(0,x)
$$

---

## Q13: 幂法一次迭代

**原题**：
> 13、用幂法求矩阵 A=\begin{bmatrix}2&1\\1&2\end{bmatrix} 的主特征值，初始向量 v_0=[1,0]^T，迭代一次后的结果是  
> A. [0,0]^T  B. [1,2]^T  C. [3,3]^T  D. [2,1]^T

**正确答案**：D（[2,1]^T）

**核心公式/结论**：
$$
v_1=Av_0=\begin{bmatrix}2&1\\1&2\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix}=\begin{bmatrix}2\\1\end{bmatrix}
$$

---

## Q14: 残差连接作用

**原题**：
> 14、Transformer 模型中残差连接的主要作用是  
> A. 减少计算量  B. 增加非线性  C. 增强位置编码  D. 缓解梯度消失，帮助深层训练

**正确答案**：D

---

## Q15: F1 计算

**原题**：
> 15、TP=3，TN=4，FP=1，FN=2，那么 F1 值为()  
> A. 0.75  B. 0.5  C. 0.67  D. 0.6

**正确答案**：C（0.67）

**核心公式/结论**：
$$
P=\frac{TP}{TP+FP}=\frac34,\quad R=\frac{TP}{TP+FN}=\frac35,\quad
F1=\frac{2PR}{P+R}=\frac{0.9}{1.35}\approx 0.67
$$

---

## Q16: 矩阵分解正确表述（多选）

**原题**：
> 16、下列关于矩阵分解的表述哪些是正确的()  
> A. 对任意矩阵 A(实或复)，SVD 分解一定存在  
> B. LU 分解总是存在且唯一  
> C. QR 分解中的 Q 矩阵可以不是方阵  
> D. SVD 分解中的 \Sigma 矩阵包含矩阵的奇异值

**正确答案**：A、C、D

---

## Q17: Scaled Dot-Product Attention 缩放因子作用（多选）

**原题**：
> 17、Scaled Dot-Product Attention 中缩放因子 \sqrt{d_k} 的作用是  
> A. 防止点积数值过大导致梯度消失  
> B. 补偿 query 和 key 向量维度的影响  
> C. 使 Softmax 输出更均匀  
> D. 减少计算量

**正确答案**：A、B、C

---

## Q18: 线性回归（多选）

**原题**：
> 18、关于线性回归的描述中，哪些是正确的?  
> A. 残差必须正态  
> B. 目标是最大化误差  
> C. 可用最小二乘法估计参数  
> D. 假设自变量与因变量近似线性

**正确答案**：C、D

---

## Q19: 事件独立/互斥/概率为 0 或 1（选“错误”的，多选）

**原题**：
> 19、设 A,B 为任意两个随机事件，则以下说法错误的是()  
> A. 若 A 和 B 独立，则二者一定互不相容(AB=\emptyset)  
> B. 若 A 和 B 互不相容(AB=\emptyset)，则二者一定独立  
> C. 若 P(A)=0，则 A 一定是不可能事件  
> D. 若 P(B)=1，则 B 不一定是必然事件

**正确答案**：A、B、C

---

## Q20: 多项式插值（多选）

**原题**：
> 20、以下关于多项式插值的说法中，正确的有  
> A. 对 6 个互异节点，满足插值条件的 6 次插值多项式存在且唯一  
> B. 三次样条是分段多项式，因此结果一定无法仅用一个多项式表示  
> C. 龙格现象：等距节点、次数升高时端点附近振荡/误差增大，易在多项式插值出现  
> D. 拉格朗日插值法属于多项式插值

**正确答案**：C、D

---

## 📌 知识点标注（Q1–Q20）

> 说明：根据追问内容识别错题（Q1,Q2,Q3,Q5,Q6,Q7,Q10,Q13,Q16,Q19,Q20 = 11题）

| 题号 | Tag1 | Tag2 | 是否错题 |
|---|---|---|---|
| Q1 | 经典机器学习 | - | ❌ 错 |
| Q2 | 经典机器学习 | - | ❌ 错 |
| Q3 | 线性代数 | - | ❌ 错 |
| Q4 | 深度学习基础 | 注意力与Transformer | ✅ 对 |
| Q5 | 概率统计 | - | ❌ 错 |
| Q6 | 线性代数 | - | ❌ 错 |
| Q7 | 线性代数 | - | ❌ 错 |
| Q8 | 概率统计 | - | ✅ 对 |
| Q9 | 注意力与Transformer | - | ✅ 对 |
| Q10 | 概率统计 | - | ❌ 错 |
| Q11 | 注意力与Transformer | - | ✅ 对 |
| Q12 | 深度学习基础 | - | ✅ 对 |
| Q13 | 线性代数 | 优化与数值方法 | ❌ 错 |
| Q14 | 深度学习基础 | - | ✅ 对 |
| Q15 | 概率统计 | - | ✅ 对 |
| Q16 | 线性代数 | - | ❌ 错 |
| Q17 | 注意力与Transformer | - | ✅ 对 |
| Q18 | 经典机器学习 | - | ✅ 对 |
| Q19 | 概率统计 | - | ❌ 错 |
| Q20 | 数值方法与插值 | - | ❌ 错 |

---


## 📊 知识点掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 线性代数 | 5 | 5 | 0% ⚠️ |
| 概率统计 | 5 | 3 | 40% |
| 经典机器学习 | 3 | 2 | 33% |
| 优化与数值方法 | 1 | 1 | 0% |
| 数值方法与插值 | 1 | 1 | 0% |
| 深度学习基础 | 3 | 0 | 100% ✅ |
| 注意力与Transformer | 4 | 0 | 100% ✅ |

---

## 🔥 薄弱环节 Top-3（20251023）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 线性代数 | 5/5 | 0% | SVD/范数/特征值/LU-QR分解 |
| 2 | 经典机器学习 | 2/3 | 33% | 聚类算法、L1/L2正则化 |
| 3 | 概率统计 | 3/5 | 40% | 贝叶斯/置信区间/独立vs互斥 |

---

## 📝 核心知识点速查（从追问中提炼）

### 经典机器学习

**K-means 簇中心**：
$$\mu_k = \frac{1}{|C_k|}\sum_{x_i \in C_k} x_i$$
- 用**均值**，对离群点极敏感
- 若用中位数 → K-medoids

**谱聚类流程**：
1. 相似度矩阵：$W_{ij} = \exp\left(-\frac{\|x_i-x_j\|^2}{2\sigma^2}\right)$
2. 度矩阵：$D_{ii} = \sum_j W_{ij}$
3. 拉普拉斯矩阵：$L = D - W$
4. 特征分解：$L u_k = \lambda_k u_k$
5. 取前 k 个特征向量 → K-means

**DBSCAN**：基于密度，自动识别**噪声点**（核心点/边界点/噪声点）

**层次聚类**：贪心合并/拆分，**无目标函数，无初始中心**

**L1 vs L2 正则化**：
$$\text{Ridge: } \min_w \|y-Xw\|_2^2 + \lambda \|w\|_2^2 \quad \Rightarrow \text{权重均匀分摊，稳定}$$
$$\text{Lasso: } \min_w \|y-Xw\|_2^2 + \lambda \|w\|_1 \quad \Rightarrow \text{稀疏化，特征选择}$$
- 多重共线性下：**L2（Ridge）更稳定**

---

### 线性代数

**SVD 分解**：$A = U\Sigma V^T$，$A \in \mathbb{R}^{m \times n}$

| 矩阵 | 维度 | 特征向量来源 |
|------|------|-------------|
| $U$ | $m \times m$ | $AA^T$ 的特征向量 |
| $\Sigma$ | $m \times n$ | 对角元素 $\sigma_i \ge 0$ 降序 |
| $V$ | $n \times n$ | $A^TA$ 的特征向量 |

**矩阵范数**：
$$\|A\|_2 = \sigma_{\max} \quad \text{（最大拉伸 = 最大奇异值）}$$
$$\|A\|_F = \sqrt{\sum_i \sigma_i^2} = \sqrt{\sum_{i,j} a_{ij}^2} \quad \text{（整体能量）}$$

**行列式 vs 迹**：
$$|A| = \prod_i \lambda_i \quad \text{（特征值之积）}$$
$$\mathrm{tr}(A) = \sum_i \lambda_i \quad \text{（特征值之和）}$$
$$|A + cI| = \prod_i (\lambda_i + c)$$

**矩阵分解对比**：

| 分解 | 要求方阵 | 一定存在 | 唯一性 |
|------|---------|---------|--------|
| LU | ✅ | ❌（需LUP） | ❌ |
| QR | ❌ | ✅ | ❌（差符号） |
| SVD | ❌ | ✅ | 值唯一 |
| Cholesky | ✅ SPD | ❌ | ✅ |

---

### 概率统计

**贝叶斯公式**：
$$P(A|M) = \frac{P(M|A)P(A)}{P(M)}$$
- 当 $P(M|A) = P(M|\bar{A})$ 时，**后验 = 先验**（无区分度）

**独立 vs 互斥**：
- 独立：$P(A \cap B) = P(A)P(B)$
- 互斥：$A \cap B = \emptyset \Rightarrow P(A \cap B) = 0$
- ⚠️ 若互斥且独立：$P(A)P(B) = 0$，即**至少一个概率为0**
- **除非概率为0，否则互斥 ≠ 独立**

**置信区间映射**：若 $X \sim N(\mu, 1)$
$$P(X \le 0) = \Phi(-\mu) = 1 - \Phi(\mu)$$
- $\mu \in [a, b] \Rightarrow P(X \le 0) \in [1-\Phi(b), 1-\Phi(a)]$

---

### 数值方法

**幂法求主特征值**：
$$v_{k+1} = Av_k$$
- 迭代收敛到最大特征向量方向
- 特征值 = Rayleigh 商：$\lambda = \frac{v^T Av}{v^T v}$

**多项式插值**：
- $n+1$ 个点 → $\le n$ 次多项式（唯一）
- **龙格现象**：等距节点 + 高次多项式 → 端点振荡

---

<details>
<summary>📎 追问详情（点击展开）</summary>

好，这一组问题我**逐条、从直觉 → 定义 → 数学/例子**给你彻底捋清楚。你这套题本身覆盖了**聚类 / 正则化 / 线代 / 概率 / 数值分析 / 插值理论**，很多“看不懂”的点其实是**题目省略了建模背景**，不是你理解能力的问题。

---

## （1）第 1 题：每种聚类方法用**最简单公式/机制**解释（尤其 D 为什么错）

### ✅ 谱聚类（B 正确）

**核心思想**：
把数据点看成图上的点，用相似度构图 → 用**图拉普拉斯矩阵**降维 → 在低维空间用 K-means。

* 相似度矩阵：
  [
  W_{ij} = \exp\left(-\frac{|x_i-x_j|^2}{2\sigma^2}\right)
  ]

* 拉普拉斯矩阵（常见一种）：
  [
  L = D - W
  ]

* 做特征分解：
  [
  L u_k = \lambda_k u_k
  ]

* 取前 (k) 个特征向量 → 每行当作新特征 → **K-means**

👉 **所以 B 完全正确**

---

### ❌ DBSCAN（A 错）

DBSCAN **不要求每个点都属于某一簇**：

* 有三类点：

  * **核心点**
  * **边界点**
  * **噪声点（outlier）**

**错误点**：

> “不能处理含噪声数据集”

👉 **恰恰相反：DBSCAN 是专门为“有噪声”设计的**

---

### ❌ 层次聚类（C 错）

层次聚类是 **贪心合并/拆分**：

* 自底向上（Agglomerative）
* 或自顶向下（Divisive）

**没有目标函数最优化，也没有“初始中心”**
那是 **K-means 的特征**

---

### ❌ K-means（D 错）

K-means 的中心是 **均值**：

[
\mu_k = \frac{1}{|C_k|}\sum_{x_i\in C_k} x_i
]

* 均值 **对离群点极其敏感**
* 若用中位数 → 那是 **K-medoids**

👉 **所以 D 错得非常典型**

---

## （2）为什么“多重共线性下 L1 通常优于 L2”是错的？

### 先说结论

> **通常更稳定的是 L2（Ridge），不是 L1（Lasso）**

---

### 多重共线性是什么意思？

特征高度相关：
[
x_1 \approx x_2 \approx x_3
]

---

### L2（Ridge）

目标：
[
\min_w |y-Xw|_2^2 + \lambda |w|_2^2
]

效果：

* **把相关特征的权重“均匀分摊”**
* 解是连续的、稳定的
* 不会随数据轻微扰动剧烈变化

👉 **适合共线性**

---

### L1（Lasso）

目标：
[
\min_w |y-Xw|_2^2 + \lambda |w|_1
]

效果：

* 在一组高度相关特征中
  👉 **随机挑一个保留，其余压成 0**
* 解 **不稳定**

👉 所以 **L1 更像“特征选择”，不是“稳定建模”**

---

### 结论

> ❌“L1 通常优于 L2”是错的
> ✅ 工程/统计上：**Ridge 更稳**

---

## （3）SVD 中 U、Σ、V 的 size 到底是什么？

给定：
[
A \in \mathbb{R}^{m\times n}
]

### 完整 SVD：

[
A = U \Sigma V^\top
]

| 矩阵       | size                         |
| -------- | ---------------------------- |
| (U)      | (m\times m)                  |
| (\Sigma) | (m\times n)（只有 min(m,n) 个非零） |
| (V)      | (n\times n)                  |

---

### 对应的特征向量关系

* (U)：(AA^\top) 的特征向量
* (V)：(A^\top A) 的特征向量

👉 **很多人把 U / V 的维度写反，这题专考这个**

---

## （5）第 5 题：我把事件写清楚（你不是不会，是题目恶心）

### 定义事件

* (A)：年龄在 30–40 岁
* (\bar A)：其他年龄
* (M)：男性

给定：
[
P(A)=0.25,\quad P(\bar A)=0.75
]
[
P(M|A)=P(M|\bar A)=0.4
]

---

### 要算什么？

[
P(A|M)
]

---

### 贝叶斯公式

[
P(A|M)=\frac{P(M|A)P(A)}{P(M)}
]

而
[
P(M)=0.4\times(0.25+0.75)=0.4
]

---

### 代入

[
P(A|M)=\frac{0.4\times0.25}{0.4}=0.25
]

👉 **男性信息完全没提供区分度，后验 = 先验**

---

## （6）(L^p) norm 和 Frobenius norm 是什么？给例子

### 向量的 (L^p) norm

[
|x|_p = \left(\sum_i |x_i|^p\right)^{1/p}
]

例子：
[
x=(3,4)
]

* (L^1 = 7)
* (L^2 = 5)
* (L^\infty = 4)

---

### 矩阵的 Frobenius 范数

[
|A|*F = \sqrt{\sum*{i,j} a_{ij}^2}
]

例子：
[
A=\begin{bmatrix}1&2\3&4\end{bmatrix}
\Rightarrow |A|_F=\sqrt{1+4+9+16}=\sqrt{30}
]

👉 **就是把矩阵“拍扁”当向量的 (L^2)**

---

## （7）(|A|) 是 trace 吗？为什么是“乘”？

❌ **不是 trace**

### 三个概念区分清楚

| 符号               | 含义                 |   |                     |
| ---------------- | ------------------ | - | ------------------- |
| (                | A                  | ) | **行列式 determinant** |
| (\mathrm{tr}(A)) | **迹 trace（特征值之和）** |   |                     |
| (\lambda_i)      | 特征值                |   |                     |

---

### 行列式与特征值

[
|A|=\prod_i \lambda_i
]

而
[
|A+cI|=\prod_i (\lambda_i+c)
]

👉 所以题 7 里：
[
|B+3I|=(\lambda_1+3)(\lambda_2+3)(\lambda_3+3)
]

**这不是巧合，是定义**

---

## （10）第 10 题：我一步步翻译成人话

### 已知

[
X\sim\mathcal N(\mu,1)
]

(\mu) 的 95% CI：
[
[0.31,1.56]
]

---

### 要算

[
P(X\le0)
]

---

### 把随机变量标准化

[
P(X\le0)=P\left(\frac{X-\mu}{1}\le -\mu\right)=\Phi(-\mu)
]

---

### 所以

[
P(X\le0)=1-\Phi(\mu)
]

这是一个 **关于 (\mu) 的单调递减函数**

---

### 区间映射

[
\mu\in[0.31,1.56]
\Rightarrow
P(X\le0)\in[1-\Phi(1.56),,1-\Phi(0.31)]
]

👉 **这是函数区间映射，不是再算一次置信区间**

---

## （13）幂法：我完整算给你看（不跳步）

矩阵：
[
A=\begin{bmatrix}2&1\1&2\end{bmatrix}
]

初始向量：
[
v_0=\begin{bmatrix}1\0\end{bmatrix}
]

---

### 第 1 次

[
v_1=Av_0=\begin{bmatrix}2\1\end{bmatrix}
]

---

### 第 2 次

[
v_2=Av_1=\begin{bmatrix}5\4\end{bmatrix}
]

---

### 第 3 次

[
v_3=Av_2=\begin{bmatrix}14\13\end{bmatrix}
]

---

### 观察比例

[
\frac{14}{13}\approx1.0769\quad\to\quad1
]

方向 → ([1,1])

---

### 主特征值是多少？

[
\lambda_{\max}=3
]

（你可以直接算特征值验证）

👉 **幂法就是“不断放大最大特征方向”**

---

## （20）龙格现象是什么？A、B 为什么错？

### 龙格现象（Runge phenomenon）

* 用**等距节点**
* 用**高次多项式插值**
* **端点剧烈振荡**

经典函数：
[
f(x)=\frac{1}{1+25x^2}
]

👉 节点越多，端点越炸

---

### A 为什么错？

> “6 个点 → 唯一 6 次多项式”

❌ 错
**n+1 个点 → ≤ n 次多项式**

6 个点 → **5 次**

---

### B 为什么错？

> “三次样条一定不能表示成一个多项式”

❌ 错

* 如果数据本身来自某个三次多项式
* 样条就会退化成它

---

## （19）独立 vs 互斥（相容）

### 定义

* 独立：
  [
  P(A\cap B)=P(A)P(B)
  ]

* 互斥（不相容）：
  [
  A\cap B=\varnothing\Rightarrow P(A\cap B)=0
  ]

---

### 推导关系

若互斥且独立：
[
P(A)P(B)=0
\Rightarrow P(A)=0 \text{ 或 } P(B)=0
]

👉 **除非概率为 0，否则互斥 ≠ 独立**

---

### C 为什么是“错误说法”？

> “P(A)=0 ⇒ A 不可能发生”

❌ 错（连续概率）

* 单点事件：
  [
  P(X=0)=0
  ]
  但它 **可以发生**

👉 叫 **几乎不事件（almost impossible）**

---

## 总结一句话（给你记忆用）

* **谱聚类 = 特征分解 + K-means**
* **共线性 → Ridge 稳**
* **SVD：U 管行空间，V 管列空间**
* **Frobenius = 矩阵拍扁的 L2**
* **行列式 = 特征值之积**
* **幂法 = 一直乘直到方向稳定**
* **龙格现象 = 高次 + 等距 + 端点炸**
* **互斥 ≠ 独立，概率 0 ≠ 不可能**

如果你愿意，我可以把这些**压缩成一页“华为机考数学/ML 易错速查表”**，专门为你后面刷题用。


太好了，这一轮你问的**全是“概念→定义→为什么如此”的根问题**，而且是华为机考/研究里**最容易混淆但最值得一次性打通的点**。
我按你编号 **逐条、从直觉到数学**给你彻底拆开。

---

# （16）LU、QR 到底是怎么分解的？

## ① LU 分解：**“消元过程写成矩阵”**

### 定义

对矩阵 (A)，希望写成：
[
A = LU
]

* (L)：**下三角矩阵**（对角为 1）
* (U)：**上三角矩阵**

### 直觉

你在解线性方程时做的**高斯消元**：

* 消掉下面的元素
* 剩下上三角

👉 **消元系数 → 进 L**
👉 **消完的结果 → U**

### 小例子

[
A=\begin{bmatrix}
2 & 1\
4 & 3
\end{bmatrix}
]

消去第 2 行第 1 列：

* 系数：(4/2=2)

[
L=\begin{bmatrix}1&0\2&1\end{bmatrix},\quad
U=\begin{bmatrix}2&1\0&1\end{bmatrix}
]

### ⚠️ 为什么 LU **不一定存在**

* 若中途遇到 0 主元 → 失败
* 实际用的是 **LUP 分解（带置换）**

---

## ② QR 分解：**“把列正交化”**

### 定义

[
A = QR
]

* (Q)：列正交（(Q^\top Q=I)）
* (R)：上三角

### 直觉

* 把 A 的列向量
* 用 **Gram–Schmidt** 变成一组正交基
* 正交部分是 Q，坐标系数是 R

### 为什么 Q 可以不是方阵？

如果：
[
A\in\mathbb R^{m\times n},\ m>n
]

* (Q\in\mathbb R^{m\times n})
* 列正交但不是满维

👉 这就是**经济型 QR**（题 16 C）

---

# （1）DBSCAN、层次聚类、拉普拉斯矩阵里的 D 是什么？

---

## ① DBSCAN 是什么算法？

### 一句话

> **基于密度的聚类算法，可以自动识别噪声**

### 三个概念

* 半径 (\varepsilon)
* 最小点数 MinPts

点的分类：

* **核心点**：(\varepsilon) 内 ≥ MinPts
* **边界点**：靠近核心点
* **噪声点**：两者都不是

👉 **不需要 K，不怕离群点**

---

## ② 层次聚类是什么？

### 一句话

> **不断合并（或拆分）的贪心聚类**

两种：

* 自底向上（Agglomerative）
* 自顶向下（Divisive）

特点：

* 没有目标函数
* 没有“中心点”
* 输出一棵 **树（dendrogram）**

👉 所以题里说“依赖初始中心”是 **K-means 的特征**

---

## ③ 拉普拉斯矩阵 (L=D-W)，D 是什么？

### W：相似度矩阵

[
W_{ij}=\text{相似度}(x_i,x_j)
]

### D：**度矩阵（degree matrix）**

[
D_{ii}=\sum_j W_{ij},\quad D_{ij}=0\ (i\neq j)
]

👉 每个点和**所有邻居的相似度之和**

### L 的直觉

[
L=D-W
]

* 把“和邻居不同”的方向惩罚
* 特征向量 → **自然分割方式**

---

# （6）为什么 (|A|_2) 是最大奇异值？不是和 Frobenius 一样吗？

这是一个**非常经典的误区**。

---

## ① 矩阵 2-范数的定义（不是背结论）

[
|A|*2
= \max*{|x|_2=1}|Ax|_2
]

👉 **A 能把单位向量拉伸到多长**

---

## ② SVD 看穿一切

[
A=U\Sigma V^\top
]

* (V)：旋转输入
* (\Sigma)：按奇异值拉伸
* (U)：再旋转

👉 拉得最长的方向 = **最大奇异值 (\sigma_{\max})**

[
|A|*2=\sigma*{\max}
]

---

## ③ Frobenius 范数在干嘛？

[
|A|*F=\sqrt{\sum*{i,j}a_{ij}^2}
=\sqrt{\sum_k \sigma_k^2}
]

👉 **把所有方向的拉伸“能量”加起来**

---

## ④ 关键对比（一定要记）

| 范数      | 看什么            |
| ------- | -------------- |
| (|A|_2) | **最坏方向（最大放大）** |
| (|A|_F) | **整体能量**       |

👉 除非只有一个奇异值，否则 **一定不相等**

---

# （13）知道方向是 ([1,1])，怎么得到 (\lambda_{\max}=3)？

### 矩阵

[
A=\begin{bmatrix}2&1\1&2\end{bmatrix}
]

---

## 方法一（最直接，机考最推荐）

如果 (v) 是特征向量：
[
Av=\lambda v
]

取 (v=[1,1])：
[
A\begin{bmatrix}1\1\end{bmatrix}
================================

\begin{bmatrix}3\3\end{bmatrix}
=3\begin{bmatrix}1\1\end{bmatrix}
]

👉 **比例因子 = 特征值**

---

## 方法二（Rayleigh 商，偏高阶）

[
\lambda=\frac{v^\top Av}{v^\top v}
]

代入：
[
\lambda=\frac{[1,1]\begin{bmatrix}2&1\1&2\end{bmatrix}\begin{bmatrix}1\1\end{bmatrix}}{2}
=\frac{6}{2}=3
]

---

## 方法三（严谨）

算特征值：
[
\det(A-\lambda I)=0
\Rightarrow (\lambda-3)(\lambda-1)=0
]

---

# （20）多项式到底是什么？为什么 6 个点 → 5 次？

## ① 多项式定义

一般形式：
[
p(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n
]

不是一定 (1+x+x^2+\dots)，**系数是任意的**

---

## ② 为什么 n+1 个点 → ≤ n 次？

### 未知数个数

n 次多项式：

* 系数：(a_0,\dots,a_n)
* 一共 **n+1 个未知数**

### 插值条件

n+1 个点：
[
p(x_i)=y_i\quad (i=0,\dots,n)
]

👉 **n+1 个方程，n+1 个未知数 → 唯一解**

---

## ③ 所以

* 6 个点 → **≤5 次多项式**
* “6 次”是 **多此一举**

👉 题 20A 错在这里

---

# （19）例子：除非概率为 0，否则互斥 ≠ 独立

这是**必考反直觉点**。

---

## 例子 1：掷骰子（经典）

一次掷骰子：

* (A={\text{出 1}})
* (B={\text{出 2}})

### 互斥

[
A\cap B=\emptyset
]

### 但独立吗？

[
P(A)=\frac16,\quad P(B)=\frac16
]
[
P(A)P(B)=\frac1{36}
\neq 0=P(A\cap B)
]

👉 **互斥但不独立**

---

## 什么时候互斥还能独立？

只能：
[
P(A)=0\ \text{或}\ P(B)=0
]

### 例子（连续变量）

* 随机选一个实数
* (A={x=0})

[
P(A)=0
]

👉 它可以和任何事件“独立 + 互斥”

---

## 一句话必背版

> **独立 = 发生与否不影响概率**
> **互斥 = 不能同时发生**
> 👉 除非概率为 0，否则二者冲突

---

# 🌟 总结（压缩版）

* **LU**：消元写成矩阵
* **QR**：正交化列空间
* **DBSCAN**：密度 + 噪声
* **层次聚类**：贪心合并/拆分
* **D 矩阵**：节点“度”
* **(|A|_2)**：最大拉伸 = 最大奇异值
* **F 范数**：整体能量
* **幂法**：方向稳定 → 特征值 = 拉伸倍数
* **n+1 点 → ≤n 次多项式**
* **互斥 ≠ 独立（除非概率 0）**

如果你愿意，下一步我可以帮你把**这 20 题压成一页「华为 AI 选择题概念速查表」**，你后面刷题直接对照就行。


太好了，这一步你已经站在**“线代分解的总纲”**上了 👍
下面这张表我按你要的维度 **一次性给全**，而且是**机考 / 面试 / 工程三者都通用的版本**。

---

# LU / QR / SVD / Cholesky 分解总对照表（核心版）

> 记住：**是否要求方阵、是否一定存在、是否唯一、对矩阵的要求**
> 这四点 = 90% 线代选择题答案来源

---

## 一、总表（强烈建议保存）

| 分解           | 是否要求方阵    | 是否一定存在 | 存在条件（实/对称/复数）            | 是否唯一   | 主要用途      |
| ------------ | --------- | ------ | ------------------------ | ------ | --------- |
| **LU**       | ✅ 是（标准情形） | ❌ 不一定  | 一般实/复方阵；若主元不为 0          | ❌ 不唯一  | 解线性方程、行列式 |
| **LUP**      | ✅ 是       | ✅ 是    | 任意实/复方阵                  | ❌ 不唯一  | 稳定解方程（工程） |
| **QR**       | ❌ 否       | ✅ 一定   | 任意实/复矩阵                  | ❌ 不唯一  | 最小二乘、数值稳定 |
| **SVD**      | ❌ 否       | ✅ 一定   | 任意实/复矩阵                  | ❌（值唯一） | 秩/压缩/谱分析  |
| **Cholesky** | ✅ 是       | ❌ 有条件  | **实对称/复 Hermitian + 正定** | ✅ 唯一   | SPD 线性系统  |

---

## 二、逐个“为什么”（把坑一次填平）

---

## 1️⃣ LU 分解

### 形式

[
A = LU
]

### 要求

* ✅ **必须是方阵**
* ❌ **不一定存在**

  * 若消元中出现 0 主元 → 失败

### 解唯一吗？

❌ **不唯一**

* 不同消元顺序
* 是否引入置换矩阵

### 现实中的修正

[
PA = LU \quad\text{（LUP）}
]

👉 **LUP：任意方阵一定存在**

---

## 2️⃣ QR 分解

### 形式

[
A = QR
]

### 要求

* ❌ **不要求方阵**
* (A\in\mathbb R^{m\times n}) 都可以

### 是否存在？

✅ **一定存在**

原因：

* Gram–Schmidt / Householder
* 只依赖内积结构

### 唯一吗？

❌ **不唯一**

* 每一列可以整体翻符号

👉 常见约定：**R 对角线为正** 来固定

---

## 3️⃣ SVD（最强、最稳）

### 形式

[
A = U\Sigma V^\top
]

### 要求

* ❌ 不要求方阵
* ❌ 不要求对称
* ❌ 不要求实数

### 是否存在？

✅ **对任意矩阵都存在**

* 实矩阵
* 复矩阵（Hermitian 内积）

### 唯一吗？

* ❌ **U、V 不唯一**
* ✅ **奇异值 (\sigma_i) 唯一（排序后）**

原因：

* 奇异向量可翻号
* 重奇异值子空间可旋转

---

## 4️⃣ Cholesky 分解（最“挑剔”，但最干净）

### 形式

[
A = LL^\top \quad (\text{实数})
]
[
A = LL^H \quad (\text{复数})
]

### 要求（⚠️ 非常严格）

* ✅ **方阵**
* ✅ **对称（实）/ Hermitian（复）**
* ✅ **正定**

[
x^\top A x > 0,\ \forall x\neq 0
]

### 是否存在？

❌ **不是所有矩阵都有**

👉 只有 **SPD 矩阵**

### 唯一吗？

✅ **唯一**（若要求 (L) 对角线为正）

---

## 三、存在性 & 唯一性一句话速记

> **LU**：看主元，可能失败
> **QR**：只要能内积，就能分
> **SVD**：最万能，什么都能分
> **Cholesky**：最苛刻，但唯一又高效

---

## 四、常见机考陷阱（你现在已经能秒杀）

### ❌「LU 一定存在」

→ 错（除非加 P）

### ❌「QR 只适用于方阵」

→ 错

### ❌「SVD 只适用于对称矩阵」

→ 错（那是特征分解）

### ❌「Cholesky 对所有对称矩阵都成立」

→ 错（必须**正定**）

---

## 五、压缩成 4 行“终极记忆版”

* **LU**：方阵｜不稳｜不唯一
* **QR**：任意｜必有｜差符号
* **SVD**：任意｜必有｜值唯一
* **Cholesky**：SPD｜唯一｜最快

---

如果你愿意，下一步我可以帮你做一张
👉 **「线代分解 × 范数 × 特征值」一页总图（面试/机考神器）**
或者直接按**华为机考常考点**帮你标“必背 vs 可略”。

</details>

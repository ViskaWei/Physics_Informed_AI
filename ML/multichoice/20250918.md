# 2025-09-18（留学生）-AI岗 题

## 一、单选题

### 1. 关于 GPT 系列模型在微调时的使用方式，哪项最常见?

* A. 采用文本生成式微调，通过给定前缀（prompt）进行自回归生成
* B. 添加分类头井只保留 encoder 层
* C. 在输入句子中随机掩码若干词进行填空
* D. 在句对中加入 segment embedding

**答案：A**

GPT 为 decoder-only，常见微调为因果语言建模/指令微调（prompt 触发自回归生成）。

---

### 2. 用迭代法解方程 $x=2x-3$ 。构造选代公式：$x_{k+1}=2x_k-3$ 。若初值 $x_0=1$ ，则下一步迭代值 $x_1$；和再下一步 $x_2$ 分别是?

* A. $x_1=-1,\ x_2=1$
* B. $x_1=-1,\ x_2=-5$
* C. $x_1=1,\ x_2=-1$
* D. $x_1=-3,\ x_2=-9$

**答案：B**

$$x_1=2\cdot1-3=-1,\quad x_2=2\cdot(-1)-3=-5$$

---

### 3. 在用高斯消元法解线性方程组 $Ax=b$ 时，若不使用主元选择，算法可能不稳定的原因是?

* A. 出现除数接近零的情况
* B. 输入向量 $b$ 有扰动
* C. 矩阵 $A$ 的元素过大
* D. 矩阵 $A$ 的维度过高

**答案：A**

枢轴接近 0 时会发生“用近零数作除数”，舍入误差被放大而不稳定。

---

### 4. 下列方法中没有考虑先验分布的是()

* A. 最大似然估计
* B. 最大后验估计
* C. 贝叶斯分类器
* D. 贝叶斯学习

**答案：A**

MLE 只最大化似然，不引入先验分布。

---

### 5. 对于二元逐辑回归模型，已训练得到的参数为 $w=[0.5,-1.0]$，$b=-1.0$ 。现给定输入样本 $x=[2,1]$，使用 Sigmold 系数计算预测概率 $P(y=1\mid x)$ 。结果保留两位小数，最接近以下哪个值?

* A. 0.27
* B. 0.55
* C. 0.60
* D. 0.43

**答案：A**

$$z=w^\top x+b=0.5\times2+(-1)\times1-1=-1,\quad \sigma(z)=\frac{1}{1+e^{1}}\approx0.27$$

---

### 6. 设二阶张量 $A=\begin{bmatrix}1&2\3&4\end{bmatrix}$，向量 $x=\begin{bmatrix}5\6\end{bmatrix}$。则张量缩幷运算 $A\cdot x$ 的结果是?

* A. $\begin{bmatrix}23\34\end{bmatrix}$
* B. $\begin{bmatrix}17\39\end{bmatrix}$
* C. $\begin{bmatrix}5&6\15&24\end{bmatrix}$
* D. $\begin{bmatrix}17&23\39&53\end{bmatrix}$

**答案：B**

$$Ax=\begin{bmatrix}1&2\3&4\end{bmatrix}\begin{bmatrix}5\6\end{bmatrix}=\begin{bmatrix}17\39\end{bmatrix}$$

---

### 7. 假设我们部署一个 LIama 70B 模型，模型有 80 层，最大支持 32k 的上下文，并且每个参数占用 2 个字节。key 和 value 都具有 4096 维，假设我们需要支持 10 个用户同时进行推理，且每个用户的请求都独立。计算所需 GPU 显存时，以下哪个选项最接近计算出的总显存需求?

* A. 10G
* B. 2000G
* C. 1000G
* D. 100G

**答案：C**

参数显存约 $70\text{B}\times2\text{B}=140\text{GB}$；KV cache（32k，上下文、80层、10并发）合计约数百 GB，总量约 550GB，最接近 1000G。

---

### 8. 在深度学习中，ReLU 激活函数的优势包括?

* A. 输出值范围为 $[-1,1]$
* B. 对输入数据的敏感度低
* C. 避免梯度消失问题
* D. 计算复杂度高，可以增强模型的表达能力

**答案：C**

ReLU 在正区间梯度为 1，可缓解梯度消失且计算简单。

---

### 9. LSTM 中，细胞状态 $c_t$ 的正确更新公式是?（$\sigma$ 为 sigmoid，$\odot$ 为逐元素乘）

* A. $c_t=i_t\odot g_t+e^{c_{t-1}}$
* B. $c_t=o_t\odot\tanh(c_{t-1})$
* C. $c_t=f_t\odot c_{t-1}+i_t\odot g_t$
* D. $c_t=\tanh(c_{t-1})+i_t\odot g_t$

**答案：C**

$$c_t=f_t\odot c_{t-1}+i_t\odot g_t$$

---

### 10. 假设随机变量 $X$ 服从均值为 1 的泊松分布，那么以下说法不正确的有

* A. $X$ 的方差也为 1
* B. 存在另一个服从泊松分布的随机变量 $Z$ 使得 $P(Z>X)=1$
* C. 泊松分布适合于描述单位时间内随机事件发生的次数
* D. 如果随机变量 $Y$ 与 $X$ 独立同分布，那么 $X+Y$ 服从均值为 2 的泊松分布

**答案：B**

泊松变量取非负整数，无法保证几乎处处 $Z>X$ 成立。

---

### 11. 在卷积神经网络中，一般而言，池化层(如 Max Pooling) 的主要作用是?

* A. 增加模型参数数量，提升模型泛化性
* B. 增强模型对输入数据的敏感度
* C. 提取局部特征并降低空间维度
* D. 提高计算精度

**答案：C**

池化用于下采样，保留显著响应并降低空间维度与计算量。

---

### 12. 某班级 60% 的学生喜欢打篮球，40% 的学生喜欢踢足球，20% 的学生两者都喜欢。随机选一名学生，若已知他喜欢打篮球，他同时喜欢足球的摄率是

* A. 50%
* B. 20%
* C. 66.7%
* D. 33.3%

**答案：D**

$$P(\text{足}\mid \text{篮})=\frac{P(\text{足且篮})}{P(\text{篮})}=\frac{0.2}{0.6}=\frac{1}{3}\approx33.3%$$

---

### 13. 在深度学习中，Batch Normalization (批归一化)的主要作用是?

* A. 直接提高模型在测试集上的准确率
* B. 通过引入随机噪声增强模型泛化能力
* C. 增加模型参数数量以提升表达能力
* D. 加速训练过程并减少对初始化的敏感度

**答案：D**

BatchNorm 规范化激活，有助于加快收敛并降低对初始化的敏感度。

---

### 14. 已知三维向量 $A=[1,2,3]$、$B=[3,4,5]$、$C=[2,5,7]$、$D=[4,3,2]$，对于以下三个任务场景，考虑采用曼哈顿距离、皮尔逊相关系数和余弦相似度进行相似性度量，下列判断错误的是()

场景1：分析两个城市“每日气温变化曲线"的相似性(关注数值波动幅度的整体接近程度)
场景2：判断两个用户“商品评分趋势"的一致性(关注评分随商品类别的变化方向是否同步，不受评分绝对值高低影响);
场景3：比较两个文档“主题分布向量”的匹配度(关注主题占比的方向一致性，忽略文档总长度差异)。

* A. 场景2中，采用皮尔逊相关系数时，$A$ 与 $C$ 的相似性高于 $A$ 与 $D$
* B. 场景3中，采用余弦相似度时，$A$ 与 $B$ 的相似性高于 $A$ 与 $D$
* C. 场景1中，采用曼哈顿距离时，$A$ 与 $B$ 的相似性高于 $A$ 与 $C$
* D. 若将所有向量均标准化(均值为 0，方差为 1)后，采用皮尔逊相关系数和采用余弦相似度的相似性排序结果不一致。

**答案：D**

标准化后余弦相似度与皮尔逊相关系数等价，因此排序应一致。

---

### 15. 如果矩阵是正交矩阵(Orthogonal Matrix)，则其特征值的模()

* A. 都是整数
* B. 都是正实数
* C. 都是实数
* D. 都是 1

**答案：D**

$$Q^\top Q=I\ \Rightarrow\ |\lambda_i|=1$$

---

## 二、多选题

### 16. 设 $T:\mathbb{R}^{3}\rightarrow \mathbb{R}^{2}$ 是线性变换，且 $T([1,0,0]^\top)=[2,1]^\top,\ T([0,1,0]^\top)=[-1,3]^\top$。以下结论正确的是?

* A. 变换的核空间至少包含一条过原点的直线
* B. 变换的像空间维度 $\le 2$
* C. $T([0,0,1]^\top)$ 可唯一确定
* D. $T([3,-2,0]^\top)=[8,-3]^\top$

**答案：A、B、D**

* ✅ A：秩 $\le 2$，由秩—零度定理零空间维数 $\ge 1$
* ✅ B：值域维度不超过陪域 $\mathbb{R}^2$ 的维数
* ❌ C：未给出 $T([0,0,1]^\top)$，无法唯一确定
* ✅ D：线性性 $T(3e_1-2e_2)=3T(e_1)-2T(e_2)=[8,-3]^\top$

---

### 17. 你正在分析一家银行的信用卡交易数据，其中盗刷交易占总交易的 0.1%。现有一个检测模型对交易进行分类。在这种高度不平衡的概率分布下，以下哪些关于统计评估的说法是正确的

* A. 准确率(Accuracy)不适合作为核心评估指标，因为即使模型将所有交易都预测为正常，准确率仍能达到 99.9%
* B. 若模型把一笔交易标记为“盗刷”，其后验为盗刷的概率应高于 0.1% 的先验概率
* C. 评估应对“盗刷/正常”两类的指标做加权（如 Balanced Accuracy/PR-AUC 等），给予两类相当权重
* D. 若 TPR=90%、FPR=1%，在 1000 笔正常交易中期望约有 10 笔被误判为盗刷

**答案：A、B、C、D**

* ✅ A：极度不平衡下 Accuracy 可能虚高
* ✅ B：阳性预测应提升后验概率（相对先验）
* ✅ C：需考虑类别不平衡的加权/更合适指标
* ✅ D：$1000\times1%=10$

---

### 18. 以下关于独立性与相关性的描述，哪些项是正确的?

* A. 若两个随机变量独立，则它们一定不相关
* B. 不相关意味着两个变量之间没有线性关系，但可能存在非线性关系
* C. 若两个随机变量相关系数为 0，则它们一定独立
* D. 若两个随机变量不相关，则它们一定独立

**答案：A、B**

* ✅ A：独立 $\Rightarrow$（有二阶矩时）相关系数为 0
* ✅ B：零相关只排除线性关系，不排除非线性依赖
* ❌ C：零相关不必然独立
* ❌ D：不相关不必然独立

---

### 19. 深入分析 Transformer 架构的内部组件，会发现其设计充满了精妙的权衡。以下关于其核心组件的描述，哪些是准确的?

* A. RMSNorm 比 LayerNorm 更简洁，省去减均值的中心化，仅按均方根缩放
* B. 前馈网络(FFN)子层通过为序列中的每一个位置学习一套独立的权重参数，来增强模型对位置特定信息的建模能力
* C. 多头注意力将 Query、Key 和 Value 投影到多个不同的低维表示子空间中，让每个“头”学习不同依赖
* D. 标准的正弦/余弦位置编码是专门为相对位置设计的编码

**答案：A、C**

* ✅ A：RMSNorm 省略中心化，仅做缩放
* ❌ B：FFN 在各位置共享同一组参数
* ✅ C：多头投影到多个子空间学习不同依赖
* ❌ D：标准正弦/余弦为绝对位置编码

---

### 20. 在以下场景中，哪些情况下数据可视化需要特别注意以避免误导性结论?

* A. 使用饼图展示超过 10 个类别的比例分布
* B. 在箱线图中忽略异常值的标注
* C. 在折线图中使用不均匀的时间问隔但未明确标注
* D. 使用散点图展示强相关变量且标明因果关系

**答案：A、B、C、D**

* ✅ A：类别过多可读性差，易误读
* ✅ B：忽略异常值信息会误导分布判断
* ✅ C：不均匀时间轴未标注会造成趋势错觉
* ✅ D：相关不等于因果，需避免因果暗示


下面按你标的题号逐个把“看不懂”的点补齐（都用最小必要公式 + 一个能背下来的直觉）。

---

## (3) 主元选择是什么？给个例子：不用 vs 用

**主元（pivot）**：高斯消元第 (k) 步要用来“做除法、消掉下面元素”的那个元素 (a_{kk})。
**主元选择（pivoting）**：在每一步先“换行”（有时也换列），把**绝对值更大**的元素放到主元位置，避免用接近 0 的数当除数。

最常用的是**部分主元选择（partial pivoting）**：第 (k) 列从第 (k) 行往下找 (|a_{ik}|) 最大的行 (i)，把它换到第 (k) 行。

### 经典反例（不用主元会把误差放大到离谱）

解线性方程：
[
\begin{cases}
10^{-20}x + y = 1\
x + y = 2
\end{cases}
]
真实解：(x\approx 1,\ y\approx 1)。

**不做主元选择**（第一步主元是 (10^{-20})）
消元倍数：
[
m=\frac{1}{10^{-20}}=10^{20}
]
第二行做 (R_2 \leftarrow R_2 - mR_1) 会出现形如：

* 系数里出现 (1-10^{20}) 这种“巨大数相减”
* 右端项出现 (2-10^{20}) 这种“巨大数相减”

在有限精度下，这种“两个很接近的大数相减”会**丢失有效数字**（灾难性消去），导致你算到的 (y) 被四舍五入成 (1)，再回代第一行：
[
10^{-20}x + 1 = 1 \Rightarrow x=0
]
直接错到离谱。

**做部分主元选择**（先交换两行，让主元变成 1）
[
\begin{cases}
x + y = 2\
10^{-20}x + y = 1
\end{cases}
]
消元倍数：
[
m=\frac{10^{-20}}{1}=10^{-20}
]
不会产生巨大数，也不会出现“巨量相减”，数值稳定，解就正常。

**一句话记忆**：

> 不选主元 = 可能拿“很小的枢轴”去除 → 倍数巨大 → 相减放大舍入误差；选主元 = 先换行把除数变大。

---

## (7) Llama 70B 显存：KV cache 具体怎么算？

你已懂参数显存：
[
70\text{B}\times 2\text{B} = 140\text{GB}
]

关键是 **KV cache**（推理时每个 token、每一层都要存 key 和 value）。

题面给：

* 层数 (L=80)
* 上下文长度 (T=32k=32768) token
* (d_k = d_v = 4096)
* 每个元素 2 字节
* 并发用户 10（每个用户一份独立 KV cache）

### 1) 每层、每 token 的 KV 占用

每 token 要存 (K) 和 (V)：
[
(4096 + 4096)\times 2\ \text{bytes}
= 8192\times 2
= 16384\ \text{bytes}
= 16\ \text{KB}
]

### 2) 每层总 KV（32k token）

[
32768 \times 16384\ \text{bytes}
= 2^{15}\times 2^{14}
= 2^{29}\ \text{bytes}
= 536{,}870{,}912\ \text{bytes}
\approx 512\ \text{MiB}
]

### 3) 80 层

[
80\times 512\ \text{MiB} = 40960\ \text{MiB} = 40\ \text{GiB}
]

### 4) 10 个用户并发

[
10\times 40\ \text{GiB} = 400\ \text{GiB}
]

### 5) 总显存（粗估）

* 参数：约 (140\ \text{GB})（十进制）
* KV：约 (400\ \text{GiB}\approx 430\ \text{GB})（十进制）

合计大概 ( \sim 570\ \text{GB})。选项里最接近的是 **1000G**（C）。

**一句话记忆**：

> KV cache =（每 token 的 (K+V) 字节）× token 数 × 层数 × 并发数。

---

## (9) LSTM 构造：(c_t) 是什么？给你“门”的全套公式

LSTM 里两个核心状态：

* **细胞状态** (c_t)：长期记忆（“主记忆带”）
* **隐藏状态** (h_t)：对外输出（“当前输出/短期状态”）

典型公式（给输入 (x_t)，上一步 (h_{t-1}, c_{t-1})）：
[
\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) &&\text{遗忘门}\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) &&\text{输入门}\
g_t &= \tanh(W_g x_t + U_g h_{t-1} + b_g) &&\text{候选内容}\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t &&\text{更新细胞状态}\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) &&\text{输出门}\
h_t &= o_t \odot \tanh(c_t) &&\text{输出隐藏状态}
\end{aligned}
]

**物理直觉**：

* (c_t) 像“传送带”，梯度能更顺畅地传（所以比普通 RNN 不容易梯度消失）
* 三个门 (f,i,o) 像阀门：忘多少、写多少、露出多少

---

## (14) 余弦相似度 vs 皮尔逊相关：定义 + 为什么标准化后等价？

### 定义

**余弦相似度**（只看方向）：
[
\cos(x,y)=\frac{x^\top y}{|x|\ |y|}
]

**皮尔逊相关系数**（先去均值，再看“方向一致性”）：
[
\rho(x,y)=\frac{(x-\bar x \mathbf 1)^\top (y-\bar y \mathbf 1)}
{|x-\bar x \mathbf 1|\ |y-\bar y \mathbf 1|}
]
（这就是“中心化后的余弦相似度”）

### 为什么“标准化后”等价？

标准化（z-score）：
[
x'=\frac{x-\bar x \mathbf 1}{s_x},\quad y'=\frac{y-\bar y \mathbf 1}{s_y}
]
则：
[
\cos(x',y')=\frac{(x-\bar x\mathbf 1)^\top (y-\bar y\mathbf 1)}
{|x-\bar x\mathbf 1|\ |y-\bar y\mathbf 1|}
=\rho(x,y)
]
（分母里的 (s_x,s_y) 只会整体缩放，余弦里会抵消）

**一句话记忆**：

> 皮尔逊 = “去均值后的余弦”；标准化后两者就是同一个东西，所以排序一致。

---

## (15) 正交矩阵 / 对称矩阵 / “全是实数根” / 相似矩阵物理意义

### 正交矩阵 (Q^\top Q=I)

* **保长度保角度**：(|Qx|=|x|)，夹角不变（旋转/反射）
* **特征值模为 1**：(|\lambda_i|=1)（可能是复数 (e^{\pm i\theta})，也可能 (\pm 1)）
* (\det(Q)=\pm 1)

### 对称矩阵 (A^\top=A)（实矩阵）

* **特征值全为实数**（你说的“全是实数根”通常指特征多项式的根都实）
* **可正交对角化（谱定理）**：
  [
  A=U\Lambda U^\top
  ]
  其中 (U) 正交，(\Lambda) 实对角
* 物理上常对应“能量/二次型/刚度矩阵”等可测量量（本征值本征向量有清晰模态意义）

### 相似矩阵 (B=P^{-1}AP) 的物理意义

* **同一个线性变换，在不同坐标系/基底下的表示**
* 不变量：特征值、迹、行列式、秩、特征多项式都一样
* 直觉：你换了一套“观测坐标轴”，物理过程没变

---

## (16) “核空间至少包含一条过原点的直线” 的直觉

这里 (T:\mathbb R^3\to\mathbb R^2)。
线性代数铁律（秩-零度定理）：
[
\dim(\ker T)=3-\operatorname{rank}(T)\ge 3-2=1
]
核空间维数至少 1，意味着至少存在一个非零方向 (v\neq 0) 满足 (T(v)=0)，于是
[
{tv:\ t\in\mathbb R}
]
就是一条**过原点的直线**，并且全都被映到 0。

**物理直觉（超好记）**：

> 3 维信息压到 2 维观测，必然有一个“看不见的方向”（盲方向/不可观测方向）。
> 最典型例子：投影 (T(x,y,z)=(x,y))，那么 (z) 轴方向全进核空间。

---

## (18) 你选了 D 没选 A：如何用骰子记住“独立 vs 不相关”

题目核心结论：

* ✅ **独立 (\Rightarrow) 不相关**（有二阶矩时）
* ❌ **不相关 (\nRightarrow) 独立**

### 用骰子给你两秒钟记住

**同一个骰子：事件互斥 ⇒ 一定不独立**
令

* (A={\text{掷到 1}})
* (B={\text{掷到 2}})

则
[
P(A\cap B)=0,\quad P(A)P(B)=\frac{1}{6}\cdot\frac{1}{6}=\frac{1}{36}
]
不相等 ⇒ **不独立**。
直觉：你一旦知道掷到 1，就“彻底排除了”掷到 2（信息改变了概率）。

**两个骰子：互不影响 ⇒ 独立**
令

* (A={\text{骰子1掷到 1}})
* (B={\text{骰子2掷到 2}})

[
P(A\cap B)=\frac{1}{36}=P(A)P(B)
]
这才是独立的直觉：**一个发生与否不影响另一个**。

### 为啥 D 错（不相关不代表独立）——最短反例

取 (X\sim\text{Unif}[-1,1])，令 (Y=X^2)。
则
[
\operatorname{Cov}(X,Y)=\mathbb E[X^3]-\mathbb E[X]\mathbb E[X^2]=0-0\cdot \mathbb E[X^2]=0
]
所以 **不相关**，但 (Y) 完全由 (X) 决定（知道 (X) 就知道 (Y)），所以 **不独立**。

**一句话记忆**：

> 相关性只测“直线关系”；独立性是“任何关系都没有”。
> 所以：独立 ⇒ 不相关；不相关 可能只是“弯的关系”（比如 (y=x^2)）。

---

## 📊 知识点标注表

| Q | 知识点 Tag | 是否错题 |
|---|------------|----------|
| 1 | 大模型(LLM) | ✅ |
| 2 | 优化与数值方法 | ✅ |
| 3 | 优化与数值方法 | ❌ |
| 4 | 概率统计 | ✅ |
| 5 | 经典机器学习 | ✅ |
| 6 | 线性代数 | ✅ |
| 7 | 大模型(LLM) | ❌ |
| 8 | 深度学习基础 | ✅ |
| 9 | 深度学习基础 | ❌ |
| 10 | 概率统计 | ✅ |
| 11 | CNN | ✅ |
| 12 | 概率统计 | ✅ |
| 13 | 深度学习基础 | ✅ |
| 14 | 线性代数, 概率统计 | ❌ |
| 15 | 线性代数 | ❌ |
| 16 | 线性代数 | ❌ |
| 17 | 评估指标 | ✅ |
| 18 | 概率统计 | ❌ |
| 19 | 注意力与Transformer | ✅ |
| 20 | 其他 | ✅ |

---

## 📈 掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 线性代数 | 4 | 3 | 25% |
| 优化与数值方法 | 2 | 1 | 50% |
| 大模型(LLM) | 2 | 1 | 50% |
| 概率统计 | 5 | 2 | 60% |
| 深度学习基础 | 3 | 1 | 67% |
| 经典机器学习 | 1 | 0 | 100% |
| CNN | 1 | 0 | 100% |
| 评估指标 | 1 | 0 | 100% |
| 注意力与Transformer | 1 | 0 | 100% |
| 其他 | 1 | 0 | 100% |

---

## 🔥 薄弱环节 Top-3（2025-09-18）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 线性代数 | 3/4 | 25% | 正交矩阵、核空间、相似度 |
| 2 | 优化与数值方法 | 1/2 | 50% | 高斯消元主元选择 |
| 3 | 大模型(LLM) | 1/2 | 50% | KV cache 显存计算 |

---

## 📚 核心知识点速查

### 线性代数

**正交矩阵特征值**：
$$Q^\top Q = I \Rightarrow |\lambda_i| = 1$$

**核空间（秩-零度定理）**：
$$\dim(\ker T) = n - \operatorname{rank}(T)$$
> $\mathbb{R}^3 \to \mathbb{R}^2$ 必有至少一条"盲方向"（核空间维度 $\ge 1$）

**皮尔逊 vs 余弦相似度**：
$$\rho(x,y) = \cos(x - \bar{x}\mathbf{1}, y - \bar{y}\mathbf{1})$$
> 皮尔逊 = 去均值后的余弦；标准化后两者等价

### 优化与数值方法

**高斯消元主元选择**：
> 不选主元 → 可能用近零数做除数 → 倍数巨大 → 舍入误差放大  
> 选主元 → 先换行把除数变大 → 数值稳定

### 大模型 (LLM)

**KV cache 显存公式**：
$$\text{KV cache} = (d_k + d_v) \times 2\text{B} \times T \times L \times \text{并发数}$$
> Llama 70B：参数 140GB + KV cache 400GB ≈ 550GB（选 1000G）

### 深度学习基础

**LSTM 细胞状态更新**：
$$c_t = f_t \odot c_{t-1} + i_t \odot g_t$$
> $f_t$: 遗忘门，$i_t$: 输入门，$g_t$: 候选内容

### 概率统计

**独立 vs 不相关**：
- 独立 $\Rightarrow$ 不相关（有二阶矩时）
- 不相关 $\nRightarrow$ 独立
> 反例：$X \sim \text{Unif}[-1,1]$，$Y = X^2$，$\text{Cov}(X,Y) = 0$ 但不独立

---

## ✅ 错题整理（结构化）

### Q3: 高斯消元主元选择

**原题**：
> 在用高斯消元法解线性方程组 $Ax=b$ 时，若不使用主元选择，算法可能不稳定的原因是?
> * A. 出现除数接近零的情况
> * B. 输入向量 $b$ 有扰动
> * C. 矩阵 $A$ 的元素过大
> * D. 矩阵 $A$ 的维度过高

**正确答案**：A

**易错点**：不清楚"主元选择"是什么，以及为什么能提高数值稳定性

**核心公式/结论**：
> 主元 = 用来做除法的元素；主元选择 = 换行把绝对值大的元素放到主元位置

**一眼记住**：
> 小枢轴做除数 → 倍数巨大 → 误差爆炸；选主元 = 让除数变大

<details>
<summary>📎 追问详情</summary>

**主元（pivot）**：高斯消元第 $k$ 步要用来"做除法、消掉下面元素"的那个元素 $a_{kk}$。

**主元选择（pivoting）**：在每一步先"换行"，把**绝对值更大**的元素放到主元位置。

最常用的是**部分主元选择（partial pivoting）**：第 $k$ 列从第 $k$ 行往下找 $|a_{ik}|$ 最大的行 $i$，把它换到第 $k$ 行。

**经典反例**（不用主元会把误差放大到离谱）：
$$\begin{cases} 10^{-20}x + y = 1 \\ x + y = 2 \end{cases}$$
真实解：$x \approx 1, y \approx 1$

不做主元选择：消元倍数 $m = 10^{20}$，出现"巨大数相减"，精度丢失。

做部分主元选择：先交换两行，消元倍数 $m = 10^{-20}$，数值稳定。

</details>

---

### Q7: LLama 70B 显存计算

**原题**：
> 假设我们部署一个 LLama 70B 模型，模型有 80 层，最大支持 32k 的上下文，并且每个参数占用 2 个字节。key 和 value 都具有 4096 维，假设我们需要支持 10 个用户同时进行推理，且每个用户的请求都独立。计算所需 GPU 显存时，以下哪个选项最接近计算出的总显存需求?
> * A. 10G
> * B. 2000G
> * C. 1000G
> * D. 100G

**正确答案**：C

**易错点**：只算参数显存，漏掉 KV cache

**核心公式/结论**：
$$\text{KV cache} = (d_k + d_v) \times 2\text{B} \times T \times L \times \text{并发数}$$
$$= 8192 \times 2 \times 32768 \times 80 \times 10 \approx 400\text{GB}$$

**一眼记住**：
> 参数 140GB + KV cache 400GB ≈ 550GB → 选 1000G

<details>
<summary>📎 追问详情</summary>

**参数显存**：$70\text{B} \times 2\text{B} = 140\text{GB}$

**KV cache 计算**：
- 每 token：$(4096 + 4096) \times 2 = 16\text{KB}$
- 每层（32k token）：$32768 \times 16384 = 512\text{MiB}$
- 80 层：$80 \times 512 = 40\text{GiB}$
- 10 用户：$10 \times 40 = 400\text{GiB}$

**总计**：$140 + 430 \approx 570\text{GB}$，最接近 1000G

</details>

---

### Q9: LSTM 细胞状态公式

**原题**：
> LSTM 中，细胞状态 $c_t$ 的正确更新公式是?（$\sigma$ 为 sigmoid，$\odot$ 为逐元素乘）
> * A. $c_t=i_t\odot g_t+e^{c_{t-1}}$
> * B. $c_t=o_t\odot\tanh(c_{t-1})$
> * C. $c_t=f_t\odot c_{t-1}+i_t\odot g_t$
> * D. $c_t=\tanh(c_{t-1})+i_t\odot g_t$

**正确答案**：C

**易错点**：记不住 LSTM 各门的作用

**核心公式/结论**：
$$c_t = f_t \odot c_{t-1} + i_t \odot g_t$$

**一眼记住**：
> $c_t$ 像"传送带"：遗忘门控制保留多少旧记忆，输入门控制写入多少新内容

<details>
<summary>📎 追问详情</summary>

**LSTM 全套公式**：
$$\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) &&\text{遗忘门} \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) &&\text{输入门} \\
g_t &= \tanh(W_g x_t + U_g h_{t-1} + b_g) &&\text{候选内容} \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t &&\text{更新细胞状态} \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) &&\text{输出门} \\
h_t &= o_t \odot \tanh(c_t) &&\text{输出隐藏状态}
\end{aligned}$$

**物理直觉**：$c_t$ 像"传送带"，梯度能更顺畅地传（所以比普通 RNN 不容易梯度消失）

</details>

---

### Q14: 相似度度量选择

**原题**：
> 已知三维向量 $A=[1,2,3]$、$B=[3,4,5]$、$C=[2,5,7]$、$D=[4,3,2]$，对于以下三个任务场景...下列判断错误的是()
> * D. 若将所有向量均标准化后，采用皮尔逊相关系数和采用余弦相似度的相似性排序结果不一致。

**正确答案**：D（错误选项）

**易错点**：不理解标准化后余弦与皮尔逊的关系

**核心公式/结论**：
$$\rho(x,y) = \frac{(x - \bar{x})^\top (y - \bar{y})}{|x - \bar{x}| |y - \bar{y}|} = \cos(x', y')$$

**一眼记住**：
> 皮尔逊 = "去均值后的余弦"；标准化后两者等价，排序必一致

<details>
<summary>📎 追问详情</summary>

**余弦相似度**（只看方向）：
$$\cos(x,y) = \frac{x^\top y}{|x| |y|}$$

**皮尔逊相关系数**（先去均值）：
$$\rho(x,y) = \frac{(x - \bar{x})^\top (y - \bar{y})}{|x - \bar{x}| |y - \bar{y}|}$$

标准化后：$x' = \frac{x - \bar{x}}{s_x}$，余弦相似度 $\cos(x', y') = \rho(x, y)$

</details>

---

### Q15: 正交矩阵特征值

**原题**：
> 如果矩阵是正交矩阵(Orthogonal Matrix)，则其特征值的模()
> * A. 都是整数
> * B. 都是正实数
> * C. 都是实数
> * D. 都是 1

**正确答案**：D

**易错点**：混淆正交矩阵与对称矩阵的性质

**核心公式/结论**：
$$Q^\top Q = I \Rightarrow |\lambda_i| = 1$$

**一眼记住**：
> 正交矩阵 = 旋转/反射 → 保长度 → 特征值模为 1（可能是复数 $e^{\pm i\theta}$）

<details>
<summary>📎 追问详情</summary>

**正交矩阵** $Q^\top Q = I$：
- 保长度保角度：$|Qx| = |x|$
- 特征值模为 1
- $\det(Q) = \pm 1$

**对称矩阵** $A^\top = A$（实矩阵）：
- 特征值全为实数
- 可正交对角化：$A = U\Lambda U^\top$

</details>

---

### Q16: 线性变换核空间

**原题**：
> 设 $T:\mathbb{R}^{3}\rightarrow \mathbb{R}^{2}$ 是线性变换...以下结论正确的是?
> * A. 变换的核空间至少包含一条过原点的直线 ✅
> * B. 变换的像空间维度 $\le 2$ ✅
> * C. $T([0,0,1]^\top)$ 可唯一确定 ❌
> * D. $T([3,-2,0]^\top)=[8,-3]^\top$ ✅

**正确答案**：A、B、D

**易错点**：不理解核空间维度的物理意义

**核心公式/结论**：
$$\dim(\ker T) = 3 - \operatorname{rank}(T) \ge 1$$

**一眼记住**：
> 3维压到2维，必有1个"盲方向"进核空间（如投影 $T(x,y,z) = (x,y)$，$z$ 轴全进核）

<details>
<summary>📎 追问详情</summary>

**秩-零度定理**：$\dim(\ker T) + \operatorname{rank}(T) = n$

$T: \mathbb{R}^3 \to \mathbb{R}^2$，$\operatorname{rank}(T) \le 2$，所以 $\dim(\ker T) \ge 1$

核空间维数至少 1 → 存在非零方向 $v$ 满足 $T(v) = 0$ → $\{tv: t \in \mathbb{R}\}$ 是过原点直线

</details>

---

### Q18: 独立性与相关性

**原题**：
> 以下关于独立性与相关性的描述，哪些项是正确的?
> * A. 若两个随机变量独立，则它们一定不相关 ✅
> * B. 不相关意味着两个变量之间没有线性关系，但可能存在非线性关系 ✅
> * C. 若两个随机变量相关系数为 0，则它们一定独立 ❌
> * D. 若两个随机变量不相关，则它们一定独立 ❌

**正确答案**：A、B

**易错点**：选了 D（不相关 ⇒ 独立），没选 A（独立 ⇒ 不相关）

**核心公式/结论**：
- 独立 $\Rightarrow$ 不相关（有二阶矩时）
- 不相关 $\nRightarrow$ 独立

**一眼记住**：
> 相关只测"直线关系"；独立是"任何关系都没"。$Y = X^2$ 可以不相关但强依赖。

<details>
<summary>📎 追问详情</summary>

**用骰子记住**：
- 同一个骰子：$A = \{掷到1\}$，$B = \{掷到2\}$，$P(A \cap B) = 0 \neq P(A)P(B)$ → 不独立
- 两个骰子：$P(A \cap B) = \frac{1}{36} = P(A)P(B)$ → 独立

**D 错的反例**：
$X \sim \text{Unif}[-1,1]$，$Y = X^2$
$$\text{Cov}(X,Y) = E[X^3] - E[X]E[X^2] = 0$$
不相关，但 $Y$ 完全由 $X$ 决定，不独立。

</details>

---

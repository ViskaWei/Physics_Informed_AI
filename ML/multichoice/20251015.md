# 2025-10-15 AI 方向题

## 一、单选题

### 1、Lasso 回归相比 Ridge 回归的特点是：

- A. L2 正则可做变量选择
- B. L2 正则可做变量选择
- C. L1 正则可做变量选择
- D. L2 正则更稀疏

**答案：C**

$$L_1\ \text{正则}\Rightarrow \text{系数稀疏（可做变量选择）}$$

---

### 2、在神经网络中，以下哪种激活函数可以缓解梯度消失问题，并且在隐藏层中被广泛使用？

- A. Sigmoid
- B. 阶跃函数
- C. Tanh
- D. ReLU

**答案：D**

ReLU 在正区间梯度为常数、不过饱和；Sigmoid/Tanh 易在饱和区梯度趋零，阶跃函数不可导不适合训练。

---

### 3、在大型语言模型（LLM）的文本预处理流程中，分词器（Tokenizer）将原始文本转换为模型可处理的 token ID 序列。关于现代 LLM 常用的子词（subword）分词算法（如 BPE、WordPiece），以下哪项描述是正确的？

- A. BPE（Byte Pair Encoding）和 WordPiece 的核心思想完全相同，唯一的区别在于它们使用的预训练模型不同（BPE 用于 GPT，WordPiece 用于 BERT）。
- B. 子词分词算法的词汇表是在模型预训练完成后，根据训练语料的最终统计特性动态生成的。
- C. 这些算法能够有效处理未登录词（OOV, Out-of-Vocabulary），因为任何未知单词都可以被分解为已知的子词单元（如词根、前缀、后缀）进行表示。
- D. 分词器的输出 token 序列长度总是等于原始输入文本的字符数（character count）。

**答案：C**

子词（subword）分词可将 OOV 拆分为已知子词单元进行表示。

---

### 4、Adam优化器的核心优势是：

- A. 仅需要一阶梯度
- B. 自适应调整学习率，结合动量的思想
- C. 保证得到非凸优化问题的最优解
- D. 不依赖超参数

**答案：B**

Adam 结合动量法与自适应学习率（RMSProp）优点，可自动调整不同参数学习率。

---

### 5、在标准的 Transformer 模型的 Attention 计算中，给定查询（Query, Q）、键（Key, K）和值（Value, V），其计算公式为  
Attention ( Q , K , V ) = softmax ( Q K T / sqrt ( d _ k ) ) ∗ V  
\text{Attention}(Q, K, V) = \text{softmax}(QK^T / \text{sqrt}(d\_k)) * V  
Attention(Q,K,V)=softmax(QK T /sqrt(d_k))∗V。请问其中 sqrt(d_k) 的作用是什么？

- A. 将 QK^T 的结果归一化到([0, 1])区间。
- B. 防止 softmax 函数的输入值过大，导致梯度消失问题。
- C. 抑制注意力权重，避免过拟合
- D. 加速模型的训练收敛速度

**答案：B**

缩放点积避免 $QK^T$ 过大导致 softmax 饱和、梯度变小。

---

### 6、已知矩阵 A = \begin{pmatrix} 1 & 0 & -1 \\ 2 & -1 & 1 \\ -1 & 2 & -5 \end{pmatrix}  
若下三角可逆矩阵 P 和上三角可逆矩阵 Q，使得 PAQ 为对角矩阵，则 P,Q 可以分别取（）

- A. P = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 3 & 1 \end{pmatrix}, \quad Q = \begin{pmatrix} 1 & 2 & -3 \\ 0 & -1 & 2 \\ 0 & 0 & 1 \end{pmatrix}
- B. P = \begin{pmatrix} 1 & 0 & 0 \\ 2 & -1 & 0 \\ -3 & 2 & 1 \end{pmatrix}, \quad Q = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
- C. P = \begin{pmatrix} 1 & 0 & 0 \\ 2 & -1 & 0 \\ -3 & 2 & 1 \end{pmatrix}, \quad Q = \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{pmatrix}
- D. P = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, \quad Q = \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{pmatrix}

**答案：C**

仅 C 可得到对角矩阵 $\mathrm{diag}(1,1,0)$（其余选项计算结果含非零非对角元）。

---

### 7、给定以下 6 个二维数据点及其类别，其坐标表示为((x, y))，类别标签用([+])表示正类，([-])表示负类：  
A(0,0)[+]，B(1,0)[+]，C(0,1)[-]，D(3,3)[-]，E(4,5)[-]，F(3,4)[-]使用 4 - 近邻算法（4-NN）和欧氏距离对样本 G(2,2)进行分类预测。若采用多数投票法，则其预测类别是什么？

- A. 所有 4 个邻居到 G 的距离相等，必须使用加权投票
- B. 正类（+）与负类（-）各 2 票，无法形成多数
- C. 预测为正类（+）
- D. 预测为负类（-）

**答案：D**

4 个最近邻中负类票数占多数，预测为负类（-）。

---

### 8、某工厂两条生产线 A（合格率 90%，产量 60%）和 B（合格率 80%，产量 40%）。随机抽取一件不合格品，其来自 A 线的概率为？

- A. 3/7
- B. 5/7
- C. 4/7
- D. 6/7

**答案：A**

$$P(A\mid \text{不合格})=\frac{0.6\times 0.1}{0.6\times 0.1+0.4\times 0.2}=\frac{3}{7}$$

---

### 9、在贝叶斯网络中，节点 A 和 B 都依赖于节点 C，且彼此之间条件独立（即给定 C 的情况下，A 和 B 独立）。如果已知 C 发生的概率为 0.5，那么 A 和 B 同时发生的联合概率是多少？

- A. 无法确定
- B. 0.25
- C. 0.75
- D. 0.5

**答案：A**

仅给出 $P(C)=0.5$ 且 $A\perp B\mid C$，不足以确定 $P(A,B)$。

---

### 10、某疾病在人群中的发病率为 1%，检测准确率如下：  
若患病，检测阳性概率 99%；若未患病，检测阴性概率 98%。假设某人检测结果为阳性，那么他实际患病的概率最接近（ ）

- A. 10%
- B. 99%
- C. 66%
- D. 33%

**答案：D**

$$P(\text{病}|\text{+})=\frac{0.99\times 0.01}{0.99\times 0.01+0.02\times 0.99}\approx 33\%$$

---

### 11、如果齐次线性方程组  
\begin{cases} a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = 0 \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = 0 \\ \cdots \cdots \\ a_{s1}x_1 + a_{s2}x_2 + \cdots + a_{sn}x_n = 0 \end{cases}  
有非零解，那么（）

- A. 以上三种情况都有可能发生
- B. s=n
- C. s > n
- D. s < n

**答案：A**

是否有非零解取决于系数矩阵秩是否小于 $n$，与 $s$ 和 $n$ 的大小关系不唯一绑定。

---

### 12、用 Newton 迭代法去求方程 f(x) = x^2 - 3 = 0 的正根，初值 x_0 = 2，求第一次迭代后的近似值 x_1：

- A. 1.25
- B. 1.75
- C. 2.5
- D. 2.25

**答案：B**

$$x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)},\ f(x)=x^2-3,\ f'(x)=2x,\ x_0=2\Rightarrow x_1=1.75$$

---

### 13、考虑一个简单的马尔可夫决策过程（MDP），包含两个状态：s1 和 s2，其中 s2 是终止状态。在每个状态有两个可行动作：a1 和 a2。环境的动态如下：  
在状态 s1：执行动作 a1：以概率 1 转移到 s2，获得即时奖励 r=3。执行动作 a2：以概率 1 转移到 s2，获得即时奖励 r=-1。  
在状态 s2：为终止状态，无动作可执行。  
使用 Q-learning 算法进行学习，折扣因子 γ=0.9，学习率 α=0.1。初始 Q 值全为 0.假设智能体从 s1 开始，执行动作 a1，转移到 s2，并终止。问题：请根据此次经历，使用 Q-learning 更新规则，计算更新后的 Q(s1, a1)的值。

- A. 0.4
- B. 0.3
- C. 0.5
- D. 0.2

**答案：B**

终止状态下一步价值为 0：$Q(s_1,a_1)\leftarrow 0+0.1\times(3+0.9\times 0)=0.3$。

---

### 14、假设某种通信编码解码算法中，在接收端需要计算如下的对数-求和-指数表达式：  
y=\log\left(\sum_{k=1}^{n} e^{x_k}\right)  
其中，x_k 为接收信号的采样值（其取值可能在区间 [-2000,2000] 内浮动）。现有数据：x_1=1000, x_2=1001, x_3=1002, x_4=-1000  
为了避免数值溢出影响计算精度，最稳定的计算方法是？

- A. 使用变换 y=\max_j(x_j)+\log\!\left(\sum_{k=1}^{n} e^{\,x_k-\max_j(x_j)}\right)
- B. 直接计算 y=\log\!\left(e^{1000}+e^{1001}+e^{1002}+e^{-1000}\right)
- C. 使用变换 y=2000+\log\!\left(\sum_{k=1}^{n} e^{\,x_k-2000}\right)
- D. 使用变换 y=\bar{x}+\log\!\left(\sum_{k=1}^{n} e^{\,x_k-\bar{x}}\right)，其中 \bar{x} 为所有采样值的平均值。

**答案：A**

log-sum-exp 稳定化：以 $\max_j(x_j)$ 为基准可避免溢出/下溢并保持数值稳定。

---

### 15、在一个文本分类任务中，某个单词‘free’在垃圾邮件 (Spam) 中出现的概率是 0.8，在正常邮件 (Ham) 中出现的概率是 0.1。假设垃圾邮件和正常邮件的先验概率相等，P(Spam)=P(Ham)=0.5。根据朴素贝叶斯分类器，如果一封新邮件中包含了单词‘free’，那么这封邮件是垃圾邮件的后验概率 P(Spam | ‘free’)是多少？

- A. 1/2
- B. 4/9
- C. 8/9
- D. 8/10

**答案：C**

$$P(\text{Spam}\mid \text{‘free’})=\frac{0.8\times 0.5}{0.8\times 0.5+0.1\times 0.5}=\frac{8}{9}$$

---

## 二、多选题

### 16、给定 B = AA^T, A \in \mathbb{R}^{m \times n}, m \le n, \text{rank}(A) = m。关于特征值与奇异值，下列描述正确的是（多选）

- A. B 可逆且正定
- B. B 的所有特征值均为非负
- C. B 的非零特征值等于 A 非零奇异值的平方
- D. 若 λ 为 B 的特征值，则 \sqrt{\lambda} 必为 A 的奇异值

**答案：A、B、C、D**

- ✅ A：$\text{rank}(A)=m$ 且 $B=AA^T$（$m\times m$）满秩 ⇒ 可逆且正定
- ✅ B：$AA^T$ 半正定 ⇒ 特征值非负
- ✅ C：$AA^T$ 的非零特征值等于 $A$ 非零奇异值的平方
- ✅ D：奇异值与 $AA^T$ 特征值满足 $\sigma=\sqrt{\lambda}$

---

### 17、矩阵范数需要满足的定义有：

- A. 正规性
- B. 正定性
- C. 对称性
- D. 相容性
- E. 半可加性
- F. 齐次性

**答案：B、D、E、F**

- ✅ B：$\|A\|\ge 0$ 且 $\|A\|=0 \Leftrightarrow A=0$
- ✅ D：相容性（常用：$\|AB\|\le \|A\|\|B\|$）
- ✅ E：半可加性（$\|A+B\|\le \|A\|+\|B\|$）
- ✅ F：齐次性（$\|\alpha A\|=|\alpha|\|A\|$）

---

### 18、在处理复杂的序列到序列（Seq2Seq）任务时，研究者们发展了多种RNN的增强技术。以下关于这些技术的描述，哪些是准确的？

- A. 在LSTM单元中，遗忘门（Forget Gate）的作用是让网络根据当前输入和前一时刻的隐藏状态，学习决定应该从长期记忆（细胞状态）中丢弃哪些信息。
- B. 双向RNN（Bidirectional RNN）因其能够整合过去和未来的上下文信息，在许多离线处理任务（如机器翻译）上表现优越，但它无法被用于需要即时响应的实时流数据预测。
- C. GRU作为LSTM的简化变体，其参数数量更少，计算成本更低，通常被认为是在模型性能和计算效率之间的一个有效权衡。
- D. GRU单元通过一个更新门和一个重置门和一个独立的输出门来精确控制信息流动，结构比LSTM更复杂。

**答案：A、B、C**

- ✅ A：遗忘门决定细胞状态中“丢弃哪些信息”
- ✅ B：双向RNN依赖未来上下文，实时流式预测不可用
- ✅ C：GRU通常参数更少、计算更省
- ❌ D：GRU不含“独立输出门”，结构通常比 LSTM 更简单

---

### 19、指令微调的目标是

- A. 让模型理解并遵循自然语言指令
- B. 替代预训练阶段
- C. 提升 zero-shot 任务泛化能力
- D. 减少模型参数量

**答案：A、C**

- ✅ A：对齐“指令→行为”
- ❌ B：不替代预训练
- ✅ C：常用于提升 zero-shot 能力
- ❌ D：不以减少参数量为目标

---

### 20、关于大模型的文本语义相似度计算，下列说法正确的是

- A. 所有大模型的嵌入向量维度必须统一才能计算相似度
- B. 可通过计算两文本的嵌入向量（Embedding）余弦相似度实现
- C. 模型对语义相似但表达方式差异大的文本识别能力有限
- D. 微调时增加同义句对数据可提升相似度计算精度

**答案：B、D**

- ❌ A：不要求“所有大模型嵌入维度统一”
- ✅ B：可用 embedding 余弦相似度
- ❌ C：题目答案未选该项
- ✅ D：增加同义句对可提升相似度精度




下面按你标的（2）（9）（14）（16）（20）来讲，**每题都给：为什么错/为什么对 + 数学式 + 物理直觉/记忆法**（冲刺版，短但能背）。

---

## （2）你说的这句为什么错？

你写的句子其实是 **第 3 题的 B 选项**：

> “子词分词算法的词汇表是在模型预训练完成后，根据训练语料的最终统计特性动态生成的。”

**为什么错（核心点）**：
子词词表（BPE/WordPiece）**通常是在预训练前**，用训练语料做统计（合并/最大化似然等）先训练出一个固定词表，然后预训练阶段就一直用它把文本映射成 token id。
如果词表在预训练后才“动态生成”，那预训练时：

* **Embedding 矩阵大小** `|V|×d` 都没法确定（词表大小 `|V|` 是模型参数形状的一部分）
* token id 的映射会变，训练得到的 embedding 就对不上

**一句话记忆**：

> **先定“字典”(vocab)，再学“语言”(pretrain)**。
> 像做谱线拟合/数值求解：**先定基底/网格**，再拟合参数；基底变了，系数就没意义。

---

## （9）这题在问什么？为什么答案是 “无法确定”？

题干（第 9 题）：A、B 都依赖 C，且 **给定 C 时 A 与 B 条件独立**；只给了 `P(C)=0.5`，问 `P(A∩B)`。

**关键公式**：
条件独立给的是
$$P(A,B\mid C)=P(A\mid C),P(B\mid C)$$

但我们要的联合概率是（对 C 求全概率）：
$$P(A,B)=\sum_{c} P(c),P(A\mid c),P(B\mid c)$$

你只有 `P(C)=0.5`，**没有** `P(A|C)`、`P(B|C)`（以及在 `¬C` 下的条件概率），所以 **算不出来**。

**直觉（像物理的“混合态”）**：
C 就像系统处在两种宏观相（c=0 或 1）的“开关/环境状态”。你只知道它有一半时间开、一半时间关，但不知道在“开/关”两种情况下 A、B 发生的概率是多少——那当然不能推出 A 和 B 同时发生的概率。

**记忆法**：

> **只给先验 P(C)，不给似然 P(A|C), P(B|C) ⇒ 后验/联合都别想算。**

---

## （14）为什么不用 C？（log-sum-exp 稳定性）

题干是计算：
$$y=\log\left(\sum_{k=1}^{n}e^{x_k}\right)$$
且 `x_k ∈ [-2000,2000]`，给了样例 `1000,1001,1002,-1000`。

**标准稳定式（答案 A）**：取实际最大值 `m=max(x)`：
$$y=m+\log\left(\sum_k e^{x_k-m}\right)$$
这样 `x_k-m ≤ 0`，指数不会溢出，同时最大的那项变成 `e^0=1`，不会被数值“压扁”。

**为什么 C 不如 A（甚至可能直接算崩）**：C 用固定常数 2000：
$$y=2000+\log\left(\sum_k e^{x_k-2000}\right)$$

对样例最大值 1002 来说：
$$1002-2000=-998,\quad e^{-998}$$
在 **float64** 里 `e^{-998}` 已经小到会**下溢成 0**（因为 float64 最小正数约 $5\times 10^{-324}$，而 $e^{-998}\approx 10^{-434}$），于是求和可能变成 0，`log(0)` 直接 `-inf`。

**物理直觉（配分函数/自由能）**：
$\log\sum e^{x}$ 就像自由能/对数配分函数。稳定计算就是把“能量最低（贡献最大）”的那项因子提出来（对应这里取最大 $x$），否则你把参考点选得太离谱，所有项都被压到 0，信息丢光。

**记忆法**：

> log-sum-exp：**永远减“当前最大值 max(x)”**，不要减“最大可能值/常数”。

---

## （16）特征值 vs 奇异值：到底一不一样？这题为什么全对？

题干：$B=AA^T$，$A\in\mathbb{R}^{m\times n}$，$m\le n$，且 `rank(A)=m`（满行秩）。

### 1) 概念差别（你问的“是不是不一样”）

* **特征值（eigenvalue）**：只对**方阵** $M$，满足
  $$Mv=\lambda v$$
* **奇异值（singular value）**：对任意矩阵 $A$（可长方），来自 SVD：
  $$A=U\Sigma V^T,\quad \Sigma=\mathrm{diag}(\sigma_1,\dots)$$
  $\sigma_i\ge 0$ 表示线性变换把单位球拉伸成椭球的“主轴长度”。

**物理直觉**：

* 特征值：沿某些方向（特征向量）**缩放且方向不变**。
* 奇异值：不要求方向不变，只看整体线性映射对长度的最大/最小拉伸（更“几何”）。

### 2) 这题为什么 A/B/C/D 都对？

用 SVD：
$$A=U\Sigma V^T$$
则
$$B=AA^T=U\Sigma V^T V\Sigma^T U^T=U(\Sigma\Sigma^T)U^T$$
所以 **$B$ 的特征值就是 $\Sigma\Sigma^T$ 的对角元**：
$$\lambda_i(B)=\sigma_i(A)^2$$

* **B 选项：特征值非负** ✅
  因为 $AA^T$ 总是半正定：
  $$x^TAA^Tx=|A^Tx|^2\ge 0$$

* **A 选项：B 可逆且正定** ✅
  已知 `rank(A)=m` ⇒ $A$ 满行秩 ⇒ $AA^T$ 满秩 ⇒ 所有 $\lambda_i>0$ ⇒ 正定可逆。

* **C 选项：B 的非零特征值 = A 非零奇异值平方** ✅
  直接来自 $\lambda_i=\sigma_i^2$。

* **D 选项：若 λ 是 B 特征值，则 √λ 是 A 奇异值** ✅
  同上。

**记忆法（超好背）**：

> **奇异值是“平方根特征值”**：
> $$\sigma(A)=\sqrt{\lambda(AA^T)}=\sqrt{\lambda(A^TA)}$$

---

## （20）正确答案应该是什么？你错在哪？

题干：关于大模型文本语义相似度计算，哪些正确？

网页答案区给的是：**B、D**（你当时选了 B、C、D，多选题“多选/少选都算错”，所以挂在 C 上）。

逐项解释（冲刺版）：

* **A 错**：
  “所有大模型嵌入向量维度必须统一才能计算相似度” ❌
  计算余弦相似度需要的是**两个向量同维**，但这通常意味着：**用同一个 embedding 模型/同一表示空间**。
  不要求“所有大模型彼此维度统一”。

* **B 对**：
  “可通过两文本 embedding 的余弦相似度实现” ✅
  $$\cos(\theta)=\frac{u\cdot v}{|u||v|}$$

* **C（在这套题的标准答案里）不选**：
  “模型对语义相似但表达差异大的文本识别能力有限” 这句话太泛、太绝对；而现代语义 embedding/对比学习就是专门做 paraphrase/同义表达对齐的。题库标准答案因此没把它当“正确说法”。

* **D 对**：
  “微调时增加同义句对数据可提升相似度计算精度” ✅
  这是典型做法（对比学习/双塔/句向量训练）。

**记忆法**：

> 相似度 = **同空间 embedding + cosine**；提升 = **加同义句对做微调**；
> “维度统一”不是跨模型硬要求，关键是“同空间”。

---

如果你愿意，我可以把你这 5 个点做成一页“**冲刺背诵卡**”（每题 3 行公式 + 1 行记忆法），你刷题时直接看这页就行。你想要卡片版吗？

---

## ✅ 错题整理（结构化）

> **整理日期**: 2026-01-04  
> **总题数**: 20 | **错题数**: 5 | **正确率**: 75%

---

### 📋 知识点标注表

| 题号 | 知识点 | Tag2 | 是否错题 |
|------|--------|------|----------|
| Q1 | 经典机器学习 | 正则化 | ✅ |
| Q2 | 深度学习基础 | 激活函数 | ✅ |
| Q3 | 大模型(LLM) | Tokenizer | ⚠️ 需复习 |
| Q4 | 优化器 | Adam | ✅ |
| Q5 | 注意力与Transformer | Scaled Dot-Product | ✅ |
| Q6 | 线性代数 | 矩阵对角化 | ✅ |
| Q7 | 经典机器学习 | KNN | ✅ |
| Q8 | 概率统计 | 贝叶斯 | ✅ |
| Q9 | 概率统计 | 条件独立 | ⚠️ 需复习 |
| Q10 | 概率统计 | 贝叶斯 | ✅ |
| Q11 | 线性代数 | 齐次方程组 | ✅ |
| Q12 | 优化与数值方法 | Newton迭代 | ✅ |
| Q13 | 强化学习与对齐 | Q-learning | ✅ |
| Q14 | 优化与数值方法 | 数值稳定性 | ❌ 错 |
| Q15 | 概率统计 | 朴素贝叶斯 | ✅ |
| Q16 | 线性代数 | SVD/奇异值 | ⚠️ 需复习 |
| Q17 | 线性代数 | 矩阵范数 | ✅ |
| Q18 | 深度学习基础 | RNN/LSTM/GRU | ✅ |
| Q19 | 大模型(LLM) | 指令微调 | ✅ |
| Q20 | 大模型(LLM) | Embedding相似度 | ❌ 错 |

---

### 📊 知识点掌握度统计

| 知识点 | 总数 | 错/需复习 | 正确率 | 状态 |
|--------|------|-----------|--------|------|
| 概率统计 | 4 | 1 | 75% | 🟨 |
| 线性代数 | 4 | 1 | 75% | 🟨 |
| 大模型(LLM) | 3 | 2 | 33% | 🔴 |
| 优化与数值方法 | 2 | 1 | 50% | 🟨 |
| 经典机器学习 | 2 | 0 | 100% | ✅ |
| 深度学习基础 | 2 | 0 | 100% | ✅ |
| 注意力与Transformer | 1 | 0 | 100% | ✅ |
| 优化器 | 1 | 0 | 100% | ✅ |
| 强化学习与对齐 | 1 | 0 | 100% | ✅ |

---

### 🔥 薄弱环节 Top-3（20251015）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 大模型(LLM) | 2/3 | 33% | Tokenizer时序、Embedding相似度 |
| 2 | 优化与数值方法 | 1/2 | 50% | log-sum-exp稳定性 |
| 3 | 概率统计 | 1/4 | 75% | 条件独立≠可算联合概率 |

---

## 错题详解

### Q3: BPE/WordPiece 词表生成时机

**原题**：
> 在大型语言模型（LLM）的文本预处理流程中，分词器（Tokenizer）将原始文本转换为模型可处理的 token ID 序列。关于现代 LLM 常用的子词（subword）分词算法（如 BPE、WordPiece），以下哪项描述是正确的？

**正确答案**：C

**易错点**：
误以为词表在预训练后动态生成（选B）

**核心公式/结论**：
$$\text{词表（Vocab）} \xrightarrow{\text{先定}} \text{Embedding矩阵 } |V| \times d \xrightarrow{\text{再训}} \text{预训练}$$

**一眼记住**：
> **先定"字典"(vocab)，再学"语言"(pretrain)**。词表大小 $|V|$ 决定 Embedding 矩阵形状，不能后改。

---

### Q9: 条件独立 ≠ 可算联合概率

**原题**：
> 在贝叶斯网络中，节点 A 和 B 都依赖于节点 C，且彼此之间条件独立。如果已知 C 发生的概率为 0.5，那么 A 和 B 同时发生的联合概率是多少？

**正确答案**：A（无法确定）

**易错点**：
只给 $P(C)=0.5$，没给 $P(A|C)$、$P(B|C)$，无法计算 $P(A,B)$

**核心公式/结论**：
$$P(A,B) = \sum_{c} P(c) \cdot P(A|c) \cdot P(B|c)$$

**一眼记住**：
> **只给先验 P(C)，不给似然 P(A|C), P(B|C) ⇒ 联合概率算不出来**

---

### Q14: log-sum-exp 数值稳定性

**原题**：
> 需要计算：$y=\log\left(\sum_{k=1}^{n} e^{x_k}\right)$，$x_k \in [-2000, 2000]$

**正确答案**：A（用 max(x) 做基准）

**易错点**：
选C（固定常数2000）会导致下溢：$e^{1002-2000}=e^{-998}$ 在 float64 下溢为0

**核心公式/结论**：
$$y = m + \log\left(\sum_k e^{x_k - m}\right), \quad m = \max_k(x_k)$$

**一眼记住**：
> **log-sum-exp：永远减"当前最大值 max(x)"**，不减固定常数

---

### Q16: 特征值 vs 奇异值

**原题**：
> 给定 $B = AA^T$, $A \in \mathbb{R}^{m \times n}$, $m \le n$, $\text{rank}(A) = m$

**正确答案**：A、B、C、D（全选）

**易错点**：
混淆特征值和奇异值的定义

**核心公式/结论**：
$$\boxed{\sigma(A) = \sqrt{\lambda(AA^T)} = \sqrt{\lambda(A^TA)}}$$

**一眼记住**：
> **奇异值 = "平方根特征值"**

---

### Q20: 文本语义相似度

**原题**：
> 关于大模型的文本语义相似度计算，哪些正确？

**正确答案**：B、D

**易错点**：
多选了C。现代语义 embedding 专门做 paraphrase 对齐，C 表述太绝对。

**核心公式/结论**：
$$\cos(\theta) = \frac{u \cdot v}{|u||v|}$$

**一眼记住**：
> 相似度 = **同空间 embedding + cosine**；提升 = **加同义句对做微调**

---

## 📚 核心知识点速查

### 1. 大模型(LLM)

| 概念 | 核心公式/结论 |
|------|---------------|
| BPE/词表 | 先定词表 → 再预训练 |
| Embedding相似度 | $\cos(\theta) = \frac{u \cdot v}{\|u\|\|v\|}$，需同空间 |

### 2. 概率统计

| 概念 | 核心公式/结论 |
|------|---------------|
| 条件独立 | $P(A,B\|C) = P(A\|C) \cdot P(B\|C)$，仍需条件概率才能算联合 |
| 贝叶斯 | $P(A\|B) = \frac{P(B\|A)P(A)}{P(B)}$ |

### 3. 优化与数值方法

| 概念 | 核心公式/结论 |
|------|---------------|
| log-sum-exp | $y = m + \log\sum e^{x_k - m}$，$m = \max(x)$ |
| Newton迭代 | $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$ |

### 4. 线性代数

| 概念 | 核心公式/结论 |
|------|---------------|
| SVD | $A = U\Sigma V^T$，$\sigma_i = \sqrt{\lambda_i(AA^T)}$ |
| 满秩条件 | $\text{rank}(A) = m \Rightarrow AA^T$ 正定可逆 |

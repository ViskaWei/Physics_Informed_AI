# 2025-10-17 AI 方向题

## 一、单选题

### 1. 设 $\lambda_1,\lambda_2$ 是矩阵 $A$ 的两个不同的特征值，对应的特征向量分别为 $\alpha_1,\alpha_2$，则 $\alpha_1, A(\alpha_1 + \alpha_2)$ 线性无关的充分必要条件是

- A. $\lambda_1 \neq 0$
- B. $\lambda_2 \neq 0$
- C. $\lambda_1 = 0$
- D. $\lambda_2 = 0$

**答案：B**

$$
A(\alpha_1+\alpha_2)=\lambda_1\alpha_1+\lambda_2\alpha_2
$$
与 $\alpha_1$ 线性相关当且仅当 $\lambda_2=0$，故线性无关当且仅当 $\lambda_2\neq 0$。

---

### 2. 设 $X \sim B(n=10, p=0.3)$，则 $P(X=2)$ 的值为

- A. $C_{10}^{2} \times 0.7^{2} \times 0.3^{8}$
- B. $e^{-3} \times \dfrac{3^{2}}{2!}$
- C. $0.3 \times 0.7$
- D. $C_{10}^{2} \times 0.3^{2} \times 0.7^{8}$

**答案：D**

$$
P(X=k)=C_n^k p^k (1-p)^{n-k}
$$
代入 $n=10,\ p=0.3,\ k=2$ 得 $P(X=2)=C_{10}^{2}(0.3)^2(0.7)^8$。

---

### 3. 在现代大语言模型（LLM）的架构设计中，关于编码器（Encoder）和解码器（Decoder）的使用，以下哪项描述是最准确的？

- A. 所有大型语言模型，如 BERT、GPT 系列和 T5，都采用标准的编码器 - 解码器（Encoder-Decoder）架构。
- B. 仅使用编码器（Encoder-only）架构的模型（如 BERT）能够进行高效的自回归文本生成，因为它可以并行处理整个输入序列。
- C. 仅使用解码器（Decoder-only）架构的模型（如 GPT 系列）在自回归语言建模任务上表现出色，其自注意力机制通常需要掩码（Masked）以防止信息泄露。
- D. 编码器 - 解码器（Encoder-Decoder）架构（如 T5）中的解码器，在训练和推理时都只关注编码器的输出，并不需要关注已经生成的前文。

**答案：C**

Decoder-only（如 GPT）做自回归建模，并使用因果掩码（Masked）防止信息泄露。

---

### 4. 一个均匀6面骰子，平均投掷多少次能投出连续的两个6点

- A. 48
- B. 37
- C. 36
- D. 42

**答案：D**

$$
E=\frac{1-p^r}{p^r(1-p)},\quad p=\frac{1}{6},\ r=2 \Rightarrow E=42
$$

---

### 5. 用牛顿迭代法求 $f(x) = x^3 - 2$ 的根，如果初始值为 $x_0 = 1$，一步迭代值 $x_1$ 应为：

- A. $\sqrt[3]{2}$
- B. $\frac{4}{3}$
- C. $\frac{3}{2}$
- D. $\frac{2}{3}$

**答案：B**

$$
x_1=x_0-\frac{f(x_0)}{f'(x_0)}=1-\frac{-1}{3}=\frac{4}{3}
$$

---

### 6. 已知 $x = 1.250 \pm 0.040,\ y = 8.000 \pm 0.200$，计算 $z = xy$ 的绝对误差限。

- A. 0.578
- B. 0.562
- C. 0.560
- D. 0.570

**答案：A**

$$
\Delta z \le |y|\Delta x + |x|\Delta y + \Delta x \Delta y
=8\times0.04+1.25\times0.2+0.04\times0.2=0.578
$$

---

### 7. 在逻辑回归中，若目标函数为
$$
J(\theta) = -\frac{1}{m}\sum_{i=1}^m \left[y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))\right]
$$
以下哪种优化方法最可能陷入局部最优？

- A. 拟牛顿法（BFGS）
- B. 高斯 - 牛顿法
- C. 随机梯度下降（SGD）
- D. 牛顿法

**答案：C**

SGD 为一阶随机优化，噪声更大，相对更容易停在较差点。

---

### 8. 一个团队构建的RAG问答系统在测试中表现不佳。用户提问：“在B-21轰炸机的研发合同中，诺斯罗普·格鲁曼公司承接的初始作战能力（IOC）目标年份是哪一年？” 系统未能找到答案，尽管知识库中的一份文档里明确写着：“……该项目的主要承包商是诺斯罗普·格鲁曼公司。根据合同条款，B-21轰炸机的初始作战能力目标设定在2027年。” 工程师排查后发现，文档切分时，恰好将这两句话分在了两个相邻但独立的文本块（chunk）中：
- Chunk A: “……该项目的主要承包商是诺斯罗普·格鲁曼公司。”
- Chunk B: “根据合同条款，B-21轰炸机的初始作战能力目标设定在2027年。”
基于以上信息，请判断导致这次检索失败的最直接、最根本的技术原因是什么？

- A. Tokenizer的词汇表不包含“诺斯罗普·格鲁曼”，导致该专有名词被错误地分割，影响了查询向量的生成。
- B. 使用的Embedding模型质量太差，无法理解“诺斯罗普·格鲁曼”和“B-21轰炸机”之间的关联。
- C. 文档切分策略不当，破坏了关键信息的上下文完整性。Chunk A虽然包含了关键实体，但与“年份”无关；Chunk B虽然含答案，但其自身文本不足以和包含“B-21”及“诺斯罗普·格鲁曼”的查询建立强语义关联。
- D. 向量搜索中的近似最近邻（ANN）算法精度过低，没能从海量向量中找到正确的Chunk B。

**答案：C**

切分把实体与答案拆到不同 chunk，破坏最小语义单元，导致检索难以同时命中“B-21/诺斯罗普·格鲁曼/年份”。

---

### 9. 关于线性回归算法的下列说法中，哪一项是正确的？

- A. 岭回归（Ridge Regression）在损失函数中引入 L1 正则项，可将部分系数压缩至零，从而实现特征选择。
- B. 多重共线性会导致特征之间高度线性相关，虽然模型在训练集上的预测精度可能仍然较高，但会显著增大回归系数的方差。
- C. 线性回归的 $R^2$ 值越高，表明模型对新样本的预测能力越强；因此，应尽可能通过增加特征来提升 $R^2$，从而提高模型泛化性能。
- D. 普通最小二乘法（OLS）通过最小化残差的绝对值之和来估计参数，该问题存在闭式解，可通过矩阵运算直接求解。

**答案：B**

多重共线性使回归系数方差膨胀、模型不稳定（训练拟合可能仍高）。

---

### 10. 已知隔壁老王夫妇生了两个孩子，且这两个孩子不是同时双胞胎。 有一天，邻居听到隔壁老王夸一个男孩活泼可爱，确认了隔壁老王夫妇至少有一个男孩。请问，老王两个孩子都是男孩的概率是多少？

- A. 2/5
- B. 7/12
- C. 1/2
- D. 1/3

**答案：D**

在“至少一个男孩”条件下样本空间为 $\{BB,BG,GB\}$（等概率），两男孩仅 $BB$，概率为 $\frac{1}{3}$。

---

### 11. 在神经网络中，以下哪种激活函数可以缓解梯度消失问题，并且在隐藏层中被广泛使用？

- A. Sigmoid
- B. Tanh
- C. ReLU
- D. 阶跃函数

**答案：C**

Sigmoid/Tanh 饱和区梯度接近 0；阶跃函数不可导；ReLU 正区间梯度为常数 1，更能缓解梯度消失。

---

### 12. 矩阵的秩是描述矩阵非常关键的性质，其描述的是一组向量中线性无关基的个数的最大值。把矩阵的行或列看成一个向量，可以定义行秩和列秩。行秩和列秩的大小关系正确的是：

- A. 行秩 < 列秩
- B. 其他三种都有可能
- C. 行秩 = 列秩
- D. 行秩 > 列秩

**答案：C**

任何矩阵的行秩恒等于列秩，二者共同等于矩阵的秩。

---

### 13. 在 Transformer 的架构中，几乎每个子层（如自注意力层、前馈网络层）的输出都会经过一个 Layer Normalization（层归一化）操作，在自然语言处理任务中，相比于在计算机视觉中常用的 Batch Normalization（批归一化），选择 LayerNorm 最关键的原因是？

- A. LayerNorm 能够更好地处理词汇表外的（Out-of-Vocabulary）单词。
- B. LayerNorm 在计算归一化统计量（均值和方差）时，独立于批次中的其他样本，因此对不同长度的输入序列和变化的批次大小（Batch Size）具有更好的鲁棒性。
- C. LayerNorm 的计算量远小于 Batch Normalization，可以显著提升模型的训练速度。
- D. Batch Normalization 只能用于卷积层，而 LayerNorm 可以用于全连接层。

**答案：B**

LayerNorm 对每个样本独立归一化，不依赖 batch 统计量，更适合可变长输入/变化 batch size 场景。

---

### 14. 已知从人群中随机抽取一名男性，该男性年龄在30-40岁之间的概率为0.25，在其他年龄段的概率为0.75，且人群中30-40岁年龄段和其他年龄段中男性的比例都为0.4。若用贝叶斯定理估计该男性属于30-40岁年龄段的后验概率，结果为：

- A. 0.4
- B. 0.25
- C. 0.1
- D. 0.5

**答案：B**

$$
P(\text{30–40}\mid \text{男})=\frac{0.4\times 0.25}{0.4\times 0.25+0.4\times 0.75}=0.25
$$

---

### 15. 关于检索增强生成技术（Retrieval-Augmented Generation, RAG）中，将 HNSW（Hierarchical Navigable Small World）等近似最近邻（ANN）向量索引用于查询阶段的主要优势是什么？

- A. 在检索过程中自动优化嵌入表示，增强查询与文档的语义对齐能力。
- B. 显著降低查询延迟，同时仅轻微牺牲召回率，实现效率与效果的平衡
- C. 利用分层图结构加速搜索路径收敛，提升高维空间中相似性排序的稳定性。
- D. 结合向量二值化技术，将存储开销压缩至 1-bit，极大节省内存占用。

**答案：B**

HNSW 属 ANN 检索：查询低延迟，通常只轻微牺牲召回率，权衡效率与效果。

---

## 二、多选题

### 16. 下列哪些方法可用于解决过拟合

- A. L2正则化
- B. 增加训练样本多样性
- C. 减少模型复杂度
- D. 提前停止

**答案：A、B、C、D**

- ✅ A：L2 正则化限制参数规模，降低过拟合风险  
- ✅ B：扩充与多样化样本提升泛化能力  
- ✅ C：减少模型复杂度降低拟合噪声的能力  
- ✅ D：提前停止避免在训练集上继续过拟合  

---

### 17. 关于大模型的文本语义相似度计算，下列说法正确的是

- A. 所有大模型的嵌入向量维度必须统一才能计算相似度
- B. 可通过计算两文本的嵌入向量（Embedding）余弦相似度实现
- C. 微调时增加同义句对数据可提升相似度计算精度
- D. 模型对语义相似但表达方式差异大的文本识别能力有限

**答案：B、C**

- ❌ A：计算相似度只需在同一表示空间内，不要求“所有大模型维度必须统一”  
- ✅ B：常用余弦相似度衡量嵌入相似性  
- ✅ C：增加同义句对可提升语义对齐与相似度质量  
- ❌ D：该表述过于绝对（标准答案未选）  

---

### 18. 关于向量空间的基和维度，以下哪些说法正确？

- A. 零向量不能包含在任何基中
- B. $\mathbb{R}^n$ 空间的维度为 $n$
- C. 矩阵列向量张成空间的维度等于其行向量张成空间的维度
- D. 一组向量是基当且仅当其线性无关且张成整个空间

**答案：A、B、C、D**

- ✅ A：基要求线性无关，零向量会导致线性相关  
- ✅ B：$\mathbb{R}^n$ 的维度为 $n$  
- ✅ C：行秩 = 列秩（维度相等）  
- ✅ D：基的充要条件：线性无关且张成  

---

### 19. 在神经网络的反向传播过程中，以下哪些描述是正确的？

- A. 反向传播直接修改网络的参数，无需梯度下降等优化算法
- B. 反向传播需要先进行一次完整的前向传播以得到输出结果
- C. 反向传播通过链式法则计算损失函数对各层参数的梯度
- D. 在反向传播中，梯度会逐层从输出层向输入层传播

**答案：B、C、D**

- ❌ A：反向传播计算梯度，参数更新仍需优化算法（如梯度下降）  
- ✅ B：需先前向得到输出与损失  
- ✅ C：链式法则逐层求梯度  
- ✅ D：梯度从输出层向前传播  

---

### 20. 数据之间的关系以及数据服从的分布形态影响了模型建模的效果，下列关于线性回归的描述中，哪些是正确的？

- A. 线性回归模型中，误差项（残差）必须服从正态分布
- B. 线性回归模型可以通过最小二乘法进行参数估计
- C. 线性回归模型假设自变量和因变量之间存在线性关系
- D. 线性回归模型的目标是最大化预测值与真实值之间的误差

**答案：B、C**

- ❌ A：残差正态性是部分统计推断条件之一，并非“必须”  
- ✅ B：可用最小二乘法估计参数  
- ✅ C：假设自变量与因变量线性关系  
- ❌ D：目标是最小化误差（如平方误差），不是最大化  

---


下面按你提的 4 个点分别讲（都对应这套题里的 Q1 / Q6 / Q17 / Q18），尽量给你**可背的公式 + 直觉**。

---

## （1）Q1 为啥答案是 **B：$\lambda_2\neq 0$**？

题面：$\lambda_1,\lambda_2$ 是矩阵 $A$ 的两个不同特征值，对应特征向量 $\alpha_1,\alpha_2$。问：$\alpha_1$ 与 $A(\alpha_1+\alpha_2)$ 线性无关的充要条件。

关键一步先写出来（这是“秒杀点”）：
$$
A(\alpha_1+\alpha_2)=A\alpha_1+A\alpha_2=\lambda_1\alpha_1+\lambda_2\alpha_2.
$$

现在判断 $\alpha_1$ 和这个向量是否线性无关：

* **如果 $\lambda_2=0$**：
  $$
  A(\alpha_1+\alpha_2)=\lambda_1\alpha_1
  $$
  这显然和 $\alpha_1$ **共线** ⇒ **线性相关**（不行）。

* **如果 $\lambda_2\neq 0$**：反证一下。若线性相关，则存在 $c$ 使
  $$
  \lambda_1\alpha_1+\lambda_2\alpha_2 = c\alpha_1
  \Rightarrow \lambda_2\alpha_2=(c-\lambda_1)\alpha_1
  $$
  因为 $\lambda_2\neq 0$，推出 $\alpha_2$ 是 $\alpha_1$ 的倍数 ⇒ $\alpha_1,\alpha_2$ 线性相关。
  但**不同特征值的特征向量线性无关**（线代基本定理）⇒ 矛盾。
  所以 $\lambda_2\neq 0$ 时必线性无关。

**一眼记住**：

> 看 $A(\alpha_1+\alpha_2)=\lambda_1\alpha_1+\lambda_2\alpha_2$，要让它不“塌”到 $\alpha_1$ 那条线上，就必须让“另一个方向”的系数 $\lambda_2$ 不为 0。

---

## （6）Q6 绝对误差怎么计算？（加减 / 乘除 / 导数总公式）

这里考的是**“绝对误差限”（worst-case 上界）**，不是统计意义的方差传播。

### 1）通用（最重要）导数公式：误差传播上界

若 $z=f(x_1,\dots,x_n)$，且 $x_i = \bar x_i \pm \Delta x_i$，则**一阶误差上界**：
$$
\Delta z ;\lesssim; \sum_i \left|\frac{\partial f}{\partial x_i}\right|\Delta x_i
$$
若你想更保险（像这题一样），乘法会多一个二阶项（来自 $(\Delta x)(\Delta y)$）：

---

### 2）加减法（最好背）

* $z=x+y$ 或 $z=x-y$：
  $$
  \Delta z \le \Delta x+\Delta y
  $$

---

### 3）乘法（本题用这个）

* $z=xy$：
  严格上界（把最坏情况都算进去）：
  $$
  \Delta z \le |y|\Delta x + |x|\Delta y + \Delta x,\Delta y
  $$
  （前两项是一阶导数项，最后一项是二阶“保险项”）

本题：$x=1.250\pm0.040,\ y=8.000\pm0.200$
代入：
$$
\Delta z \le 8\times0.04 + 1.25\times0.2 + 0.04\times0.2 = 0.578
$$
所以答案 0.578。

**小直觉**：乘法误差≈“一个变动 × 另一个的量级”，所以是 $|y|\Delta x + |x|\Delta y$；最后的 $\Delta x\Delta y$ 是“双双走极端”的补偿。

---

### 4）除法（常考）

* $z=\dfrac{x}{y}$：
  $$
  \Delta z \lesssim \left|\frac{1}{y}\right|\Delta x + \left|\frac{x}{y^2}\right|\Delta y
  $$
  或用相对误差记忆：
  $$
  \frac{\Delta z}{|z|}\approx \frac{\Delta x}{|x|}+\frac{\Delta y}{|y|}
  $$
  （相对误差更好背）

---

## （17）“不要求所有大模型维度统一”怎么理解？维度不同怎么求相似度？

你卡的点非常对：**余弦相似度/点积相似度要求两个向量同维度**。

### 这句话正确的含义是：

> 做相似度计算时，只要**你在同一个 embedding 空间里**（同一个模型/同一个编码器产出的向量），就行。
> 并不需要“世界上所有大模型输出维度都一致”。

在 RAG/语义检索里，标准流程是：

* 用同一个 embedding 模型 $f(\cdot)$
* 文档：$e_d=f(\text{doc}) \in \mathbb{R}^d$
* 查询：$e_q=f(\text{query}) \in \mathbb{R}^d$
* 相似度（最常用余弦）：
  $$
  \cos(e_q,e_d)=\frac{e_q^\top e_d}{|e_q|,|e_d|}
  $$

### 那如果你真的拿到了两个不同模型、不同维度的向量呢？

**不能直接算**（因为连 $e_q^\top e_d$ 都不定义）。常见解决方案只有这些：

1. **最推荐：重算一遍 embedding**
   统一用一个模型把两段文本都编码到同一个空间。

2. **学一个投影/对齐（需要训练数据）**
   学 $W$ 把 $\mathbb{R}^{d_1}\to\mathbb{R}^{d_2}$：
   $$
   e^{(2)} \approx W e^{(1)}
   $$
   然后再在同一维度里算相似度。（这不是考试重点，但是真实工程会这么干）

3. **改用 cross-encoder（模型直接吃文本对给分）**
   不靠“向量相似度”，而是模型输出一个标量相似度 $s(\text{text}_1,\text{text}_2)$。

**一眼记住**：

> 相似度不是“所有模型统一维度”，而是“同一次比较必须同空间同维度”。工程上就是“检索用同一个 embedding 模型”。

---

## （18）基一定要正交吗？（不需要）

**基（basis）的定义**就是你说的那句充要条件：

* **线性无关**
* **张成整个空间**

**完全不要求正交**。

### 反例（最经典，背住）

在 $\mathbb{R}^2$，向量
$$
v_1=(1,0),\quad v_2=(1,1)
$$
它们：

* 不共线 ⇒ 线性无关 ⇒ 张成整个平面 ⇒ **是一组基**
* 但 $v_1\cdot v_2 = 1 \neq 0$ ⇒ **不正交**

### 什么时候会提“正交基”？

* 当你的空间带内积（比如欧氏空间）时，“正交/正交归一基”只是**更方便**（坐标好算、数值稳定），不是必须。
* 而且在这种空间里，任意基都能用 **Gram–Schmidt** 正交化成一组正交（归一）基。

**一眼记住**：

> 基=能当“坐标轴”的一组向量；正交基=“互相垂直的坐标轴”，方便但不强制。

---
不是的——你把 **“线性无关”** 和 **“统计里的相关/协方差（cov）”** 混到一起了。

---

## 1) 线性无关 ≠ 用 cov 判断

**cov（协方差）**是随机变量的概念：
$$
\mathrm{Cov}(X,Y)=\mathbb{E}\big[(X-\mathbb{E}X)(Y-\mathbb{E}Y)\big]
$$
它讨论的是“随机变量是否相关”（而且 **Cov=0 也不一定独立**，除非一些特殊情形如联合高斯）。

**线性无关（linear independence）**是线性代数概念，定义是：

对向量 $v_1,v_2$，
$$
c_1 v_1 + c_2 v_2 = 0 \ \Rightarrow\ c_1=c_2=0
$$
这跟 cov 没关系。

---

## 2) 点乘为 0 是“正交”，不是“线性无关”的定义

* **正交**：$v_1^\top v_2 = 0$
* **线性无关**：不互为倍数（对两个向量来说）

关系是：

* 若 $v_1^\top v_2 = 0$ 且 $v_1,v_2\neq 0$ ⇒ **一定线性无关**（充分条件）
* 但线性无关 **不要求** 点乘为 0（不是必要条件）

两个秒杀反例：

1. **点乘不为 0 也可以线性无关**
   $$
   v_1=(1,0),\quad v_2=(1,1)
   $$
   $v_1^\top v_2 = 1 \neq 0$，但它们显然不共线 ⇒ 线性无关。

2. **点乘不为 0 也可能线性相关**
   $$
   v_1=(1,0),\quad v_2=(2,0)
   $$
   $v_1^\top v_2 = 2 \neq 0$，但 $v_2=2v_1$ ⇒ 线性相关。

所以你写的 “$\lambda_1\alpha_1 * \lambda_1\alpha_1 >0$ 就线性无关” 不对：
这是在算 **一个向量和它自己** 的点乘：
$$
(\lambda_1\alpha_1)^\top(\lambda_1\alpha_1)=\lambda_1^2|\alpha_1|^2>0
$$
只说明它不是零向量，**完全不能说明**它和另一个向量是否线性无关。

---

## 3) 回到 Q1：怎么判断 $\alpha_1$ 和 $A(\alpha_1+\alpha_2)$ 是否线性无关？

题目关键等式（网页 TextSol 里也写了）：
$$
A(\alpha_1+\alpha_2)=\lambda_1\alpha_1+\lambda_2\alpha_2
$$

要判断它和 $\alpha_1$ 是否线性相关，就假设存在 $c$ 使：
$$
\lambda_1\alpha_1+\lambda_2\alpha_2 = c\alpha_1
\Rightarrow \lambda_2\alpha_2=(c-\lambda_1)\alpha_1
$$

由于 $\alpha_1,\alpha_2$ 对应 **不同特征值**，所以它们线性无关 ⇒ 上式成立只能是
$$
\lambda_2=0
$$
因此：

* 线性相关 $\Leftrightarrow \lambda_2=0$
* 线性无关 $\Leftrightarrow \lambda_2\neq 0$（所以答案是 B）

---

## 4) Q18 顺带：基一定要正交吗？

不需要。**基的充要条件就是：线性无关 + 张成**。正交只是“更舒服”的一种基（坐标好算），不是必须。

---

## ✅ 错题整理（结构化）

> 📅 整理日期: 2026-01-04  
> 📊 统计: 20 题，4 错，正确率 80%

---

### 📋 知识点标注表

| 题号 | 知识点 Tag | 是否错题 |
|------|-----------|----------|
| Q1 | 线性代数（特征值/线性无关） | ❌ 错 |
| Q2 | 概率统计（二项分布） | ✅ 对 |
| Q3 | 注意力与Transformer（架构） | ✅ 对 |
| Q4 | 概率统计（期望/马尔可夫） | ✅ 对 |
| Q5 | 优化与数值方法（牛顿迭代） | ✅ 对 |
| Q6 | 优化与数值方法（误差传播） | ❌ 错 |
| Q7 | 优化与数值方法（优化算法） | ✅ 对 |
| Q8 | 大模型LLM（RAG/Chunk） | ✅ 对 |
| Q9 | 经典机器学习（多重共线性） | ✅ 对 |
| Q10 | 概率统计（条件概率） | ✅ 对 |
| Q11 | 深度学习基础（激活函数） | ✅ 对 |
| Q12 | 线性代数（行秩=列秩） | ✅ 对 |
| Q13 | 注意力与Transformer（LayerNorm） | ✅ 对 |
| Q14 | 概率统计（贝叶斯） | ✅ 对 |
| Q15 | 大模型LLM（HNSW/ANN） | ✅ 对 |
| Q16 | 经典机器学习（过拟合） | ✅ 对 |
| Q17 | 大模型LLM（Embedding相似度） | ❌ 错 |
| Q18 | 线性代数（基/正交） | ❌ 错 |
| Q19 | 深度学习基础（反向传播） | ✅ 对 |
| Q20 | 经典机器学习（线性回归） | ✅ 对 |

---

### 📊 知识点掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 线性代数 | 3 | 2 | 33% ⚠️ |
| 优化与数值方法 | 3 | 1 | 67% |
| 大模型(LLM) | 3 | 1 | 67% |
| 概率统计 | 4 | 0 | 100% ✅ |
| 注意力与Transformer | 2 | 0 | 100% ✅ |
| 深度学习基础 | 2 | 0 | 100% ✅ |
| 经典机器学习 | 3 | 0 | 100% ✅ |

---

### 🔥 薄弱环节 Top-3（2025-10-17）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 线性代数 | 2/3 | 33% | 特征值线性无关、基不需正交 |
| 2 | 优化与数值方法 | 1/3 | 67% | 误差传播公式 |
| 3 | 大模型(LLM) | 1/3 | 67% | Embedding相似度需同空间 |

---

## 📚 核心知识点速查

### 1. 线性代数

#### 特征向量线性无关判断
$$
A(\alpha_1+\alpha_2) = \lambda_1\alpha_1 + \lambda_2\alpha_2
$$
- 与 $\alpha_1$ 线性相关 $\Leftrightarrow \lambda_2 = 0$
- 与 $\alpha_1$ 线性无关 $\Leftrightarrow \lambda_2 \neq 0$
- **关键**: 不同特征值的特征向量一定线性无关

#### 线性无关 vs 正交
| 概念 | 定义 | 关系 |
|------|------|------|
| 线性无关 | $c_1v_1 + c_2v_2 = 0 \Rightarrow c_1=c_2=0$ | 不互为倍数 |
| 正交 | $v_1^\top v_2 = 0$ | 垂直 |
| **关系** | 正交 ⇒ 线性无关（充分不必要） | 线性无关不要求点乘=0 |

#### 基的定义
- **充要条件**: 线性无关 + 张成整个空间（不要求正交）

### 2. 优化与数值方法

#### 误差传播公式
| 运算 | 绝对误差上界 |
|------|-------------|
| $z = x \pm y$ | $\Delta z \le \Delta x + \Delta y$ |
| $z = xy$ | $\Delta z \le \|y\|\Delta x + \|x\|\Delta y + \Delta x \Delta y$ |
| $z = x/y$ | $\Delta z \lesssim \frac{\Delta x}{\|y\|} + \frac{\|x\|\Delta y}{y^2}$ |

### 3. 大模型 (LLM)

#### Embedding 相似度计算
- **关键**: 只需在**同一表示空间**内（同一模型编码），不要求所有模型维度统一
- **余弦相似度**: $\cos(e_q, e_d) = \frac{e_q^\top e_d}{|e_q||e_d|}$

---

## Q1: 特征向量线性无关条件

**原题**：
> 设 $\lambda_1,\lambda_2$ 是矩阵 $A$ 的两个不同的特征值，对应的特征向量分别为 $\alpha_1,\alpha_2$，则 $\alpha_1, A(\alpha_1 + \alpha_2)$ 线性无关的充分必要条件是
> - A. $\lambda_1 \neq 0$
> - B. $\lambda_2 \neq 0$
> - C. $\lambda_1 = 0$
> - D. $\lambda_2 = 0$

**正确答案**：B

**易错点**：
混淆"线性无关"与"正交/协方差"概念；忘记展开 $A(\alpha_1+\alpha_2)$

**核心公式/结论**：
$$
A(\alpha_1+\alpha_2) = \lambda_1\alpha_1 + \lambda_2\alpha_2
$$
- 若 $\lambda_2=0$，则与 $\alpha_1$ 共线 ⇒ 线性相关
- 若 $\lambda_2\neq 0$，则无法化简 ⇒ 线性无关

**一眼记住**：
> 看 $\lambda_2$ 是否为 0 —— $\lambda_2=0$ 让"另一个方向"塌掉

---

## Q6: 乘法误差传播

**原题**：
> 已知 $x = 1.250 \pm 0.040,\ y = 8.000 \pm 0.200$，计算 $z = xy$ 的绝对误差限。
> - A. 0.578
> - B. 0.562
> - C. 0.560
> - D. 0.570

**正确答案**：A

**易错点**：
漏掉二阶项 $\Delta x \cdot \Delta y$

**核心公式/结论**：
$$
\Delta z \le |y|\Delta x + |x|\Delta y + \Delta x \cdot \Delta y = 8 \times 0.04 + 1.25 \times 0.2 + 0.04 \times 0.2 = 0.578
$$

**一眼记住**：
> 乘法误差 = "一个变动 × 另一个量级" + 二阶保险项

---

## Q17: Embedding 相似度维度问题

**原题**：
> 关于大模型的文本语义相似度计算，下列说法正确的是
> - A. 所有大模型的嵌入向量维度必须统一才能计算相似度
> - B. 可通过计算两文本的嵌入向量（Embedding）余弦相似度实现
> - C. 微调时增加同义句对数据可提升相似度计算精度
> - D. 模型对语义相似但表达方式差异大的文本识别能力有限

**正确答案**：B、C

**易错点**：
误以为"不同维度也能算相似度"；实际是"不要求所有模型维度统一"，但同一次比较必须同空间

**核心公式/结论**：
$$
\cos(e_q, e_d) = \frac{e_q^\top e_d}{|e_q||e_d|}
$$

**一眼记住**：
> "同一次比较必须同空间同维度"，工程上用同一个 embedding 模型

---

## Q18: 基是否需要正交

**原题**：
> 关于向量空间的基和维度，以下哪些说法正确？
> - A. 零向量不能包含在任何基中
> - B. $\mathbb{R}^n$ 空间的维度为 $n$
> - C. 矩阵列向量张成空间的维度等于其行向量张成空间的维度
> - D. 一组向量是基当且仅当其线性无关且张成整个空间

**正确答案**：A、B、C、D

**易错点**：
误以为"基必须正交"

**核心公式/结论**：
- **基的充要条件**: 线性无关 + 张成整个空间（不要求正交）
- **经典反例**: $v_1=(1,0), v_2=(1,1)$ 不正交但是 $\mathbb{R}^2$ 的基

**一眼记住**：
> 基 = "能当坐标轴的一组向量"；正交基只是"更方便"，不是必须

---

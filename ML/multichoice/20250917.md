# 2025-09-17 AI岗题

## 一、单选题

### 1. 设总体 $X$ 服从参数为 $\lambda(\lambda>0)$ 的泊松分布，其概率分布为 $P(X=a)=\dfrac{\lambda^a e^{-\lambda}}{a!}$，$a=0,1,2,\ldots$。从该总体中抽取容量为 $n$ 的独立随机样本 $X_1,\ldots,X_n$。则参数 $\lambda$ 的极大似然估计是（）。

- A. $\displaystyle \frac{1}{n - 1}\sum_{i=1}^{n} (X_i - \overline{X}) ^ 2$（其中 $\overline{X}=\frac{1}{n}\displaystyle \sum_{i=1}^{n} X_i$）
- B. $\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i ^ 2$
- C. $\displaystyle \max(X_1,\ldots,X_n)$
- D. $\displaystyle \frac{1}{n}\displaystyle \sum_{i=1}^{n} X_i$

**答案：D**

$$
L(\lambda)\propto \lambda^{\sum X_i}e^{-n\lambda},\quad \hat\lambda=\frac{1}{n}\sum_{i=1}^{n}X_i=\overline{X}
$$

---

### 2. 在神经网络中，反向传播算法的主要作用是（）

- A. 随机初始化网络中的权重参数
- B. 增加网络的深度以提升模型表达能力
- C. 根据损失函数的梯度来更新网络参数
- D. 计算损失函数在训练数据上的平均值

**答案：C**

- 反向传播通过链式法则计算损失对各层参数的梯度，并据此更新参数。

---

### 3. 在集成学习的 Stacking 方法中，以下哪一项准确描述了第二层模型（Meta-Model）的作用？（）

- A. 将基模型的预测结果作为输入，学习它们的组合方式
- B. 对基模型的预测结果进行加权平均
- C. 对基模型的参数进行优化
- D. 对基模型的预测结果进行特征组合

**答案：A**

- 元模型以一级基模型预测作为新特征，学习最优组合方式提升泛化。

---

### 4. 假设 $\theta$ 的一个无偏估计量是 $\hat\theta$，其在一组样本下的值为 $z$。对于下述陈述，正确的是（）

- A. $\hat\theta$ 是 $\theta$ 的组合估计
- B. $E\hat\theta=z$
- C. $\theta=z$
- D. 估计量 $\hat\theta$ 不存在系统性误差

**答案：D**

$$
E[\hat\theta]=\theta
$$

---

### 5. 对于二元逻辑回归问题，已训练得到的参数为 $w=[0.5,-1.0],\ b=-1.0$。现在给输入样本 $x=[2,1]$，使用 Sigmoid 函数计算预测概率 $P(y=1\mid x)$，结果最接近的是以下哪个值？

- A. 0.27
- B. 0.43
- C. 0.56
- D. 0.60

**答案：A**

$$
z=w^\top x+b=0.5\times2-1.0\times1-1.0=-1,\quad \sigma(z)=\frac{1}{1+e^{-z}}=\sigma(-1)\approx0.2689
$$

---

### 6. 给定二维平面样本点：A(0,0)，B(1,1)，C(6,5)，D(7,6)。使用 K-means 算法（设聚类数 $k=2$，且用欧氏距离）进行聚类，初始聚类中心分别为 (2,2) 和 (8,8)。经过一次聚类迭代（即“分配–更新”各执行一次）后，新的中心坐标分别是（）。

- A. (0.0,0.0) (6.5,5.5)
- B. (0.5,0.5) (6.0,5.0)
- C. (0.5,0.5) (6.5,5.5)
- D. (3.5,3.5) (4.0,4.0)

**答案：C**

$$
\mu_1=\left(\frac{0+1}{2},\frac{0+1}{2}\right)=(0.5,0.5),\quad
\mu_2=\left(\frac{6+7}{2},\frac{5+6}{2}\right)=(6.5,5.5)
$$

---

### 7. 在机器学习中，关于监督学习、无监督学习、半监督学习和强化学习的下列说法中，哪一项是正确的？

- A. 半监督学习结合少最标记样本和大量未标记样本，以降低标注成本并提升性能。
- B. 强化学习通过从带标签的数据集中学习输入到输出的映射关系，以最小化预测误差。
- C. 无监督学习的目标是根据输入特征预测离散类别标签，常用于分类任务。
- D. 监督学习通过环境奖励信号训练智能体，常见于游戏和机器人控制。

**答案：A**

- 半监督学习：少量标注 + 大量未标注联合训练。

---

### 8. 指数分布常用于非负随机变量事件发生的时间间隔，例如用户下次访问网站的等待时间。关于指数分布，下列哪项说法是错误的？

- A. 如果一系列事件的发生次数服从泊松分布，那么这些事件之间的时间间隔服从指数分布。
- B. 指数分布的期望和方差是相等的。
- C. 指数分布的概率密度函数在定义域内是单调递减的。
- D. 指数分布具有“无记忆性”，即等待了多久不影响未来还需要等待多久的概率分布。

**答案：B**

$$
X\sim \text{Exp}(\lambda)\Rightarrow E[X]=\frac{1}{\lambda},\quad \mathrm{Var}(X)=\frac{1}{\lambda^2}
$$

---

### 9. 在神经网络中，由于模型层数增加，在计算梯度反向传播时容易造成数值表示过小，引发“梯度消失”问题，以下哪种激活函数可以有效地缓解梯度消失问题？

- A. Softmax
- B. Tanh
- C. ReLU
- D. Sigmoid

**答案：C**

- ReLU 在正区间梯度不饱和，能缓解梯度消失。

---

### 10. 一个 LLM 推理服务在处理用户请求时，将过程分为 Prefill（处理提示）和 Decode（生成回复）两个阶段。现在有两个典型的请求：请求A（代码生成）：提示(Prompt)很长，包含大量上下文代码(2000 tokens)，但期望生成的代码片段较短(100 tokens)。请求B(多轮对话)：提示很短，只是用户的最新一句话(20 tokens)，但需要生成一段较长的回复(500 tokens)，且此时对话历史已有1500 tokens。基于对现代 GPU 架构和 Transformer 推理机制的理解，以下哪个判断是正确的？

- A. 请求A的耗时瓶颈主要在于 Decode 阶段，因为需要处理复杂的代码逻辑。
- B. 请求B的耗时瓶颈主要在于 Prefill 阶段，因为它需要快速响应用户的输入。
- C. 请求A的耗时主要由 GPU 的计算单元（如 Tensor Core）的吞吐量决定；请求B的耗时主要由 GPU 显存（HBM）的带宽决定。
- D. 为了优化请求B的总体延迟，最有效的措施是采用先进的模型量化技术（如 INT4），因为这能大幅减少每次 Decode 步骤中的矩阵乘法计算量。

**答案：C**

- Prefill 常为大矩阵乘法（偏计算瓶颈）；Decode 常受 KV cache 读写（偏带宽瓶颈）。

---

### 11. 下列方法中没有考虑先验分布的是（）

- A. 贝叶斯分类器
- B. 最大后验估计
- C. 最大似然估计
- D. 贝叶斯学习

**答案：C**

- 最大似然估计只依赖似然函数，不显式使用先验分布。

---

### 12. 为减少数值计算中的舍入误差，以下哪种方法是有效的？

- A. 增加算法的迭代次数
- B. 简化数学模型
- C. 增大数值方法的步长
- D. 使用更高精度的浮点数表示

**答案：D**

- 提高浮点数精度可直接减少舍入误差。

---

### 13. 在大型语言模型(LLM)进行自回归文本生成（例如，根据提示生成后续文本）的过程中，关于每一步生成新token时的计算，以下哪项描述是正确的？

- A. 生成每个新token时，只需要计算当前token对应的Query向量，而不需要重新计算Key和Value。
- B. 模型在生成第一个token后，其后续所有token的生成都可以完全并行计算，无需等待。
- C. 通常使用“KV缓存”(Key-Value Cache)技术来存储已计算的注意力Key和Value向量，避免在生成后续token时重复计算历史信息。
- D. 每生成一个新token，模型都需要重新计算之前所有已生成token的隐蔽状态（hidden states）。

**答案：C**

- 使用 KV cache 存历史 Key/Value，避免重复计算历史信息。

---

### 14. 在使用混合精度训练（如 FP16/BF16 与 FP32 结合）训练大型语言模型时，引入“损失缩放”（Loss Scaling）技术的主要原因是什么？

- A. 加速前向传播计算中矩阵乘法的执行速度。
- B. 防止梯度在低精度（如 FP16）表示下因数值过小而下溢（underflow）为零。
- C. 降低模型权重更新时的数值噪声，提高训练稳定性。
- D. 减少反向传播过程中激活值（activations）的内存占用。

**答案：B**

- 通过放大 loss/梯度避免 FP16 下梯度下溢为 0。

---

### 15. 已知准确值 $a=100$ 的一个近似值 $a^*=50$，该近似值的相对误差为

- A. -0.5
- B. 1
- C. 50
- D. -50

**答案：A**

$$
e_r=\frac{a^*-a}{a}=\frac{50-100}{100}=-0.5
$$

---

## 二、多选题

### 16. 在数值计算中，以下关于误差的叙述中，正确的有

- A. 相对误差较小时，绝对误差一定也很小
- B. 对误差定义为近似值减去真实值的绝对值
- C. 截断误差是由于使用无限过程近似有限步骤计算引起的
- D. 有效数字位数越多，计算结果的精度越高

**答案：B、C、D**

- ❌ A：相对误差小并不一定意味着绝对误差小。
- ✅ B：误差常定义为近似值与真实值之差的绝对值。
- ✅ C：截断误差来自对无穷过程的有限截断近似。
- ✅ D：有效数字位数越多通常精度越高。

---

### 17. 误差来源中，截断误差和舍入误差的区别正确表述有哪些?

- A. 两者均可完全消除
- B. 舍入误差源于有限精度表示
- C. 截断误差源于有限步近似无穷过程(如泰勒级数截断)
- D. 在迭代法中，通常情况下，截断误差随迭代减小，舍入误差随迭代累积

**答案：B、C、D**

- ❌ A：两者不可能完全消除。
- ✅ B：舍入误差来自有限精度表示。
- ✅ C：截断误差来自有限步近似无穷过程（如泰勒级数截断）。
- ✅ D：迭代中截断误差通常减小，舍入误差可能累积。

---

### 18. 深入分析 Transformer 架构的内部组件，会发现其设计充满了精妙的权衡。以下关于其核心组件的描述，哪些是准确的？

- A. 标准的正弦/余弦位置编码（Sinusoidal Positional Encoding）是专门为相对位置设计的编码。
- B. 多头注意力机制的核心思想是将 Query、Key 和 Value 投影到多个不同的低维表示子空间中，让每个“头”有机会学习到不同类型的依赖关系。
- C. 前馈网络（FFN）子层通过为序列中的每一个位置学习一套独立的权重参数，来增强模型对位置信息的建模能力。
- D. RMSNorm 作为一种比 LayerNorm 更简洁的归一化技术，其关键改进在于省略了对输入向量减去均值的中心化步骤，只进行缩放操作。

**答案：B、D**

- ❌ A：标准正弦/余弦位置编码是绝对位置编码。
- ✅ B：多头注意力将 Q/K/V 投影到多个子空间学习不同依赖。
- ❌ C：FFN 参数在序列各位置共享。
- ✅ D：RMSNorm 省略减均值，仅按均方根缩放并线性缩放。

---

### 19. 在大语言模型分布式并行训练中，all-reduce 是一个常用的集合通信算子。在以下哪些场景下，all-reduce 更倾向于通过 reduce-scatter 和 all-gather 实现？

- A. 短序列
- B. 大 batch size
- C. 长序列
- D. 低带宽

**答案：B、C**

- ❌ A：短序列通信规模小，额外拆分收益有限。
- ✅ B：大 batch size 通信规模大，更适合分解实现以提升效率。
- ✅ C：长序列通信规模大，更适合分解实现以提升效率。
- ❌ D：低带宽下拆分并不一定更优，可能被额外开销抵消。

---

### 20. 已知三个数据点 (1,3)、(2,6)、(3,11)，用二次多项式 $P(x)=ax^2+bx+c$ 进行拟合（要求拟合多项式过前两个点，且与第三个点的误差平方和最小），则下列说法正确的是（）

- A. 系数 $a=1$
- B. 系数 $b=0$
- C. 拟合多项式与第三个点的误差为 0
- D. 系数 $c=2$

**答案：A、B、C、D**

$$
a=1,\ b=0,\ c=2,\quad P(x)=x^2+2,\quad P(3)=11
$$
















下面按你问的编号来讲（尽量把“直觉 + 公式 + 怎么记”都说清楚）。

---

## （1）“$\lambda$ 的 MLE 是什么？不同 distribution 有关系吗？”

先澄清：你题里是 **泊松分布** $X_i\sim \text{Pois}(\lambda)$，问的是 **$\lambda$ 的极大似然估计（MLE）**。

### MLE 的通用定义

给定 i.i.d. 样本 $x_1,\dots,x_n$，MLE 就是
[
\hat\theta=\arg\max_\theta \prod_{i=1}^n p(x_i\mid \theta)
]
实际做法几乎总是取对数（把乘积变求和）：
[
\ell(\theta)=\sum_{i=1}^n \log p(x_i\mid\theta)
]

### 泊松的 MLE（题1）

泊松 pmf：
[
p(x\mid\lambda)=\frac{\lambda^x e^{-\lambda}}{x!}
]
对数似然：
[
\ell(\lambda)=\sum_{i=1}^n \big(x_i\log\lambda-\lambda-\log(x_i!)\big)
]
求导：
[
\frac{\partial \ell}{\partial \lambda}=\sum_{i=1}^n\left(\frac{x_i}{\lambda}-1\right)=\frac{1}{\lambda}\sum x_i-n
]
令其为 0：
[
\hat\lambda=\frac{1}{n}\sum_{i=1}^n x_i=\overline{x}
]
**结论：泊松参数的 MLE 就是样本均值。**

> 你写的 “var(lambda) 的 MLE” 如果你是想问“$\hat\lambda$ 的方差”，那泊松下也很好算：因为 $\overline X$ 的方差是
> [
> \mathrm{Var}(\hat\lambda)=\mathrm{Var}(\overline X)=\frac{\mathrm{Var}(X)}{n}=\frac{\lambda}{n}
> ]
> （用到泊松 $\mathrm{Var}(X)=\lambda$。）

### 不同 distribution 有关系吗？

**有关系但不是“一个公式通吃”。** MLE 取决于分布的 $p(x\mid\theta)$。

但很多常见分布（指数族）会出现很“规律”的 MLE，比如常常等于某个“样本统计量”（均值、方差等）。几个你可以背的典型：

* Bernoulli($p$)：$\hat p=\overline x$
* Poisson($\lambda$)：$\hat\lambda=\overline x$
* Exponential(rate $\lambda$)：$\hat\lambda=\dfrac{1}{\overline x}$
* Normal($\mu,\sigma^2$)：$\hat\mu=\overline x$，且 $\widehat{\sigma^2}_{\text{MLE}}=\dfrac{1}{n}\sum (x_i-\overline x)^2$（注意不是 $1/(n-1)$）

---

## （2）Stacking 是什么？和 Bagging / Boosting 平行吗？

**是平行的“集成学习（ensemble）”大类里的三种主要套路**，但机制不同。

### Bagging（并行、降方差）

* 核心：对数据做 bootstrap 重采样 → 训练很多基模型 → **平均/投票**
* 直觉：把“抖动很大（高方差）”的模型平均掉
* 代表：Random Forest（= bagging + 特征子采样）

### Boosting（串行、降偏差）

* 核心：模型一个接一个训练，每个新模型更关注前面错的样本（或拟合残差）
* 直觉：逐步“补短板”，把弱学习器叠成强学习器
* 代表：AdaBoost / GBDT / XGBoost / LightGBM

### Stacking（学一个“组合器”）

* 核心：先训练多个基模型 $f_1,\dots,f_m$，再训练一个“元模型” $g$ 来学怎么组合它们：
  [
  \hat y = g\big(f_1(x),f_2(x),\dots,f_m(x)\big)
  ]
* 关键点：**元模型训练必须用 out-of-fold 预测**，否则会“泄漏”（基模型在训练集上过拟合，元模型被假象骗了）。
* 直觉：不是“平均”，而是“让另一个模型学会：谁在什么区域更靠谱”。

一句话对比：

* Bagging：**平均**（稳）
* Boosting：**逐步纠错**（强）
* Stacking：**学组合规则**（灵活、异构模型也行）

---

## （8）哪个分布满足“期望=方差”？怎么记指数分布的均值/方差？

题8里说“指数分布期望和方差相等”是错的。

### 期望 = 方差 的经典分布

**泊松分布：** 若 $X\sim\text{Pois}(\lambda)$，
[
E[X]=\lambda,\quad \mathrm{Var}(X)=\lambda
]

### 指数分布怎么记

若 $X\sim\text{Exp}(\lambda)$（这里 $\lambda$ 是 **rate**，单位是 $1/\text{time}$）：
[
E[X]=\frac{1}{\lambda},\quad \mathrm{Var}(X)=\frac{1}{\lambda^2},\quad \mathrm{SD}(X)=\frac{1}{\lambda}
]

**最稳的记忆法：量纲（单位）分析**

* $\lambda$ 是“到达率/发生率”：每单位时间发生多少次，单位 $1/t$
* “等待时间”的平均值当然是“率的倒数”：$t = 1/\lambda$
* 方差单位是 $t^2$，所以必须是 $1/\lambda^2$

**更硬核的一个公式（顺手记）**
[
E[X^k]=\frac{k!}{\lambda^k}
]
所以 $E[X]=1/\lambda$，$E[X^2]=2/\lambda^2$，于是
[
\mathrm{Var}(X)=E[X^2]-E[X]^2=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}
]

---

## （9）“ReLU 正区间梯度不饱和”是什么意思？

先写 ReLU：
[
\text{ReLU}(x)=\max(0,x)
]
导数（反传用的“梯度”）：
[
\text{ReLU}'(x)=
\begin{cases}
1, & x>0\
0, & x<0
\end{cases}
]

### “饱和”是什么意思？

像 Sigmoid：
[
\sigma(x)=\frac{1}{1+e^{-x}}
]
当 $x\to +\infty$，$\sigma(x)\to 1$，导数 $\sigma'(x)=\sigma(x)(1-\sigma(x))\to 0$
当 $x\to -\infty$，$\sigma(x)\to 0$，导数也 $\to 0$

这就叫**在大正/大负时“饱和”**：输出贴着 0 或 1，导数几乎 0，反向传播的梯度传不动 → **梯度消失**。

### ReLU 为什么好

ReLU 在 $x>0$ 时导数恒为 1，**不会因为数值变大就导数变 0**，所以说“正区间不饱和”，梯度更容易往前传。

> 小提醒：ReLU 在 $x<0$ 导数是 0，会有 “dead ReLU”（永远不激活）。工程上常用 LeakyReLU / GELU 缓解。

---

## （10）Prefill / Decode 为什么 C 对、D 不对？

你卡住的是这句：
**A（长 prompt 短生成）主要看算力吞吐；B（短 prompt 长生成）主要看显存带宽。**

### 先把两个阶段想成两种“工作形态”

* **Prefill**：一次性把 prompt 的很多 token（例如 2000）同时过模型
  → 大矩阵乘法（GEMM）很“饱满”，GPU Tensor Core 吃得很满
  → 往往是 **compute-bound（算力瓶颈）**

* **Decode**：每次只生成 1 个新 token，要连续生成 500 次
  每一步都要做 attention：当前 token 的 Query 要去和“历史所有 token 的 K/V（KV cache）”做交互
  → 每一步都要从 HBM 里把很长的 KV cache 读出来（层数多、头多、序列长就更大）
  → 小算子 + 大量内存读写，容易变 **memory-bandwidth-bound（显存带宽瓶颈）**

所以 C 的直觉是：

* 请求 A：prefill 吃算力（矩阵乘法吞吐）
* 请求 B：decode 每步读 KV cache 吃带宽

### 为什么 D（INT4 量化）不是“最有效”

权重量化（INT4）主要减少：

* 权重显存占用
* 部分矩阵乘法算力开销

但对 B 这种 **decode 被 KV cache 带宽卡住** 的情况：

* **KV cache 不是“权重”**，通常权重量化不直接缩小 KV cache 的读带宽
* 生成 500 token 仍然是 500 步串行（量化也不能消除串行依赖）

对 B 更对症的优化反而是（给你建立正确直觉）：

* **减少 KV cache 体积**：比如 MQA/GQA（减少 KV 头数）
* **KV cache 量化/压缩**（专门针对 KV）
* **speculative decoding**（用小模型草拟、多 token 并行验算）
* 更高效的 attention kernel / paged KV 等

---

## （14）损失缩放（Loss Scaling）举个最简单例子

核心问题：**FP16 能表示的最小正数有限，梯度太小会下溢成 0。**

### 一个玩具例子（下溢）

假设某层真实梯度是
[
g=10^{-9}
]
FP16 最小的可表示数量级大概在 $10^{-8}$ 左右（更准确说最小 subnormal 约 $6\times 10^{-8}$），所以存进去就可能变成
[
g_{\text{fp16}}\approx 0
]
一旦梯度成 0，这一层就**不更新**了。

### Loss Scaling 怎么救

我们把 loss 乘一个缩放因子 $S$（比如 $S=1024$）：
[
L' = S\cdot L
]
因为反传梯度对 loss 是线性的：
[
\nabla_\theta L' = S\cdot \nabla_\theta L = Sg
]
于是梯度变成：
[
g' = 1024 \times 10^{-9} = 1.024\times 10^{-6}
]
这就能被 FP16 表示，不会下溢为 0。

**更新参数之前再“除回来”**：
[
g = \frac{g'}{S}
]
所以参数更新量不变，但数值计算过程中避免了下溢。

### 动态 Loss Scaling（你经常会看到）

如果 $S$ 太大，会导致梯度溢出成 inf/NaN；动态策略就是：

* 如果检测到溢出 → 降低 $S$
* 一段时间没溢出 → 尝试增大 $S$

---
下面把你这 4 个点用“**定义 → 直觉 → 小例子**”讲透。

---

## （18）正弦/余弦位置编码：到底是绝对还是相对？

**结论：标准 sinusoidal positional encoding 是“绝对位置编码”**——每个位置 index（pos=0,1,2, …）都有一个固定向量，直接 **加到 token embedding 上**。

经典定义（Transformer 原版）：
[
\mathrm{PE}(pos,2i)=\sin\left(\frac{pos}{10000^{2i/d}}\right),\quad
\mathrm{PE}(pos,2i+1)=\cos\left(\frac{pos}{10000^{2i/d}}\right)
]
这里的输入就是 **pos 本身**（绝对位置）。

那为什么你会觉得“像相对位置”？因为它有个“**可以被模型用来推相对位移**”的性质：
[
\sin(pos+\Delta)=\sin(pos)\cos(\Delta)+\cos(pos)\sin(\Delta)
]
[
\cos(pos+\Delta)=\cos(pos)\cos(\Delta)-\sin(pos)\sin(\Delta)
]
所以理论上模型能从绝对 PE 的组合里学到“位移 (\Delta)”相关的信息。

✅ **关键区分**

* **绝对位置编码**：输入是 (pos)，每个位置独立有编码，跟“你是谁”无关。
* **相对位置编码**：输入是 ((pos_q - pos_k)) 或者 bucket(distance)，直接把“距离/相对位移”作为 bias 或 embedding 加进注意力分数里，例如：
  [
  \text{score}_{qk}=\frac{qk^\top}{\sqrt{d_k}} + b(pos_q-pos_k)
  ]
  所以题目里说“标准正弦/余弦 PE 专门为相对位置设计”是不对的：它**不是专门为相对位置设计**，只是**有利于**模型“推”相对关系。

---

## （17）舍入误差为什么会“累积”？给个最简单例子

舍入误差会累积的核心原因：**每一步运算都会产生一个小误差，这个误差会进入下一步运算继续被使用**，于是越做越多步，就可能把小误差“叠加/放大”。

### 玩具例子：每步都四舍五入到 1 位小数

假设机器只能保留 1 位小数，每一步都做：
[
x \leftarrow \mathrm{round}(x+0.06,\ 1\text{位小数})
]
从 (x_0=0.0) 开始：

* 第 1 步：(0.0+0.06=0.06 \to 0.1)
* 第 2 步：(0.1+0.06=0.16 \to 0.2)
* 第 3 步：(0.2+0.06=0.26 \to 0.3)

你会发现：**每步都“多进”了 0.04**（因为 0.06 被舍入到 0.1）。
于是 10 步后：

* 机器结果：(x_{10}=1.0)
* 真实结果：(10\times0.06=0.60)

误差：(1.00-0.60=0.40) ——这就是“累积”。

> 现实 FP16/FP32 不是十进制一位小数，但道理一样：每步 rounding 产生的小偏差会进入下一步，迭代多了就可能累计成明显偏差。

---

## （19）all-reduce 为啥会用 reduce-scatter + all-gather？我完全不会

先把三个东西搞清楚（以数据并行 DP 的“梯度同步”为例）：

### 1）all-reduce 是啥

每张卡都有一个梯度向量 (g_r)（r=rank），all-reduce 做的是：
[
g ;=;\sum_{r=1}^{p} g_r
]
并且 **每张卡最后都拿到同一个 (g)**（然后更新参数）。

### 2）reduce-scatter + all-gather 是啥（拆法）

假设有 (p) 张卡，把向量按长度切成 (p) 份：
[
g_r = \big[g_r^{(1)}, g_r^{(2)}, \dots, g_r^{(p)}\big]
]

* **reduce-scatter**：做“求和 + 分发”，让第 k 张卡只拿到第 k 段的总和：
  [
  s^{(k)}=\sum_{r=1}^{p} g_r^{(k)}
  ]
  rank k 最终得到 (s^{(k)})，其他段不要。

* **all-gather**：把各卡手里的段拼回去，让每卡都有完整的：
  [
  g=\big[s^{(1)}, s^{(2)}, \dots, s^{(p)}\big]
  ]

所以：
[
\text{all-reduce} \equiv \text{reduce-scatter} + \text{all-gather}
]

### 3）为什么“大 batch / 长序列”更倾向用这种拆法（题 19 的 B、C）

这题的直觉是：当通信/计算量更大时，你更关心**吞吐**和**重叠**（overlap）。

* **消息很大时（更“带宽瓶颈”）**：reduce-scatter / all-gather 这种分块式通信更容易做 pipeline、更容易和计算重叠，工程上也更常见（尤其配合分片优化器/ZeRO/各种并行策略）。
* **消息很小时（短序列）**：主要是 **延迟（latency）** 占比高，拆成两个 collective（RS + AG）反而可能不划算。

为什么 D“低带宽”不选：低带宽下不代表拆分就更快，拆成两次 collective 还可能增加额外开销；真正决定是否拆分，更多是**消息规模/可重叠空间**，不是“带宽低就一定拆”。

> 你可以把它类比成：
> 小文件传输（短序列）→ 多一次握手都嫌慢；
> 大文件传输（长序列/大负载）→ 分块传、边传边处理更划算。

---

## （4）为什么不是 B？“无偏”不是定义 (E[\hat\theta]=z) 吗？

这里最关键的是区分两个词：

* **Estimator（估计量）**：(\hat\theta) 是随机变量（因为它由随机样本算出来）
* **Estimate（估计值）**：(z) 是你这一次抽样算出来的具体数

无偏的定义是：
[
E[\hat\theta] = \theta
]
也就是 **对“重复抽样的平均表现”** 没有系统偏差。

而 B 写的是：
[
E[\hat\theta]=z
]
这把“长期平均（期望）”错当成“某一次结果”。

### 一个极简例子

掷硬币估计 (p)：(\hat p=) 样本均值是无偏的（(E[\hat p]=p)）。
如果你这次 10 次里出现 7 次正面，得到 (z=0.7)。
你不能说 (E[\hat p]=0.7)。期望仍然是 (p)，0.7 只是这一次的 realization。

✅ 所以 D 对：无偏 ⇒ **不存在系统性误差**（在频率学派意义下）。

> 如果你硬要让“等于 z”成立，那得写 **条件期望**：在你已经观测到这组样本后，(\hat\theta) 变成常数 (z)，所以 (E[\hat\theta \mid \text{这组样本}]=z)。但题里写的是无条件的 (E[\hat\theta])。

---

你问得很到位：这两个点本质都是**“定义 + 量纲 + 分布来源”**。

---

## 1）为什么方差的单位是 (t^2)？

因为方差的定义就是“**偏离均值的平方的期望**”：

[
\mathrm{Var}(X)=\mathbb{E}\big[(X-\mathbb{E}[X])^2\big]
]

* 如果 (X) 的单位是 **时间 (t)**（秒、分钟……），那 ((X-\mathbb{E}[X])) 的单位还是 (t)
* 再平方一次 ((\cdot)^2)，单位就变成 (t^2)

所以：

* **均值** (\mathbb{E}[X])：单位是 (t)
* **方差** (\mathrm{Var}(X))：单位是 (t^2)
* **标准差** (\mathrm{SD}(X)=\sqrt{\mathrm{Var}(X)})：单位又回到 (t)

> 这也解释了为什么“标准差”更直观：它跟原变量同单位，而方差是平方单位。

应用到指数分布 (X\sim\mathrm{Exp}(\lambda))：
(\lambda) 是 rate，单位是 (1/t)，因此
[
\mathbb{E}[X]=\frac{1}{\lambda};;(\text{单位 }t),\qquad
\mathrm{Var}(X)=\frac{1}{\lambda^2};;(\text{单位 }t^2)
]
量纲完全一致。

---

## 2）泊松分布为什么 (\mathrm{Var}(X)=\lambda)？

先给结论：如果 (X\sim\mathrm{Pois}(\lambda))，那么
[
\mathbb{E}[X]=\lambda,\qquad \mathrm{Var}(X)=\lambda
]

### 直觉解释（最推荐背这个）

泊松描述的是：**很多“极小概率、近似独立”的事件次数**。
你可以把一段时间切成 (n) 个非常小的格子，每个格子里事件发生与否是 Bernoulli：

* 第 (i) 格：(I_i\in{0,1})，(\mathbb{P}(I_i=1)=p)（很小）
* 总次数：(X=\sum_{i=1}^n I_i)

对于 Bernoulli：
[
\mathbb{E}[I_i]=p,\qquad \mathrm{Var}(I_i)=p(1-p)\approx p\quad (p\ll 1)
]

独立相加时（关键性质）：
[
\mathbb{E}[X]=\sum \mathbb{E}[I_i]=np
]
[
\mathrm{Var}(X)=\sum \mathrm{Var}(I_i)\approx \sum p = np
]

当 (n\to\infty,;p\to 0) 且 (np=\lambda) 固定时，(X) 的极限就是泊松分布。于是得到：
[
\mathbb{E}[X]=\lambda,\qquad \mathrm{Var}(X)=\lambda
]

**一句话记忆：**
泊松 = “很多小概率独立事件的求和” → 单个 Bernoulli 有 (\mathrm{Var}\approx \mathbb{E}) → 加起来仍 (\mathrm{Var}\approx \mathbb{E}) → 所以泊松 (\mathrm{Var}=\mathbb{E}=\lambda)。

### 你可能担心的“单位”问题

* 泊松的 (X) 是“次数/计数”，通常当成**无量纲**（count），所以 (\lambda) 也是无量纲的“期望次数”。
* 如果你来自泊松过程：(\lambda = rT)，其中 (r) 是 rate（单位 (1/t)），(T) 是时间（单位 (t)），乘起来无量纲，仍是“期望次数”。

---

## 超好背的 3 个小结（你刷题直接用）

1. **方差单位 = 原单位平方**：因为 $\mathrm{Var}(X)=\mathbb{E}[(X-\mu)^2]$
2. **指数分布**：$\lambda$ 是 $1/t$ ⇒ $E=1/\lambda$（$t$） ⇒ $\mathrm{Var}=1/\lambda^2$（$t^2$）
3. **泊松分布**：$\mathbb{E}[X]=\mathrm{Var}(X)=\lambda$（稀有独立事件相加的极限）

---

## ✅ 错题整理（结构化）

> **整理日期**: 2026-01-04  
> **统计**: 20 题，10 错，正确率 50%

---

### 📊 知识点标注表

| 题号 | Tag1 | Tag2 | 错题 |
|------|------|------|------|
| Q1 | 概率统计 | - | ❌ |
| Q2 | 深度学习基础 | - | ✅ |
| Q3 | 经典机器学习 | - | ❌ |
| Q4 | 概率统计 | - | ❌ |
| Q5 | 经典机器学习 | - | ✅ |
| Q6 | 经典机器学习 | - | ✅ |
| Q7 | 经典机器学习 | - | ✅ |
| Q8 | 概率统计 | - | ❌ |
| Q9 | 深度学习基础 | - | ❌ |
| Q10 | 注意力与Transformer | 大模型推理 | ❌ |
| Q11 | 概率统计 | - | ✅ |
| Q12 | 优化与数值方法 | - | ✅ |
| Q13 | 注意力与Transformer | - | ✅ |
| Q14 | 深度学习基础 | - | ❌ |
| Q15 | 优化与数值方法 | - | ✅ |
| Q16 | 优化与数值方法 | - | ✅ |
| Q17 | 优化与数值方法 | - | ❌ |
| Q18 | 注意力与Transformer | - | ❌ |
| Q19 | 注意力与Transformer | 大模型分布式 | ❌ |
| Q20 | 线性代数 | 优化与数值方法 | ✅ |

---

### 📈 掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 概率统计 | 5 | 3 | 40% |
| 深度学习基础 | 3 | 2 | 33% |
| 经典机器学习 | 4 | 1 | 75% |
| 注意力与Transformer | 4 | 3 | 25% |
| 优化与数值方法 | 4 | 1 | 75% |
| 线性代数 | 1 | 0 | 100% |

---

### 🔥 薄弱环节 Top-3（2025-09-17）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 注意力与Transformer | 3/4 | 25% | Prefill/Decode、位置编码、all-reduce |
| 2 | 深度学习基础 | 2/3 | 33% | ReLU梯度、Loss Scaling |
| 3 | 概率统计 | 3/5 | 40% | MLE、无偏估计、指数分布 |

---

### 📚 核心知识点速查

#### 1. 概率统计

| 概念 | 核心公式 | 一眼记住 |
|------|----------|----------|
| 泊松分布 MLE | $\hat\lambda = \bar{X} = \frac{1}{n}\sum X_i$ | MLE = 样本均值 |
| 无偏估计 | $E[\hat\theta] = \theta$ | 期望等于真值，不是某次估计值 |
| 泊松分布 | $E[X] = \mathrm{Var}(X) = \lambda$ | 期望=方差（稀有独立事件求和极限） |
| 指数分布 | $E[X] = \frac{1}{\lambda}$，$\mathrm{Var}(X) = \frac{1}{\lambda^2}$ | 期望≠方差！用量纲记：$\lambda$ 是 rate ($1/t$) |

#### 2. 深度学习基础

| 概念 | 核心公式 | 一眼记住 |
|------|----------|----------|
| ReLU 梯度 | $\text{ReLU}'(x) = \begin{cases}1, & x>0\\0, & x<0\end{cases}$ | 正区间导数恒=1，不饱和，缓解梯度消失 |
| Loss Scaling | $L' = S \cdot L$，反传后除 $S$ | 放大梯度避免 FP16 下溢为 0 |

#### 3. 注意力与Transformer

| 概念 | 核心公式 | 一眼记住 |
|------|----------|----------|
| 正弦/余弦位置编码 | $\mathrm{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)$ | 绝对位置编码，不是相对！ |
| Prefill vs Decode | Prefill=计算瓶颈(Tensor Core吞吐)；Decode=带宽瓶颈(KV cache读写) | 长prompt看算力，长生成看HBM带宽 |
| all-reduce 分解 | all-reduce = reduce-scatter + all-gather | 大消息规模时拆分更高效（可重叠） |

#### 4. 优化与数值方法

| 概念 | 核心公式 | 一眼记住 |
|------|----------|----------|
| 舍入误差累积 | 每步 round 误差进入下一步运算 | 迭代多了小误差叠成大偏差 |

---

## Q1: 泊松分布 MLE

**原题**：
> 设总体 $X$ 服从参数为 $\lambda(\lambda>0)$ 的泊松分布，其概率分布为 $P(X=a)=\dfrac{\lambda^a e^{-\lambda}}{a!}$，从该总体中抽取容量为 $n$ 的独立随机样本 $X_1,\ldots,X_n$。则参数 $\lambda$ 的极大似然估计是（）。

**正确答案**：D. $\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i$

**易错点**：
混淆样本方差（选项A）与样本均值。泊松分布的MLE就是样本均值。

**核心公式/结论**：
$$
\hat\lambda_{\text{MLE}} = \bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i
$$

**一眼记住**：
泊松MLE = 样本均值（因为 $E[X]=\lambda$，矩估计和MLE一致）

---

## Q3: Stacking 集成学习

**原题**：
> 在集成学习的 Stacking 方法中，以下哪一项准确描述了第二层模型（Meta-Model）的作用？

**正确答案**：A. 将基模型的预测结果作为输入，学习它们的组合方式

**易错点**：
混淆 Stacking 与 Bagging/Boosting。Stacking 是"学组合"，不是简单平均。

**核心公式/结论**：
$$
\hat{y} = g\big(f_1(x), f_2(x), \dots, f_m(x)\big)
$$
元模型 $g$ 学习如何组合基模型预测。

**一眼记住**：
- Bagging = 并行 + 平均（降方差）
- Boosting = 串行 + 纠错（降偏差）
- Stacking = 学组合（元模型必须用 out-of-fold 预测）

---

## Q4: 无偏估计的定义

**原题**：
> 假设 $\theta$ 的一个无偏估计量是 $\hat\theta$，其在一组样本下的值为 $z$。对于下述陈述，正确的是（）。

**正确答案**：D. 估计量 $\hat\theta$ 不存在系统性误差

**易错点**：
误选 B（$E[\hat\theta]=z$）。$z$ 是某一次抽样的结果，不是期望！

**核心公式/结论**：
$$
E[\hat\theta] = \theta \quad (\text{不是 } z!)
$$

**一眼记住**：
- Estimator（估计量）= 随机变量 $\hat\theta$
- Estimate（估计值）= 具体数 $z$
- 无偏 = 长期平均无系统偏差，不是"等于某次结果"

---

## Q8: 指数分布的期望与方差

**原题**：
> 关于指数分布，下列哪项说法是错误的？
> B. 指数分布的期望和方差是相等的。

**正确答案**：B（错误的说法）

**易错点**：
混淆泊松和指数。泊松 $E=\mathrm{Var}=\lambda$，但指数 $E \neq \mathrm{Var}$。

**核心公式/结论**：
$$
X \sim \text{Exp}(\lambda) \Rightarrow E[X] = \frac{1}{\lambda}, \quad \mathrm{Var}(X) = \frac{1}{\lambda^2}
$$

**一眼记住**：
- 指数分布：$E \neq \mathrm{Var}$（期望是 $1/\lambda$，方差是 $1/\lambda^2$）
- 泊松分布：$E = \mathrm{Var} = \lambda$
- 用量纲记：$\lambda$ 是 rate ($1/t$)，期望单位 $t$，方差单位 $t^2$

---

## Q9: ReLU 正区间梯度不饱和

**原题**：
> 以下哪种激活函数可以有效地缓解梯度消失问题？

**正确答案**：C. ReLU

**易错点**：
不理解"饱和"的含义。Sigmoid/Tanh 在大正/大负时导数趋近0，梯度传不动。

**核心公式/结论**：
$$
\text{ReLU}'(x) = \begin{cases}1, & x>0\\0, & x<0\end{cases}
$$

**一眼记住**：
- ReLU 正区间导数恒=1，不管输入多大，梯度不会"消失"
- Sigmoid 两端导数→0 = "饱和" = 梯度消失

---

## Q10: Prefill/Decode 的瓶颈

**原题**：
> 请求A（长prompt短生成）和请求B（短prompt长生成），以下哪个判断正确？

**正确答案**：C. 请求A的耗时主要由GPU计算单元吞吐量决定；请求B的耗时主要由GPU显存带宽决定。

**易错点**：
选D（INT4量化最有效优化B）。量化减少权重大小，但KV cache不是权重，对decode帮助有限。

**核心公式/结论**：
- Prefill：大矩阵乘法 → **compute-bound**（算力瓶颈）
- Decode：每步读KV cache → **memory-bandwidth-bound**（带宽瓶颈）

**一眼记住**：
- 长prompt → Prefill → 看Tensor Core吞吐
- 长生成 → Decode → 看HBM带宽
- 优化Decode：MQA/GQA减少KV头数、KV cache压缩、speculative decoding

---

## Q14: Loss Scaling（损失缩放）

**原题**：
> 在混合精度训练中，引入"损失缩放"技术的主要原因是什么？

**正确答案**：B. 防止梯度在低精度（如FP16）表示下因数值过小而下溢为零。

**易错点**：
混淆"加速计算"与"防止下溢"。Loss Scaling 主要解决精度问题。

**核心公式/结论**：
$$
L' = S \cdot L \Rightarrow g' = S \cdot g \quad (\text{更新时除回 } S)
$$

**一眼记住**：
- FP16 最小数约 $10^{-8}$，更小的梯度下溢为0
- 放大Loss → 放大梯度 → 避免下溢 → 更新时除回来

---

## Q17: 舍入误差累积

**原题**：
> 误差来源中，截断误差和舍入误差的区别正确表述有哪些？
> D. 在迭代法中，通常情况下，截断误差随迭代减小，舍入误差随迭代累积

**正确答案**：B、C、D

**易错点**：
不理解"舍入误差累积"的机制。每步运算的小误差会进入下一步继续被使用。

**核心公式/结论**：
每步 round 误差 → 进入下一步 → 迭代多了叠加成大偏差

**一眼记住**：
- 截断误差：有限步近似无穷过程，迭代多了反而减小
- 舍入误差：有限精度表示，迭代多了累积放大

---

## Q18: 正弦/余弦位置编码

**原题**：
> A. 标准的正弦/余弦位置编码是专门为相对位置设计的编码。

**正确答案**：A 是错误的（应选B、D）

**易错点**：
误以为 sin/cos PE 是相对位置编码。它是绝对位置编码！

**核心公式/结论**：
$$
\mathrm{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$
输入是 $pos$（绝对位置），不是 $(pos_q - pos_k)$（相对位移）。

**一眼记住**：
- 标准 sin/cos PE = 绝对位置编码
- 相对位置编码：直接把距离作为 bias，如 $\text{score} + b(pos_q - pos_k)$
- sin/cos 有"可推相对位移"的性质，但不是专门为相对位置设计

---

## Q19: all-reduce 分解

**原题**：
> 在以下哪些场景下，all-reduce 更倾向于通过 reduce-scatter 和 all-gather 实现？

**正确答案**：B（大batch size）、C（长序列）

**易错点**：
不理解 all-reduce = reduce-scatter + all-gather 的分解方式。

**核心公式/结论**：
$$
\text{all-reduce} \equiv \text{reduce-scatter} + \text{all-gather}
$$

**一眼记住**：
- 消息大（长序列/大batch）→ 分块传更高效，可与计算重叠
- 消息小（短序列）→ 拆两次反而有额外开销
- 类比：大文件分块传输更划算，小文件多握手反而慢

---

<details>
<summary>📎 追问详情（原始解答）</summary>

（1）泊松分布的MLE推导、与其他分布的关系
（2）Stacking vs Bagging vs Boosting 的对比
（4）无偏估计的定义：Estimator vs Estimate
（8）指数分布 vs 泊松分布的期望方差
（9）ReLU "不饱和"的含义
（10）Prefill/Decode 的瓶颈分析
（14）Loss Scaling 原理与动态策略
（17）舍入误差累积的玩具例子
（18）绝对位置编码 vs 相对位置编码
（19）reduce-scatter + all-gather 分解

详细内容见本文档第307行起的原始解答。

</details>

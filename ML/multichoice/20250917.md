# 2025-09-17 AI岗题

## 一、单选题

### 1. 设总体 $X$ 服从参数为 $\lambda(\lambda>0)$ 的泊松分布，其概率分布为 $P(X=a)=\dfrac{\lambda^a e^{-\lambda}}{a!}$，$a=0,1,2,\ldots$。从该总体中抽取容量为 $n$ 的独立随机样本 $X_1,\ldots,X_n$。则参数 $\lambda$ 的极大似然估计是（）。

- A. $\displaystyle \frac{1}{n - 1}\sum_{i=1}^{n} (X_i - \overline{X}) ^ 2$（其中 $\overline{X}=\frac{1}{n}\displaystyle \sum_{i=1}^{n} X_i$）
- B. $\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i ^ 2$
- C. $\displaystyle \max(X_1,\ldots,X_n)$
- D. $\displaystyle \frac{1}{n}\displaystyle \sum_{i=1}^{n} X_i$

**答案：D**

$$
L(\lambda)\propto \lambda^{\sum X_i}e^{-n\lambda},\quad \hat\lambda=\frac{1}{n}\sum_{i=1}^{n}X_i=\overline{X}
$$

---

### 2. 在神经网络中，反向传播算法的主要作用是（）

- A. 随机初始化网络中的权重参数
- B. 增加网络的深度以提升模型表达能力
- C. 根据损失函数的梯度来更新网络参数
- D. 计算损失函数在训练数据上的平均值

**答案：C**

- 反向传播通过链式法则计算损失对各层参数的梯度，并据此更新参数。

---

### 3. 在集成学习的 Stacking 方法中，以下哪一项准确描述了第二层模型（Meta-Model）的作用？（）

- A. 将基模型的预测结果作为输入，学习它们的组合方式
- B. 对基模型的预测结果进行加权平均
- C. 对基模型的参数进行优化
- D. 对基模型的预测结果进行特征组合

**答案：A**

- 元模型以一级基模型预测作为新特征，学习最优组合方式提升泛化。

---

### 4. 假设 $\theta$ 的一个无偏估计量是 $\hat\theta$，其在一组样本下的值为 $z$。对于下述陈述，正确的是（）

- A. $\hat\theta$ 是 $\theta$ 的组合估计
- B. $E\hat\theta=z$
- C. $\theta=z$
- D. 估计量 $\hat\theta$ 不存在系统性误差

**答案：D**

$$
E[\hat\theta]=\theta
$$

---

### 5. 对于二元逻辑回归问题，已训练得到的参数为 $w=[0.5,-1.0],\ b=-1.0$。现在给输入样本 $x=[2,1]$，使用 Sigmoid 函数计算预测概率 $P(y=1\mid x)$，结果最接近的是以下哪个值？

- A. 0.27
- B. 0.43
- C. 0.56
- D. 0.60

**答案：A**

$$
z=w^\top x+b=0.5\times2-1.0\times1-1.0=-1,\quad \sigma(z)=\frac{1}{1+e^{-z}}=\sigma(-1)\approx0.2689
$$

---

### 6. 给定二维平面样本点：A(0,0)，B(1,1)，C(6,5)，D(7,6)。使用 K-means 算法（设聚类数 $k=2$，且用欧氏距离）进行聚类，初始聚类中心分别为 (2,2) 和 (8,8)。经过一次聚类迭代（即“分配–更新”各执行一次）后，新的中心坐标分别是（）。

- A. (0.0,0.0) (6.5,5.5)
- B. (0.5,0.5) (6.0,5.0)
- C. (0.5,0.5) (6.5,5.5)
- D. (3.5,3.5) (4.0,4.0)

**答案：C**

$$
\mu_1=\left(\frac{0+1}{2},\frac{0+1}{2}\right)=(0.5,0.5),\quad
\mu_2=\left(\frac{6+7}{2},\frac{5+6}{2}\right)=(6.5,5.5)
$$

---

### 7. 在机器学习中，关于监督学习、无监督学习、半监督学习和强化学习的下列说法中，哪一项是正确的？

- A. 半监督学习结合少最标记样本和大量未标记样本，以降低标注成本并提升性能。
- B. 强化学习通过从带标签的数据集中学习输入到输出的映射关系，以最小化预测误差。
- C. 无监督学习的目标是根据输入特征预测离散类别标签，常用于分类任务。
- D. 监督学习通过环境奖励信号训练智能体，常见于游戏和机器人控制。

**答案：A**

- 半监督学习：少量标注 + 大量未标注联合训练。

---

### 8. 指数分布常用于非负随机变量事件发生的时间间隔，例如用户下次访问网站的等待时间。关于指数分布，下列哪项说法是错误的？

- A. 如果一系列事件的发生次数服从泊松分布，那么这些事件之间的时间间隔服从指数分布。
- B. 指数分布的期望和方差是相等的。
- C. 指数分布的概率密度函数在定义域内是单调递减的。
- D. 指数分布具有“无记忆性”，即等待了多久不影响未来还需要等待多久的概率分布。

**答案：B**

$$
X\sim \text{Exp}(\lambda)\Rightarrow E[X]=\frac{1}{\lambda},\quad \mathrm{Var}(X)=\frac{1}{\lambda^2}
$$

---

### 9. 在神经网络中，由于模型层数增加，在计算梯度反向传播时容易造成数值表示过小，引发“梯度消失”问题，以下哪种激活函数可以有效地缓解梯度消失问题？

- A. Softmax
- B. Tanh
- C. ReLU
- D. Sigmoid

**答案：C**

- ReLU 在正区间梯度不饱和，能缓解梯度消失。

---

### 10. 一个 LLM 推理服务在处理用户请求时，将过程分为 Prefill（处理提示）和 Decode（生成回复）两个阶段。现在有两个典型的请求：请求A（代码生成）：提示(Prompt)很长，包含大量上下文代码(2000 tokens)，但期望生成的代码片段较短(100 tokens)。请求B(多轮对话)：提示很短，只是用户的最新一句话(20 tokens)，但需要生成一段较长的回复(500 tokens)，且此时对话历史已有1500 tokens。基于对现代 GPU 架构和 Transformer 推理机制的理解，以下哪个判断是正确的？

- A. 请求A的耗时瓶颈主要在于 Decode 阶段，因为需要处理复杂的代码逻辑。
- B. 请求B的耗时瓶颈主要在于 Prefill 阶段，因为它需要快速响应用户的输入。
- C. 请求A的耗时主要由 GPU 的计算单元（如 Tensor Core）的吞吐量决定；请求B的耗时主要由 GPU 显存（HBM）的带宽决定。
- D. 为了优化请求B的总体延迟，最有效的措施是采用先进的模型量化技术（如 INT4），因为这能大幅减少每次 Decode 步骤中的矩阵乘法计算量。

**答案：C**

- Prefill 常为大矩阵乘法（偏计算瓶颈）；Decode 常受 KV cache 读写（偏带宽瓶颈）。

---

### 11. 下列方法中没有考虑先验分布的是（）

- A. 贝叶斯分类器
- B. 最大后验估计
- C. 最大似然估计
- D. 贝叶斯学习

**答案：C**

- 最大似然估计只依赖似然函数，不显式使用先验分布。

---

### 12. 为减少数值计算中的舍入误差，以下哪种方法是有效的？

- A. 增加算法的迭代次数
- B. 简化数学模型
- C. 增大数值方法的步长
- D. 使用更高精度的浮点数表示

**答案：D**

- 提高浮点数精度可直接减少舍入误差。

---

### 13. 在大型语言模型(LLM)进行自回归文本生成（例如，根据提示生成后续文本）的过程中，关于每一步生成新token时的计算，以下哪项描述是正确的？

- A. 生成每个新token时，只需要计算当前token对应的Query向量，而不需要重新计算Key和Value。
- B. 模型在生成第一个token后，其后续所有token的生成都可以完全并行计算，无需等待。
- C. 通常使用“KV缓存”(Key-Value Cache)技术来存储已计算的注意力Key和Value向量，避免在生成后续token时重复计算历史信息。
- D. 每生成一个新token，模型都需要重新计算之前所有已生成token的隐蔽状态（hidden states）。

**答案：C**

- 使用 KV cache 存历史 Key/Value，避免重复计算历史信息。

---

### 14. 在使用混合精度训练（如 FP16/BF16 与 FP32 结合）训练大型语言模型时，引入“损失缩放”（Loss Scaling）技术的主要原因是什么？

- A. 加速前向传播计算中矩阵乘法的执行速度。
- B. 防止梯度在低精度（如 FP16）表示下因数值过小而下溢（underflow）为零。
- C. 降低模型权重更新时的数值噪声，提高训练稳定性。
- D. 减少反向传播过程中激活值（activations）的内存占用。

**答案：B**

- 通过放大 loss/梯度避免 FP16 下梯度下溢为 0。

---

### 15. 已知准确值 $a=100$ 的一个近似值 $a^*=50$，该近似值的相对误差为

- A. -0.5
- B. 1
- C. 50
- D. -50

**答案：A**

$$
e_r=\frac{a^*-a}{a}=\frac{50-100}{100}=-0.5
$$

---

## 二、多选题

### 16. 在数值计算中，以下关于误差的叙述中，正确的有

- A. 相对误差较小时，绝对误差一定也很小
- B. 对误差定义为近似值减去真实值的绝对值
- C. 截断误差是由于使用无限过程近似有限步骤计算引起的
- D. 有效数字位数越多，计算结果的精度越高

**答案：B、C、D**

- ❌ A：相对误差小并不一定意味着绝对误差小。
- ✅ B：误差常定义为近似值与真实值之差的绝对值。
- ✅ C：截断误差来自对无穷过程的有限截断近似。
- ✅ D：有效数字位数越多通常精度越高。

---

### 17. 误差来源中，截断误差和舍入误差的区别正确表述有哪些?

- A. 两者均可完全消除
- B. 舍入误差源于有限精度表示
- C. 截断误差源于有限步近似无穷过程(如泰勒级数截断)
- D. 在迭代法中，通常情况下，截断误差随迭代减小，舍入误差随迭代累积

**答案：B、C、D**

- ❌ A：两者不可能完全消除。
- ✅ B：舍入误差来自有限精度表示。
- ✅ C：截断误差来自有限步近似无穷过程（如泰勒级数截断）。
- ✅ D：迭代中截断误差通常减小，舍入误差可能累积。

---

### 18. 深入分析 Transformer 架构的内部组件，会发现其设计充满了精妙的权衡。以下关于其核心组件的描述，哪些是准确的？

- A. 标准的正弦/余弦位置编码（Sinusoidal Positional Encoding）是专门为相对位置设计的编码。
- B. 多头注意力机制的核心思想是将 Query、Key 和 Value 投影到多个不同的低维表示子空间中，让每个“头”有机会学习到不同类型的依赖关系。
- C. 前馈网络（FFN）子层通过为序列中的每一个位置学习一套独立的权重参数，来增强模型对位置信息的建模能力。
- D. RMSNorm 作为一种比 LayerNorm 更简洁的归一化技术，其关键改进在于省略了对输入向量减去均值的中心化步骤，只进行缩放操作。

**答案：B、D**

- ❌ A：标准正弦/余弦位置编码是绝对位置编码。
- ✅ B：多头注意力将 Q/K/V 投影到多个子空间学习不同依赖。
- ❌ C：FFN 参数在序列各位置共享。
- ✅ D：RMSNorm 省略减均值，仅按均方根缩放并线性缩放。

---

### 19. 在大语言模型分布式并行训练中，all-reduce 是一个常用的集合通信算子。在以下哪些场景下，all-reduce 更倾向于通过 reduce-scatter 和 all-gather 实现？

- A. 短序列
- B. 大 batch size
- C. 长序列
- D. 低带宽

**答案：B、C**

- ❌ A：短序列通信规模小，额外拆分收益有限。
- ✅ B：大 batch size 通信规模大，更适合分解实现以提升效率。
- ✅ C：长序列通信规模大，更适合分解实现以提升效率。
- ❌ D：低带宽下拆分并不一定更优，可能被额外开销抵消。

---

### 20. 已知三个数据点 (1,3)、(2,6)、(3,11)，用二次多项式 $P(x)=ax^2+bx+c$ 进行拟合（要求拟合多项式过前两个点，且与第三个点的误差平方和最小），则下列说法正确的是（）

- A. 系数 $a=1$
- B. 系数 $b=0$
- C. 拟合多项式与第三个点的误差为 0
- D. 系数 $c=2$

**答案：A、B、C、D**

$$
a=1,\ b=0,\ c=2,\quad P(x)=x^2+2,\quad P(3)=11
$$
















下面按你问的编号来讲（尽量把“直觉 + 公式 + 怎么记”都说清楚）。

---

## （1）“$\lambda$ 的 MLE 是什么？不同 distribution 有关系吗？”

先澄清：你题里是 **泊松分布** $X_i\sim \text{Pois}(\lambda)$，问的是 **$\lambda$ 的极大似然估计（MLE）**。

### MLE 的通用定义

给定 i.i.d. 样本 $x_1,\dots,x_n$，MLE 就是
[
\hat\theta=\arg\max_\theta \prod_{i=1}^n p(x_i\mid \theta)
]
实际做法几乎总是取对数（把乘积变求和）：
[
\ell(\theta)=\sum_{i=1}^n \log p(x_i\mid\theta)
]

### 泊松的 MLE（题1）

泊松 pmf：
[
p(x\mid\lambda)=\frac{\lambda^x e^{-\lambda}}{x!}
]
对数似然：
[
\ell(\lambda)=\sum_{i=1}^n \big(x_i\log\lambda-\lambda-\log(x_i!)\big)
]
求导：
[
\frac{\partial \ell}{\partial \lambda}=\sum_{i=1}^n\left(\frac{x_i}{\lambda}-1\right)=\frac{1}{\lambda}\sum x_i-n
]
令其为 0：
[
\hat\lambda=\frac{1}{n}\sum_{i=1}^n x_i=\overline{x}
]
**结论：泊松参数的 MLE 就是样本均值。**

> 你写的 “var(lambda) 的 MLE” 如果你是想问“$\hat\lambda$ 的方差”，那泊松下也很好算：因为 $\overline X$ 的方差是
> [
> \mathrm{Var}(\hat\lambda)=\mathrm{Var}(\overline X)=\frac{\mathrm{Var}(X)}{n}=\frac{\lambda}{n}
> ]
> （用到泊松 $\mathrm{Var}(X)=\lambda$。）

### 不同 distribution 有关系吗？

**有关系但不是“一个公式通吃”。** MLE 取决于分布的 $p(x\mid\theta)$。

但很多常见分布（指数族）会出现很“规律”的 MLE，比如常常等于某个“样本统计量”（均值、方差等）。几个你可以背的典型：

* Bernoulli($p$)：$\hat p=\overline x$
* Poisson($\lambda$)：$\hat\lambda=\overline x$
* Exponential(rate $\lambda$)：$\hat\lambda=\dfrac{1}{\overline x}$
* Normal($\mu,\sigma^2$)：$\hat\mu=\overline x$，且 $\widehat{\sigma^2}_{\text{MLE}}=\dfrac{1}{n}\sum (x_i-\overline x)^2$（注意不是 $1/(n-1)$）

---

## （2）Stacking 是什么？和 Bagging / Boosting 平行吗？

**是平行的“集成学习（ensemble）”大类里的三种主要套路**，但机制不同。

### Bagging（并行、降方差）

* 核心：对数据做 bootstrap 重采样 → 训练很多基模型 → **平均/投票**
* 直觉：把“抖动很大（高方差）”的模型平均掉
* 代表：Random Forest（= bagging + 特征子采样）

### Boosting（串行、降偏差）

* 核心：模型一个接一个训练，每个新模型更关注前面错的样本（或拟合残差）
* 直觉：逐步“补短板”，把弱学习器叠成强学习器
* 代表：AdaBoost / GBDT / XGBoost / LightGBM

### Stacking（学一个“组合器”）

* 核心：先训练多个基模型 $f_1,\dots,f_m$，再训练一个“元模型” $g$ 来学怎么组合它们：
  [
  \hat y = g\big(f_1(x),f_2(x),\dots,f_m(x)\big)
  ]
* 关键点：**元模型训练必须用 out-of-fold 预测**，否则会“泄漏”（基模型在训练集上过拟合，元模型被假象骗了）。
* 直觉：不是“平均”，而是“让另一个模型学会：谁在什么区域更靠谱”。

一句话对比：

* Bagging：**平均**（稳）
* Boosting：**逐步纠错**（强）
* Stacking：**学组合规则**（灵活、异构模型也行）

---

## （8）哪个分布满足“期望=方差”？怎么记指数分布的均值/方差？

题8里说“指数分布期望和方差相等”是错的。

### 期望 = 方差 的经典分布

**泊松分布：** 若 $X\sim\text{Pois}(\lambda)$，
[
E[X]=\lambda,\quad \mathrm{Var}(X)=\lambda
]

### 指数分布怎么记

若 $X\sim\text{Exp}(\lambda)$（这里 $\lambda$ 是 **rate**，单位是 $1/\text{time}$）：
[
E[X]=\frac{1}{\lambda},\quad \mathrm{Var}(X)=\frac{1}{\lambda^2},\quad \mathrm{SD}(X)=\frac{1}{\lambda}
]

**最稳的记忆法：量纲（单位）分析**

* $\lambda$ 是“到达率/发生率”：每单位时间发生多少次，单位 $1/t$
* “等待时间”的平均值当然是“率的倒数”：$t = 1/\lambda$
* 方差单位是 $t^2$，所以必须是 $1/\lambda^2$

**更硬核的一个公式（顺手记）**
[
E[X^k]=\frac{k!}{\lambda^k}
]
所以 $E[X]=1/\lambda$，$E[X^2]=2/\lambda^2$，于是
[
\mathrm{Var}(X)=E[X^2]-E[X]^2=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}
]

---

## （9）“ReLU 正区间梯度不饱和”是什么意思？

先写 ReLU：
[
\text{ReLU}(x)=\max(0,x)
]
导数（反传用的“梯度”）：
[
\text{ReLU}'(x)=
\begin{cases}
1, & x>0\
0, & x<0
\end{cases}
]

### “饱和”是什么意思？

像 Sigmoid：
[
\sigma(x)=\frac{1}{1+e^{-x}}
]
当 $x\to +\infty$，$\sigma(x)\to 1$，导数 $\sigma'(x)=\sigma(x)(1-\sigma(x))\to 0$
当 $x\to -\infty$，$\sigma(x)\to 0$，导数也 $\to 0$

这就叫**在大正/大负时“饱和”**：输出贴着 0 或 1，导数几乎 0，反向传播的梯度传不动 → **梯度消失**。

### ReLU 为什么好

ReLU 在 $x>0$ 时导数恒为 1，**不会因为数值变大就导数变 0**，所以说“正区间不饱和”，梯度更容易往前传。

> 小提醒：ReLU 在 $x<0$ 导数是 0，会有 “dead ReLU”（永远不激活）。工程上常用 LeakyReLU / GELU 缓解。

---

## （10）Prefill / Decode 为什么 C 对、D 不对？

你卡住的是这句：
**A（长 prompt 短生成）主要看算力吞吐；B（短 prompt 长生成）主要看显存带宽。**

### 先把两个阶段想成两种“工作形态”

* **Prefill**：一次性把 prompt 的很多 token（例如 2000）同时过模型
  → 大矩阵乘法（GEMM）很“饱满”，GPU Tensor Core 吃得很满
  → 往往是 **compute-bound（算力瓶颈）**

* **Decode**：每次只生成 1 个新 token，要连续生成 500 次
  每一步都要做 attention：当前 token 的 Query 要去和“历史所有 token 的 K/V（KV cache）”做交互
  → 每一步都要从 HBM 里把很长的 KV cache 读出来（层数多、头多、序列长就更大）
  → 小算子 + 大量内存读写，容易变 **memory-bandwidth-bound（显存带宽瓶颈）**

所以 C 的直觉是：

* 请求 A：prefill 吃算力（矩阵乘法吞吐）
* 请求 B：decode 每步读 KV cache 吃带宽

### 为什么 D（INT4 量化）不是“最有效”

权重量化（INT4）主要减少：

* 权重显存占用
* 部分矩阵乘法算力开销

但对 B 这种 **decode 被 KV cache 带宽卡住** 的情况：

* **KV cache 不是“权重”**，通常权重量化不直接缩小 KV cache 的读带宽
* 生成 500 token 仍然是 500 步串行（量化也不能消除串行依赖）

对 B 更对症的优化反而是（给你建立正确直觉）：

* **减少 KV cache 体积**：比如 MQA/GQA（减少 KV 头数）
* **KV cache 量化/压缩**（专门针对 KV）
* **speculative decoding**（用小模型草拟、多 token 并行验算）
* 更高效的 attention kernel / paged KV 等

---

## （14）损失缩放（Loss Scaling）举个最简单例子

核心问题：**FP16 能表示的最小正数有限，梯度太小会下溢成 0。**

### 一个玩具例子（下溢）

假设某层真实梯度是
[
g=10^{-9}
]
FP16 最小的可表示数量级大概在 $10^{-8}$ 左右（更准确说最小 subnormal 约 $6\times 10^{-8}$），所以存进去就可能变成
[
g_{\text{fp16}}\approx 0
]
一旦梯度成 0，这一层就**不更新**了。

### Loss Scaling 怎么救

我们把 loss 乘一个缩放因子 $S$（比如 $S=1024$）：
[
L' = S\cdot L
]
因为反传梯度对 loss 是线性的：
[
\nabla_\theta L' = S\cdot \nabla_\theta L = Sg
]
于是梯度变成：
[
g' = 1024 \times 10^{-9} = 1.024\times 10^{-6}
]
这就能被 FP16 表示，不会下溢为 0。

**更新参数之前再“除回来”**：
[
g = \frac{g'}{S}
]
所以参数更新量不变，但数值计算过程中避免了下溢。

### 动态 Loss Scaling（你经常会看到）

如果 $S$ 太大，会导致梯度溢出成 inf/NaN；动态策略就是：

* 如果检测到溢出 → 降低 $S$
* 一段时间没溢出 → 尝试增大 $S$

---

如果你愿意，我可以把你这套题（尤其 10、13、14、19）做成一个“**一页纸速记**”：每题一个“关键直觉 + 1 行公式 + 1 个踩坑点”，方便你二刷直接背。你想要偏**应试**（记忆点多）还是偏**理解**（推导多）？

【2025年12月3日-AI方向】

---

## 一、单选题

### 1️⃣ 迁移学习（Transfer Learning）的适用场景是？

A. 数据量充足但任务与预训练模型任务无关
B. 数据量不足但任务与预训练模型任务相关
C. 数据量不足且任务与预训练模型任务无关
D. 数据量充足且任务与预训练模型任务高度相关

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 迁移学习：复用预训练参数/表征 (\theta_{\text{pre}})，在目标任务上微调
* 目标数据少且任务相关 ⇒ 降低样本复杂度、提升泛化

---

### 2️⃣ 在推理（Inference）阶段相对于训练阶段，通常会被关闭以提高效率的操作是？

A. Layer Normalization
B. Tokenization
C. Dropout
D. 残差连接

【单选】

答案：C

精简解释（≤3 行，偏数学/原理）：

* Dropout 训练期：(h' = m\odot h,; m\sim\text{Bernoulli}(p))
* 推理期关闭以保证确定性与稳定输出（不再随机失活）

---

### 3️⃣ 假设一个深度学习模型的权重初始化自 (N(0, 0.01^2))。随机抽取一个权重，其值落在 ([-0.02, 0.02]) 之间的概率大约是多少？（提示：正态分布 3σ 法则）

A. 68%
B. 95%
C. 34%
D. 99.7%

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* ([-0.02,0.02]=\pm 2\sigma)（(\sigma=0.01)）
* 正态经验法则：(|Z|\le 2 \Rightarrow \approx 95%)

---

### 4️⃣ DPM-Solver 在扩散模型中的主要优势是：

A. 增加模型参数
B. 降低采样步数
C. 提高训练速度
D. 提高模型鲁棒性

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 扩散采样可视作求解 ODE/SDE；DPM-Solver 用高阶数值法逼近
* 更少步数达到相近质量 ⇒ 推理采样更快

---

### 5️⃣ Multi-Head Attention 的输出如何计算？

A. 各头结果拼接后线性投影
B. 各头注意力结果取平均
C. 通过门控机制加权合并
D. 仅保留最大相似度的头

【单选】

答案：A

精简解释（≤3 行，偏数学/原理）：

* (\text{head}_i=\text{softmax}(QK^T/\sqrt{d_k})V)（每头）
* (\text{MHA}=\text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O)

---

### 6️⃣ Jacobi 迭代法和 Gauss–Seidel 迭代法的核心区别在于：

A. Gauss–Seidel 需要矩阵对称
B. Gauss–Seidel 每次迭代使用最新计算的分量
C. Jacobi 收敛更快
D. Jacobi 只能用于对角占优矩阵

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* Jacobi：本轮更新全用上一轮旧值；GS：更新即使用最新值
* 因此 GS 往往更快收敛（但条件不同）

---

### 7️⃣ 在 K-Means 聚类中，若目标函数

[
J=\sum_{k=1}^K \sum_{x\in C_k}|x-\mu_k|^2
]
的值不再变化，以下哪种情况必然成立？

A. 所有样本被分配到最优簇
B. 所有簇中心点已收敛
C. 簇内样本方差为零
D. 簇间距离达到最大

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* K-Means 交替最小化 (J)：更新分配 (C_k) 与中心 (\mu_k)
* (J) 不再变化 ⇒ 分配/中心不再改变 ⇒ (\mu_k) 收敛

---

### 8️⃣ 信息增益 IG 与熵 H 的关系：

A. 两者无关
B. (IG=H(X|Y)-H(X))
C. (IG=H(X,Y))
D. (IG=H(Y)-H(Y|X))

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* 信息增益（互信息）：(IG=I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y))
* 选项 D 符合定义

---

### 9️⃣ 关于向量点积的性质，以下说法正确的是？

A. (|a\cdot b|\le |a|+|b|) 是柯西-施瓦茨不等式的表述
B. (a\cdot (b+c)=a\cdot b+a\cdot c)
C. (a\cdot b=b\cdot a) 仅在正交基下成立
D. 零向量与任意向量的点积恒为 1

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 点积满足线性/分配律：(a\cdot(b+c)=a\cdot b+a\cdot c)
* 柯西-施瓦茨应为：(|a\cdot b|\le |a|,|b|)

---

### 1️⃣0️⃣ 在梯度下降中，学习率 (\alpha) 与梯度 (\Delta L) 是如何更新权重 (\omega) 的？

A. (\omega=\omega/\alpha\Delta L)
B. (\omega=\omega-\alpha\Delta L)
C. (\omega=\omega\times\alpha\Delta L)
D. (\omega=\omega+\alpha\Delta L)

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* 最小化 (L(\omega))：(\omega_{t+1}=\omega_t-\alpha\nabla_\omega L)
* 负梯度方向是局部最速下降方向

---

### 1️⃣1️⃣ 下列哪个算法属于监督学习？

A. 线性回归
B. 主成分分析（PCA）
C. K-means 聚类

【单选】

答案：A

精简解释（≤3 行，偏数学/原理）：

* 监督学习：使用标签 (y) 学习映射 (x\mapsto y)（如线性回归 (y\approx Xw)）
* PCA/K-means 不依赖标签，属无监督

---

### 1️⃣2️⃣ 在垃圾邮件分类中，贝叶斯定理可用于：

A. 计算邮件中包含特定关键词的联合概率
B. 根据关键词出现频率更新邮件为垃圾的概率
C. 生成随机噪声以混淆分类结果
D. 直接判定邮件的发送者身份

【单选】

答案：B

精简解释（≤3 行，偏数学/原理）：

* (;P(\text{spam}\mid w)\propto P(w\mid \text{spam})P(\text{spam}))
* 由关键词证据更新后验概率（朴素贝叶斯）

---

### 1️⃣3️⃣ 在逻辑回归中，若正则化参数 (\lambda) 过大，可能导致以下哪种情况？

A. 训练集好，测试集差
B. 训练集差，测试集好
C. 训练集和测试集均表现良好
D. 训练集和测试集上均表现差

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* 正则化：(\min_w L(w)+\lambda|w|^2)（或 (|w|_1)）
* (\lambda) 过大 ⇒ 权重被强压缩 ⇒ 欠拟合 ⇒ 训练/测试都差

---

### 1️⃣4️⃣ 奇异值分解（SVD）对于给定矩阵 (A)，其形式为？

A. (A=QR)
B. (A=Q\Lambda Q^T)
C. (A=LU)
D. (A=U\Sigma V^T)

【单选】

答案：D

精简解释（≤3 行，偏数学/原理）：

* SVD：(A=U\Sigma V^T)，其中 (U,V) 正交，(\Sigma) 为非负对角奇异值
* 是降维/压缩/协同过滤等基础分解

---

### 1️⃣5️⃣ 以下哪项是大模型推理阶段优化的主要目标？

A. 增加模型的参数量
B. 提高模型的训练数据多样性
C. 降低推理时的显存占用和计算延迟
D. 提高模型训练速度

【单选】

答案：C

精简解释（≤3 行，偏数学/原理）：

* 推理优化关注部署成本：显存（权重+KV cache）与延迟/吞吐
* 常见手段：量化、KV cache 优化、并行/融合算子等

---

## 二、多选题

### 1️⃣6️⃣ 极大似然估计法中，似然函数的性质包括：

A. 似然函数在最大值处生成已知观测的可能性最大
B. 似然函数是关于参数的凸函数
C. 似然函数是关于参数的连续函数
D. 似然函数在最小值处生成已知观测的可能性最大

【多选】

答案：A、C

精简解释（≤3 行，偏数学/原理）：

* MLE：(\hat\theta=\arg\max_\theta L(\theta)=\arg\max_\theta p(X|\theta))
* (L(\theta)) 一般可视作关于 (\theta) 的连续函数；不保证凸

---

### 1️⃣7️⃣ 关于迭代法收敛性，正确的说法是？

A. 对于对称正定矩阵，Jacobi 方法收敛
B. 对于对称正定矩阵，Gauss–Seidel 方法收敛
C. 若 (A) 严格对角占优，则 Jacobi 和 Gauss–Seidel 迭代均收敛
D. 若 (A) 是不可分弱严格对角占优矩阵，则 SOR 方法收敛
E. 对于对称正定矩阵，若松弛因子 (0<\omega<2)，则 SOR 方法收敛
F. (A) 是对角线非零的实 H 矩阵，则 SOR 迭代法收敛

【多选】

答案：B、C、E

精简解释（≤3 行，偏数学/原理）：

* 对称正定 (A)：GS 收敛；SOR 在 (0<\omega<2) 时收敛
* 严格对角占优：Jacobi 与 GS 都收敛

---

### 1️⃣8️⃣ 关于伯努利分布（0-1 分布）(X\sim \text{Bernoulli}(p))，以下说法正确的是：

A. 伯努利分布是二项分布的特例（(n=1)）
B. 伯努利分布可以描述多次重复实验的结果
C. (P(X=1)=1-P(X=0))
D. 伯努利分布的期望 (E(X)=p)，方差 (D(X)=p(1-p))

【多选】

答案：A、C、D

精简解释（≤3 行，偏数学/原理）：

* (\text{Bernoulli}(p)\equiv \text{Binomial}(n=1,p))
* (E[X]=p,;\text{Var}(X)=p(1-p))，且 (P(1)=1-P(0))

---

### 1️⃣9️⃣ LoRA 的优点包括：

A. 支持快速任务切换
B. 显著减少微调参数量
C. 保留原模型权重不变
D. 降低推理时显存占用

【多选】

答案：A、B、C

精简解释（≤3 行，偏数学/原理）：

* LoRA：冻结 (W)，仅学低秩增量 (\Delta W=BA)（rank (r\ll d)）
* 微调参数量大幅减少；切换不同 adapter 即快速切任务

---

### 2️⃣0️⃣ 关于常见损失函数及其应用场景的描述，哪些是正确的？

A. 交叉熵损失广泛用于分类任务，能衡量预测分布与真实标签差异
B. MSE 通常用于回归，但在某些情况下也可用于分类/概率回归
C. Hinge Loss 主要用于 SVM，也可扩展到多分类的 margin 学习
D. BLEU、ROUGE 等直接作为训练阶段的优化目标
E. Sigmoid + BCE 可自动解决类别不平衡，无需额外调整

【多选】

答案：A、B、C

精简解释（≤3 行，偏数学/原理）：

* 交叉熵：(\mathcal L=-\sum_i y_i\log \hat y_i)（分类标准配置）
* BLEU/ROUGE多为评测指标且不可微；不平衡通常仍需加权/采样

---

> 📝 **待整理**: 错题整理（结构化）章节待补充

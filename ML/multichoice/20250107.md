# 2025-01-07 AI 方向题

## 一、单选题

### 1. 若二次型

$$f(x_1,x_2,x_3)=5x{_1^2}+5x{_2^2}+cx{_3^2}-2x_1x_2+6x_1x_3-6x_1x_3$$

的秩为 2 2 2 ,则 c=()

- A. 3
- B. 5
- C. 9
- D. 6

**答案：A**

$$|A|=24(c-3)=0\Rightarrow c=3$$

---

### 2. 在处理用户评论情感分类任务时，你发现数据集里正面评论和负面评论的比例是9:1，这是一个典型的数据不平衡问题。在这种情况下，以下哪个评估指标最不能客观地反映模型的性能?

- A. 召回率(Recall)
- B. AUC(ROC曲线下面积)
- C. F1-score
- D. 准确率(Accuracy)

**答案：D**

类别不平衡时，全预测为正类也可能 Accuracy 很高，无法客观反映真实性能。

---

### 3. 设 A A A 为 3 3 3 阶矩阵，且 ∣A∣=2，则

$$\left|(\tfrac{1}{3}A)^{-1}-\tfrac{1}{2}A^*\right|=()$$

注：A\* 表示 A A A 的伴随矩阵

- A. 1
- B. $\frac{125}{16}$
- C. 5
- D. $-\frac{4}{27}$

**答案：D**

页面解析结论：$|A^*|=|A|^{n-1}=4$，最终得到 $-\frac{4}{27}$。

---

### 4. 设 A 是 3 阶方阵，将 A 的第 1 列与第 2 列交换得到 B，再把 B 的第 2 列加到第 3 列得到 C，则满足 AQ=C 的可逆矩阵 Q 为

- A. $$\begin{pmatrix}0&1&1\\1&0&0\\0&0&1\end{pmatrix}$$
- B. $$\begin{pmatrix}0&1&0\\1&0&1\\0&0&1\end{pmatrix}$$
- C. $$\begin{pmatrix}0&1&0\\1&0&0\\1&0&1\end{pmatrix}$$
- D. $$\begin{pmatrix}0&1&0\\1&0&0\\0&1&1\end{pmatrix}$$

**答案：A**

列变换等价于右乘初等矩阵；两步合并得到 Q。

---

### 5. 大模型训练中的"LoRA(Low-Rank Adaptation)“技术主要作用是

- A. 解决模型推理时的幻觉问题
- B. 在微测阶段通过低秩矩阵减少要更新的参数量
- C. 提升模型的上下文窗口长度
- D. 降低预渊练阶段的计算成本

**答案：B**

冻结原权重，仅训练低秩增量矩阵，减少可训练参数量。

---

### 6. 向量 A=[1,2,3,0,−3] 的 L1 范数为?

- A. 3
- B. $\sqrt{23}$
- C. 4
- D. 9

**答案：D**

$$\|A\|_1=|1|+|2|+|3|+|0|+|-3|=9$$

---

### 7. 如果一个模型出现了过拟合，我们通过以下哪些操作和优化能够缓解或者减少这种过拟合可能带来的问题?

- A. 在原有数据集上重新训练
- B. 添加正则化
- C. 将数据集复制后再训练
- D. 减少数据集后再训练

**答案：B**

加正则（L1/L2/Dropout 等）约束复杂度，提升泛化。

---

### 8. Transformer 位置编码(Position Encoding)主要作用:

- A. 增强非线性
- B. 提供序列位置信息
- C. 提升并行带宽
- D. 降低模型计算量

**答案：B**

自注意力不含顺序信息，位置编码注入位置信息。

---

### 9. 在机器学习中，将一张 28 × 28 像素的灰度图像转换为特征向量时，要求特征向量能完整保存原有信息的操作是?

- A. 仅保留边缘检测后的像素值
- B. 计算每个 4×4 块的平均值，生成 49 维向量
- C. 将像素矩阵按行展开为784维向量
- D. 直接使用原始像素矩阵作为输入

**答案：C**

按行展开是信息无损变换（28×28→784）。

---

### 10. 在参数高效微调(PEFT)方法中，LoRA 通过在 Transformer 线性层插入可训练的低秩矩阵来近似权重更新。下面哪一项最能解释为什么 LoRA 对推理时延几平没有负面影响?

- A. LORA 会把权重量化为 INT4，从而加速推理
- B. 低秩更新的秩r很小，且与原矩阵乘法可并行融合
- C. LORA 只在训练阶段发挥作用，推理阶段被完全丢弃
- D. 低秩矩阵的乘法可以离线编译到权重中

**答案：D**

推理前可合并 $\Delta W=BA$ 到原权重，推理路径不变。

---

### 11. 某12层Transformer，hidden size=768，MLP 扩展比=4，则每层 MLP 的参数量约为

- A. 18.88 M
- B. 9.44 M
- C. 4.72 M
- D. 2.36 M

**答案：C**

$$768\times3072+3072\times768\approx 2\times768\times3072=4{,}718{,}592\approx4.72\text{M}$$

---

### 12. 设 A∈R^{2×2} 的特征值为 λ1=3, λ2=−1，则 A 的迹为:

- A. 3
- B. 4
- C. -1
- D. 2

**答案：D**

$$\mathrm{tr}(A)=\lambda_1+\lambda_2=3+(-1)=2$$

---

### 13. 用梯形法则计算定积分时，若被积函数的二阶导数在区间 [a,b]上的最大值为M，积分区间等分为 n 段，则截断误差的上界为以下哪一项?

- A. $\frac{M(b-a)^2}{2n^2}$
- B. $\frac{M(b-a)^3}{12n^2}$
- C. $\frac{M(b-a)}{n}$
- D. $\frac{M(b-a)^4}{24n^3}$

**答案：B**

$$|E_T|\le \frac{M(b-a)^3}{12n^2}$$

---

### 14. 在一个基于Transformer的多模态模型中，视觉-文本对齐模块的目标是将图像特征 V∈R^{n×d_v} 和文本特征 T∈R^{m×d_t} 映射到同一语义空间若采用对比学习(Contrastive Learning)框架，则损失函数可选下列哪个?

- A. $$\log \frac{\exp(\sin(V_{1},T_{j}))}{\sum_{k\neq j}\exp(\sin(V_{i},T_{k}))}$$
- B. CrossEntropy($V_i,T_j$)
- C. KL-Divergence($V_i\|T_j$)
- D. MSE($V_i,T_j$)

**答案：A**

对比学习常用 InfoNCE/softmax 对比损失（基于相似度的 log-softmax）。

---

### 15. 二分法(Bisection Method)求解方程 时，其收敛速度是()

- A. 超线性收敛
- B. 不收敛
- C. 二次收敛
- D. 线性收敛

**答案：D**

每次区间长度减半，误差按固定比例缩小→线性收敛。

---

## 二、多选题

### 16. 以下说法正确的是()

- A. 向量组 α1,α2,…,a_n, 线性相关的充要条件是向量组中的任意一个向量 α_i 都可以由剩余的 n−1 个向量线性表示
- B. n≥2，向量组 α1−a2,…,α_{n−1}−α_n,2(α_n−α1) 一定线性相关
- C. 设 A,B 是满足 AB=0 的任意两个非零矩阵，则一定有 A 的列向量线性相关，B 的行向量线性相关
- D. 设 A 为 3 阶非零实方阵，A* 为 A 的伴随方阵，若 A*=−A^T，则 det(A)<0

**答案：B、C**

- ❌ A：充要条件是“至少存在一个向量可由其余向量线性表示”
- ✅ B：页面解析判定该向量组必线性相关
- ✅ C：AB=0 且 A,B 非零 ⇒ A 列向量或 B 行向量必线性相关
- ❌ D：由 A*=−A^T 不能推出 det(A)<0

---

### 17. 主成分分析(PCA)的前k个主成分具有哪些性质?

- A. 方差依次最大
- B. 原始数据旋转后主成分不变
- C. 主成分之间正交
- D. 与原始特征线性无关

**答案：A、C**

- ✅ A：前 k 个主成分依次最大化方差
- ❌ B：旋转后主成分方向一般会变化
- ✅ C：对应协方差矩阵特征向量，彼此正交
- ❌ D：主成分是原始特征的线性组合

---

### 18. 在为一个大型语言模型(LLM)选择和调整优化器时...下列哪些论断是理论上成立或在工程实践中被广泛认可的?

- A. Adam优化器中的偏差校正(Bias Corecion)机制...有助于在训练开始阶段获得更合理的学习步长
- B. 在损失函数的"峡谷“地形中...Adam或RMSprop...抑制振荡并加速收敛
- C. 对于RMSprop或Adam...梯度长期稀疏...会因二阶矩较小而提高有效学习率，使更新更稳定、更有效
- D. 相比于SGD with Momentum...Adam或RMSprop...会显著增加优化器状态的内存占用

**答案：A、B、C、D**

- ✅ A：偏差校正修正初期动量低估
- ✅ B：自适应学习率缓解“峡谷”震荡
- ✅ C：稀疏梯度时有效学习率可更大/更稳定
- ✅ D：需维护额外状态量，显存占用更大

---

### 19. 事件 A1,A2,A3 构成样本空间的一个划分...以下哪些是正确的?

- A. $P(A_2|B)=P(B|A_2)$
- B. $P(B)=0.44$
- C. $P(A_1|B)\approx0.045$
- D. $P(A_2|B)\approx0.273$

**答案：B、D**

- ❌ A：条件概率不可互换
- ✅ B：全概率公式计算得到 0.44
- ❌ C：页面解析判为错误（给出 $P(A_1|B)\approx0.0455$）
- ✅ D：贝叶斯公式计算约 0.273

---

### 20. 对于以下优化算法，说法正确的是?

- A. L-BFGS是二阶优化方法
- B. Momentum是二阶优化方法
- C. Adam是一阶优化方法
- D. AdaGrad是一阶优化方法
- E. Adamw是一阶优化方法

**答案：A、C、D、E**

- ✅ A：拟牛顿法，利用梯度近似 Hessian（或其逆）
- ❌ B：Momentum 仅一阶梯度信息
- ✅ C：Adam 属一阶优化方法
- ✅ D：AdaGrad 属一阶优化方法
- ✅ E：AdamW 本质仍是一阶优化方法


下面我按“**零基础**”来讲，每题只抓住一个核心招式，你照着做就能复现。

---

## （1）二次型的秩=2 ⇒ 求 c

**核心：二次型 = (x^\top A x)，秩(二次型) = 秩(对称矩阵 (A))。**
把
[
f=5x_1^2+5x_2^2+cx_3^2-2x_1x_2+6x_1x_3-6x_2x_3
]
写成矩阵：

* 平方项：(5x_1^2,5x_2^2,cx_3^2) ⇒ (A_{11}=5,A_{22}=5,A_{33}=c)
* 交叉项规则：**(2A_{ij}x_ix_j)**
  所以
  (-2x_1x_2\Rightarrow 2A_{12}=-2\Rightarrow A_{12}=-1)
  (6x_1x_3\Rightarrow 2A_{13}=6\Rightarrow A_{13}=3)
  (-6x_2x_3\Rightarrow 2A_{23}=-6\Rightarrow A_{23}=-3)

因此
[
A=\begin{pmatrix}
5 & -1 & 3\
-1 & 5 & -3\
3 & -3 & c
\end{pmatrix}
]

**秩=2（3×3矩阵）意味着：**

* (\det(A)=0)（否则秩=3）
* 同时**至少有一个 2×2 子式不为 0**（否则秩会掉到 1 或 0）

算 (\det(A))（按第一行展开最直观）：
[
\det(A)=5\begin{vmatrix}5&-3\-3&c\end{vmatrix}-(-1)\begin{vmatrix}-1&-3\3&c\end{vmatrix}+3\begin{vmatrix}-1&5\3&-3\end{vmatrix}
]
[
=5(5c-9) +(-c+9) +3(3-15)=24(c-3)
]
令 (\det(A)=0\Rightarrow c=3)。

再验一下“秩不是 1”：看左上 2×2 子式
[
\begin{vmatrix}5&-1\-1&5\end{vmatrix}=25-1=24\neq0
]
所以秩确实是 2。

---

## （3）这个题的关键是**括号歧义**，答案为什么是 (-4/27)

你会看到两种长相很像但结果完全不同的写法：

### 写法 A：(\big((\tfrac13 A)^{-1}\big))

那就是“先把 (A) 乘 (\tfrac13)，再整体求逆”：
[
(\tfrac13A)^{-1}=3A^{-1}
]
又因为 (|A|=2)，伴随矩阵（adjugate）满足
[
A^{-1}=\frac{1}{|A|}A^*=\frac12A^*
]
所以
[
(\tfrac13A)^{-1}-\tfrac12A^* = 3\cdot\tfrac12A^*-\tfrac12A^* = A^*
]
那么行列式应是 (|A^*|=|A|^{n-1}=2^2=4)。
**但选项里没有 4**，所以题库给的答案 (-4/27) 显然不是按这个理解出的。

### 写法 B（更符合题库答案）：(\tfrac13 A^{-1})

也就是“先求 (A^{-1})，再乘 (\tfrac13)”：
[
\tfrac13A^{-1}-\tfrac12A^*=\tfrac13\cdot\tfrac12A^*-\tfrac12A^*=\left(\tfrac16-\tfrac12\right)A^*=-\tfrac13A^*
]
3×3 行列式缩放规则：(|\alpha M|=\alpha^3|M|)。
所以
[
\left|-\tfrac13A^*\right|=\left(-\tfrac13\right)^3|A^*|=-\tfrac{1}{27}\cdot 4=-\tfrac{4}{27}
]
这就对上选项 D。

**记忆法：**

* ((\alpha A)^{-1}=\alpha^{-1}A^{-1})（标量“翻一次”）
* (|\alpha A|=\alpha^n|A|)（行列式“翻 n 次”）

---

## （4）列变换为什么等价于右乘？怎么拼出 Q？

**核心：(AQ) 的第 k 列 = (A) 的列向量按 (Q) 的第 k 列做线性组合。**
所以：**右乘改变列**（左乘改变行）。

题目两步：

### 第一步：交换第1列和第2列

这相当于右乘一个“交换矩阵”
[
P=\begin{pmatrix}
0&1&0\
1&0&0\
0&0&1
\end{pmatrix}
]
因为 (AP) 会把 (A) 的第2列搬到第1列、第1列搬到第2列。

### 第二步：把（交换后）的第2列加到第3列

要实现 “col3 ← col3 + col2”，右乘
[
E=\begin{pmatrix}
1&0&0\
0&1&1\
0&0&1
\end{pmatrix}
]
原因：(AE) 的第3列 = (A(e_3+e_2)=A e_3 + A e_2=\text{col3}+\text{col2})。

### 合起来

先交换，再相加：
[
C = APE \Rightarrow Q=PE
]
算一下：
[
Q=
\begin{pmatrix}
0&1&0\
1&0&0\
0&0&1
\end{pmatrix}
\begin{pmatrix}
1&0&0\
0&1&1\
0&0&1
\end{pmatrix}
=============

\begin{pmatrix}
0&1&1\
1&0&0\
0&0&1
\end{pmatrix}
]
你在选项里找这一坨矩阵就是答案（题库“答案整理”标成 A，但页面选项排版可能对不上，**以矩阵长相为准**）。

---

## （13）梯形法则误差上界怎么记？

**核心：梯形法误差由“弯曲程度”决定，弯曲程度就是二阶导 (|f''|)。**

标准结论：
[
|E_T|\le \frac{(b-a)}{12}h^2\max_{[a,b]}|f''(x)|
]
其中步长 (h=\frac{b-a}{n})，题里给的 (M=\max|f''|)。

代入：
[
|E_T|\le \frac{(b-a)}{12}\left(\frac{b-a}{n}\right)^2M
=\frac{M(b-a)^3}{12n^2}
]

**记忆法：**

* “梯形”误差：(\text{长度}\times \text{步长}^2\times \text{弯曲})
* 常数：(1/12)

---

## （16）D 你不会：其实按“伴随矩阵=adj(A)”它**应该能推出 det(A)<0**

选项 D：若 (A^*=-A^T)，则 (\det(A)<0)。

**一招秒杀：两边取行列式。**

* 3×3 时 (\det(A^*)=\det(A)^{n-1}=\det(A)^2)
* (\det(-A^T)=(-1)^3\det(A^T)=-\det(A))

等式 (A^*=-A^T) ⇒
[
\det(A)^2 = -\det(A)
\Rightarrow \det(A)(\det(A)+1)=0
]
又因为题设 (A\neq0) 且该等式会排除 (\det(A)=0) 的情况（否则推出 (A=0)），所以
[
\det(A)=-1<0
]
**所以从标准线代来看，D 是对的。**

> 但你页面“答案整理”没选 D，说明这题库整理可能有误，或者它把 (A^*) 误用成了别的符号（有些教材用 (A^*) 表示共轭转置）。你做机考时以出题方符号定义为准；你这套题里前面（第3题）明显是把 (A^*) 当“伴随矩阵/adjugate”用的，所以 D 推导是成立的。

---

## （18）你问 A？C？——用“Adam 两个平均”理解

Adam 做两件事（都只用梯度）：

1. **一阶动量**：梯度的滑动平均
   [
   m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t
   ]

2. **二阶动量**：梯度平方的滑动平均
   [
   v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2
   ]

更新：
[
\theta_{t+1}=\theta_t-\eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}
]

### A 为啥对（Bias correction 是干嘛的）

一开始 (m_0=v_0=0)，所以前几步的 (m_t,v_t) 会被“0”拖小（**偏向 0**）。
于是 Adam 做“偏差校正”：
[
\hat m_t=\frac{m_t}{1-\beta_1^t},\quad \hat v_t=\frac{v_t}{1-\beta_2^t}
]
直觉：**把早期被压小的平均值拉回正常尺度**，训练开头更稳。

### C 怎么理解（稀疏梯度时会发生什么）

对“很少更新/梯度稀疏”的参数维度，(v_t) 往往较小 ⇒ (\sqrt{v_t}) 小 ⇒ **分母小 ⇒ 相对步子更大**，这就是自适应法对稀疏特征更友好的原因（AdaGrad/Adam 常被这么用）。
你页面选项 C 的文字里有“有效学习率减小”的说法，**这句表述本身很容易写反**；真正稳定/广泛认可的结论是：

* **梯度大且频繁** ⇒ (v) 大 ⇒ 步子变小（防抖）
* **梯度稀疏/小** ⇒ (v) 小 ⇒ 步子相对更大（不让它“学不动”）

---

## （20）Adam 为什么是一阶优化？

**一句话：看它用没用“二阶导/Hessian”。**

* **一阶方法**：只用梯度 (g=\nabla f)（最多加点“梯度的统计量”）
  例：SGD、Momentum、AdaGrad、RMSprop、**Adam/AdamW**
* **二阶方法**：用 Hessian (H=\nabla^2 f) 或其近似/逆
  例：Newton、BFGS、**L-BFGS**

Adam 虽然有 (v_t)（平方梯度平均），但它仍然是**由梯度构造出来的缩放**，不需要也不计算二阶导，所以归类为**一阶优化方法**。

---

如果你愿意，我可以把（16）D 这个“题库答案可能有误”的点，给你构造一个**具体矩阵例子**满足 (A^*=-A^T) 且 (\det(A)=-1)，你就会彻底信服。你想看吗？

这条其实用一个**“反证 + 零解判别”**就能秒懂（零基础版）。

---

## 先记一个最重要的等价关系（就用这一条）

对矩阵 (A) 来说：

> **A 的列向量线性无关  ⇔  方程 (Ax=0) 只有一个解：(x=0)**

（反过来：只要存在一个 **非零** 的 (x\neq0) 让 (Ax=0)，那 A 的列向量就一定线性相关。）

---

## 1）为什么 (A) 的列向量一定线性相关？

已知 (AB=0)。这句话的意思是：对任何向量 (v)，都有
[
ABv = 0
]
令 (x = Bv)，那就变成
[
Ax = 0
]

现在用反证法：

* 假设 **A 的列向量线性无关**
  ⇒ 由上面的等价关系，(Ax=0) 只能推出 (x=0)。

* 但这里 (x=Bv)，所以对所有 (v) 都有 (Bv=0)
  ⇒ 这就意味着 **(B=0)**（把任何输入都映射成 0 的线性变换就是零矩阵）

* 可题目说 (B\neq0)，矛盾！

所以假设不成立 ⇒ **A 的列向量必线性相关**。

---

## 2）为什么 (B) 的行向量也一定线性相关？

这一步用同一个套路，但从“B 的行无关”推出“B 的输出覆盖整个空间”。

如果 (B) 是 (n\times p)（能和 (A) 相乘），那么：

> **B 的行向量线性无关** ⇔ **(\text{rank}(B)=n)**
> ⇔ (B) 的输出可以“铺满”(\mathbb{R}^n)（也就是：对任意 (y\in\mathbb{R}^n)，都能找到 (v) 使 (Bv=y)）

现在看 (AB=0)：对所有 (v)，都有
[
A(Bv)=0
]
如果 (Bv) 能取遍所有 (y\in\mathbb{R}^n)，那就等价于
[
Ay=0 \quad \text{对所有 } y
]
这只能说明一个事：**A 把所有向量都送去 0 ⇒ (A=0)**

但题设 (A\neq0)，矛盾！

所以 **B 的行向量不可能线性无关** ⇒ **B 的行向量必线性相关**。

---

## 一句“图像化”直觉（很好背）

* (AB=0) 表示：**B 先把东西送出来，A 再把它全部“杀死”变成 0**
* 如果 **B 送出来的东西很丰富（行无关→几乎能送出所有方向）**，那 A 就只能是全杀死→(A=0)（不允许）
* 如果 **A 没有“可被杀死”的非零方向（列无关→(Ax=0) 只有零解）**，那 B 送出来只能永远是 0 →(B=0)（不允许）

所以两边都得“有缺陷”：
✅ A 列相关、✅ B 行相关。

---
有，但**很弱**（而且阶数会变差）。

---

## 1）标准“梯形法误差上界”为啥用二阶导？

因为梯形法的误差本质来自“**曲率**”，曲率由 (f'') 控制，所以才有你题里那条：

[
|E_T|\le \frac{M(b-a)^3}{12n^2}\quad (M=\max|f''|)
]

这是 **(O(1/n^2))** 的。

---

## 2）如果我只知道一阶导数上界 (M_1=\max_{[a,b]}|f'(x)|)，有没有上界？

**有，但只能保证 (O(1/n))**（比 (1/n^2) 差一档）。

记 (h=\frac{b-a}{n})。一个很直观、好记的上界是：

[
|E_T|\ \le\ \frac{M_1(b-a)h}{4}
\ =\ \frac{M_1(b-a)^2}{4n}
]

### 为啥是这个量级（零基础直觉）

在每个小区间长度 (h) 内：

* 斜率最多 (M_1)，所以函数在区间中间“最多拱起”的高度大约不超过 (M_1\cdot \frac{h}{2})
* 这会造成“曲线面积”和“梯形面积”差一个三角形，面积最多
  [
  \frac{1}{2}\cdot h \cdot \left(M_1\frac{h}{2}\right)=\frac{M_1h^2}{4}
  ]
* 一共有 (n) 段：总误差 (\le n\cdot \frac{M_1h^2}{4}=\frac{M_1(b-a)h}{4})

---

## 3）考试怎么记

* **想要 (1/n^2)** → 必须有 **二阶导上界**（你题 13 就是这个）
* **只有一阶导上界** → 只能给 **(1/n)** 级别的粗上界

如果你告诉我你想要“严格证明版”还是“背诵版”，我可以把上面推导再压成 2 行可背公式。

# 2025-10-29 AI 方向题

## 一、单选题

### 1. 用雅可比迭代法求解方程组
\begin{cases} 4x - y = 7 \\ -x + 4y = 5 \end{cases}
取初值 x^{(0)} = (0, 0)^T，求第一次迭代结果 x^{(1)}。

- A. (1.75, 1.5)^T
- B. (1.75, 1.25)^T
- C. (1.5, 1.5)^T
- D. (1.5, 1.25)^T

**答案：B**

$$x=\frac{7+y}{4},\quad y=\frac{5+x}{4}$$
代入 $x^{(0)}=(0,0)^T$ 得 $x^{(1)}=(1.75,1.25)^T$。

---

### 2. 设n阶矩阵A和B等价，则必有

- A. 当 |A|=a(a≠0) 时，|B|=-a
- B. 当 |A|=0 时，|B|=0
- C. 当 |A|≠0 时，|B|=0
- D. 当 |A|=a(a≠0) 时，|B|=a

**答案：B**

若 $A$ 与 $B$ 等价，则 $B=PAQ$（$P,Q$ 可逆），故 $|B|=|P||A||Q|$，所以 $|A|=0\Rightarrow |B|=0$。

---

### 3. 对于平稳时间序列，若自相关函数在滞后阶 k=0 处为 1，而在其他滞后阶上快速衰减，则该序列最可能是：

- A. 周期信号
- B. 白噪声序列
- C. 存在强烈趋势的非平稳序列
- D. 阶跃趋势序列

**答案：B**

白噪声自相关函数除 $k=0$ 外近似为 0，且快速衰减。

---

### 4. 以下哪种分布是连续型分布且具有无记忆性

- A. 指数分布
- B. 泊松分布
- C. 正态分布
- D. 均匀分布

**答案：A**

指数分布满足无记忆性：$$P(X>s+t\mid X>s)=P(X>t).$$

---

### 5. 某研发中心有甲、乙两组团队负责一款芯片的缺陷检测，甲团队检测过的芯片占总量 60%，乙团队检测过的芯片占总量 40%。已知甲团队的检测准确率为95%（即缺陷芯片被检出的概率为95%，非缺陷芯片被误判为缺陷的概率为 5%），乙团队的检测准确率为 90%（即缺陷芯片被检出的概率为 90%，非缺陷芯片被误判为缺陷的概率为 10%）。现随机抽取一片被判定为 "缺陷" 的芯片，该芯片实际确实为缺陷芯片的概率为（）。
（注：该芯片的缺陷率为 1%，即随机抽取一片芯片，其本身为缺陷的概率为 0.01）

- A. 68.3%
- B. 11.8%
- C. 5.7%
- D. 92.5%

**答案：B**

$$P(D\mid +)=\frac{P(D,+)}{P(+)}=\frac{0.0093}{0.0786}\approx 11.8\%.$$

---

### 6. 在RLHF（基于人类反馈的强化学习）流程中，奖励模型（Reward Model，RM）扮演了至关重要的角色。关于奖励模型的作用，以下描述最准确的是？

- A. 奖励模型是一个可微分的"人类偏好代理（proxy）"，它将人类对答案好坏的、主观的、不可导的判断，转化为了一个可用于优化语言模型的、连续的、可导的奖励信号。
- B. 奖励模型直接修改基座模型的权重，使其生成更高质量的文本。
- C. 奖励模型的目标是学习模仿人类的写作风格，以便在后续的PPO阶段指导语言模型生成更像人话的文本。
- D. 奖励模型本身就是一个强化学习智能体（Agent），它通过与语言模型互动来学习如何给出最佳回答。

**答案：A**

RM 将偏好转为可导奖励信号供 RL（如 PPO）优化；RM 本身不直接改权重。

---

### 7. 关于协方差和相关系数，以下哪个说法是正确的？

- A. 如果变量 Y 是变量 X 的线性函数 Y=aX+b（其中 a>0），那么它们的相关系数会大于 1。
- B. 两个随机变量的协方差为 0，意味着这两个变量相互独立。
- C. 相关系数的取值范围是 [0, 1]。
- D. 相比于协方差，相关系数更能体现变量之间线性关系的强弱，因为它经过了标准化。

**答案：D**

相关系数标准化到 $[-1,1]$，能反映线性关系强弱；零协方差不必然独立。

---

### 8. 在注意力机制中，假设为单头注意力，QKV的矩阵维度均为 [L, D]，其中 L是序列长度，D是每个向量的维度。请问通过 QK 点积计算出的 Attention Score 和输出的矩阵维度分别是？

- A. attention score [L, L]，输出为 [L, L]
- B. attention score [L, D]，输出为 [L, L]
- C. attention score [D, L]，输出为 [L, D]
- D. attention score [L, L]，输出为 [L, D]

**答案：D**

$$Q\in\mathbb{R}^{L\times D},K\in\mathbb{R}^{L\times D}\Rightarrow QK^\top\in\mathbb{R}^{L\times L},\quad (QK^\top)V\in\mathbb{R}^{L\times D}.$$

---

### 9. LSTM（长短期记忆网络）引入 "遗忘门" 的主要作用是：

- A. 决定当前输入信息是否写入记忆单元
- B. 控制上一时刻隐藏状态有多少保留到当前状态
- C. 过滤当前输入的噪声
- D. 删除无关历史信息，避免梯度爆炸

**答案：B**

遗忘门控制上一时刻状态保留比例。

---

### 10. 在基于Transformer的大语言模型推理中，为了减少KV-Cache的显存占用并提升长文本生成吞吐，一种被广泛采用的策略是

- A. 使用FlashAttention重新计算部分注意力
- B. 按token粒度对KV-Cache进行LRU淘汰
- C. 在每一步解码后对KV向量做PCA降维
- D. 将KV-Cache组织成固定大小的"页"（paged attention）

**答案：D**

Paged Attention 将 KV-Cache 分块成固定大小"页"，降低碎片化并提升吞吐。

---

### 11. PCA（主成分分析）在做降维处理时，优先选取哪些特征（）

- A. 最大的特征根
- B. 最大的梯度方向
- C. 最强梯度方向
- D. 中心化样本的协方差矩阵的最大特征值对应的特征向量

**答案：D**

PCA 取协方差矩阵最大特征值对应特征向量作为主成分方向。

---

### 12. 已知三维向量 u=[1,-2,3]^T 和 v=[4,0,-1]^T，则 u×v（叉积）结果是？

- A. [-8,13,-2]^T
- B. [6,12,0]^T
- C. [2,13,8]^T
- D. [2,13,-8]^T

**答案：C**

$$u\times v=\begin{vmatrix}\mathbf{i}&\mathbf{j}&\mathbf{k}\\1&-2&3\\4&0&-1\end{vmatrix}=(2,13,8).$$

---

### 13. 假设有一个三层全连接神经网络，输入维度为 d，隐藏层维度为 h，输出层维度为 k。若使用 L2 正则化（权重衰减系数为 λ），并采用均方误差损失函数（MSE），则反向传播中输出的权重 W(3) 的梯度更新公式为？

- A. ∇W(3)= 1/n X^T(y−ŷ) − λW(3)
- B. ∇W(3)= 1/n X^T(y−ŷ) + λW(3)
- C. ∇W(3)= 1/n X^T(y−ŷ) + (λ/2)W(3)
- D. ∇W(3)= 1/n X^T(y−ŷ) · diag(W(3))

**答案：B**

L2 正则 $\frac{\lambda}{2}\|W^{(3)}\|^2$ 的梯度为 $+\lambda W^{(3)}$，故为"数据项 + $\lambda W^{(3)}$"。

---

### 14. 在三维图形变换中，点 P(1,2,3) 先绕 Z 轴旋转 θ=90°，再平移 d=(4,5,6)。用齐次坐标表示，最终变换矩阵 T 与点坐标 P′ 分别为：

- A. T = D Rz，P′ = (3,7,9,1)^T
- B. T = D Rz，P′ = (2,6,9,1)^T
- C. T = Rz D，P′ = (5,3,9,1)^T
- D. T = D Rz，P′ = (6,-1,9,1)^T

**答案：B**

先旋转后平移：$$T=D\,R_z.$$ 旋转得 $(-2,1,3)$，再平移得 $P'=(2,6,9,1)^T$。

---

### 15. 关于常见特征选择策略，下列哪项说法最为准确？

- A. L1 正则化策略是在训练过程中使部分特征权重趋近于零，从而实现特征选择。
- B. 递归特征消除（RFE）通过分析特征与目标的统计相关性进行筛选，其过程独立于具体模型。
- C. 方差阈值法通过设定一个方差阈值，剔除那些在样本间变化很小的特征，但该方法依赖于模型训练后得到的特征重要性权重。
- D. 卡方检验需要训练模型来评估不同特征组合的预测性能。

**答案：A**

L1 正则会产生稀疏解，使部分权重收缩到 0，实现特征选择。

---

## 二、多选题

### 16. 在设计先进的卷积神经网络（CNN）架构时，对每个组件的深层特性和相互作用的理解至关重要。以下关于CNN设计原则的陈述中，哪些是准确的？

- A. 1×1卷积层可以在不改变特征图空间（高分辨率）情况下，对通道维度进行线性组合与非线性变换，常用于实现跨通道的信息交互和维度调整。
- B. 相较于使用最大池化层（Max Pooling）进行下采样，采用步长（Stride）大于1的卷积层可以使下采样过程本身成为一个可学习的参数化操作。
- C. 连续堆叠两个步长为1的 3×3卷积层（含激活函数），可以达到与单个 5×5卷积层相同的感受野大小，但前者的参数量更少且引入了更多的非线性。
- D. 为了迅速增加模型的感受野（Receptive Field）并捕获更全局的上下文信息，设计时应优先采用单个较大的卷积核（如7×7或9×9），而不是堆叠多个小卷积核（如3×3），因为前者在参数数量和计算效率上更具优势。

**答案：A、B、C**

- ✅ A：跨通道线性组合 + 可接非线性，常用于通道调整/交互。
- ✅ B：Stride>1 卷积下采样是可学习的参数化操作。
- ✅ C：两层 $3\times3$ 感受野≈一层 $5\times5$，且参数更少、非线性更多。
- ❌ D：大核不一定更省参/更高效，常见设计更偏好多小核堆叠。

---

### 17. 下列哪几种 Transformer 变体 / 技术旨在降低长序列自注意力的计算或显存复杂度？（可多选）

- A. Sparse Attention（如 Longformer, Sparse Transformer）
- B. Performer（Random Feature Attention）
- C. Linformer
- D. Rotary Position Embedding (RoPE)

**答案：A、B、C**

- ✅ A：稀疏化降低注意力复杂度。
- ✅ B：随机特征近似注意力，降复杂度。
- ✅ C：低秩投影近似，降复杂度。
- ❌ D：RoPE 是位置编码，不直接降 $O(n^2)$ 复杂度。

---

### 18. 数据之间的关系以及数据服从的分布形态影响了模型建模的效果，下列关于线性回归的描述中，哪些是正确的？

- A. 线性回归模型的目标是最大化预测值与真实值之间的误差
- B. 线性回归模型假设自变量和因变量之间存在线性关系
- C. 线性回归模型可以通过最小二乘法进行参数估计
- D. 线性回归模型中，误差项（残差）必须服从正态分布

**答案：B、C**

- ✅ B：基本假设之一是线性关系。
- ✅ C：常用最小二乘估计参数。
- ❌ A：目标通常是最小化误差（如最小二乘）。
- ❌ D：残差正态不是"必须"，更多是用于推断/检验的假设。

---

### 19. 假设有两个离散随机变量 X 和 Y 的联合概率质量函数 (PMF) 用下表给出：  
|   | Y=0 | Y=1 |
|---|-----|-----|
| X=0 | 0.1 | 0.4 |
| X=1 | 0.3 | 0.2 |

根据上表，以下哪些计算或推断是正确的？

- A. 在 X=1 的条件下，Y=0 的条件概率 P(Y=0|X=1)=0.6
- B. Y=1 的概率 P(Y=1)=0.6
- C. 随机变量 X 和 Y 是相互独立的
- D. X 的期望值 E[X]=0.5%

**答案：A、B、D**

- ✅ A：$P(Y=0|X=1)=0.3/0.5=0.6$。
- ✅ B：$P(Y=1)=0.4+0.2=0.6$。
- ❌ C：不独立（如 $0.3\neq 0.5\cdot 0.4$）。
- ✅ D：$E[X]=0\cdot0.5+1\cdot0.5=0.5$。

---

### 20. 设 B＝{b1，b2，b3} 是 R^3 的一个组基，其中：  
b1＝[1,0,1]^T，b2＝[0,1,1]^T，b3＝[1,1,0]^T。向量 v 在基 B 下的坐标表示为 [v]_B＝[2,-1,3]^T，则以下说法正确的是（ ）

- A. 基B为一组正交基
- B. 从基B到标准基的过渡矩阵P的第三列为 [1,1,0]^T;注：过渡矩阵是将一个基下的坐标转化为另一个基下的坐标的矩阵，即 v_E＝P·[v]_B
- C. 向量 v 在标准基下的表示为 [5,2,1]^T
- D. 若 [w]_B＝[1,1,1]^T，则 v 与 w 的内积为6

**答案：B、C**

- ✅ B：过渡矩阵 $P=[b_1\ b_2\ b_3]$，第三列即 $b_3=[1,1,0]^T$。
- ✅ C：$v_E=P[v]_B=2b_1-1b_2+3b_3=[5,2,1]^T$。
- ❌ A：该组基不满足两两正交。
- ❌ D：网页未给出内积在"标准基/欧氏内积"下的进一步结论支持此项为真。

---

## ✅ 错题整理（结构化）

> **统计**: 20 题，7 错，正确率 65%

---

### Q3: 白噪声序列的自相关函数特征

**原题**：
> 对于平稳时间序列，若自相关函数在滞后阶 k=0 处为 1，而在其他滞后阶上快速衰减，则该序列最可能是：
> - A. 周期信号
> - B. 白噪声序列
> - C. 存在强烈趋势的非平稳序列
> - D. 阶跃趋势序列

**正确答案**：B

**易错点**：
不熟悉自相关函数（ACF）的形状与序列类型的对应关系

**核心公式/结论**：
$$
\gamma(k)=
\begin{cases}
\sigma^2, & k=0\\
0, & k\neq 0
\end{cases}
$$

| 序列类型 | 自相关函数形状 |
|---------|---------------|
| **白噪声** | 只有 $k=0$ 有峰，其余 $\approx 0$ |
| 周期信号 | 在周期倍数处反复出现峰 |
| 趋势序列 | 自相关缓慢衰减 |
| 阶跃 | 长相关 |

**一眼记住**：
「k=0 为 1 + 其余迅速衰减 = 白噪声」直接选 B

---

### Q5: 贝叶斯定理 + 全概率展开

**原题**：
> 某研发中心有甲、乙两组团队负责一款芯片的缺陷检测，甲团队检测过的芯片占总量 60%，乙团队检测过的芯片占总量 40%。已知甲团队的检测准确率为95%，乙团队的检测准确率为 90%。现随机抽取一片被判定为 "缺陷" 的芯片，该芯片实际确实为缺陷芯片的概率为？（缺陷率为 1%）
> - A. 68.3%
> - B. 11.8%
> - C. 5.7%
> - D. 92.5%

**正确答案**：B

**易错点**：
不知道要「按团队拆」，分母要用全概率公式展开

**核心公式/结论**：
$$
P(D\mid +)=\frac{P(+\mid D)P(D)}{P(+)}
$$

分母全概率展开（因为有甲/乙两个团队）：
$$
P(+) = 0.6(0.95\cdot0.01 + 0.05\cdot0.99) + 0.4(0.90\cdot0.01 + 0.10\cdot0.99) = 0.0786
$$

分子：
$$
P(D,+)=0.6\cdot0.95\cdot0.01 + 0.4\cdot0.90\cdot0.01 = 0.0093
$$

答案：$P(D\mid +)=0.0093/0.0786\approx 11.8\%$

**一眼记住**：
贝叶斯题必须「先画结构再算」，分母一定要全概率展开

---

### Q6: RLHF 中奖励模型的作用

**原题**：
> 在RLHF（基于人类反馈的强化学习）流程中，奖励模型（Reward Model，RM）扮演了至关重要的角色。关于奖励模型的作用，以下描述最准确的是？
> - A. 奖励模型是一个可微分的"人类偏好代理"，它将人类主观判断转化为可导的奖励信号
> - B. 奖励模型直接修改基座模型的权重
> - C. 奖励模型的目标是学习模仿人类的写作风格
> - D. 奖励模型本身就是一个强化学习智能体

**正确答案**：A

**易错点**：
误以为「RM 可微分」就能直接用于 SFT 反传

**核心公式/结论**：
$$
R_\phi(x, y)\in\mathbb{R}
$$

RM 的梯度是 $\frac{\partial R}{\partial y}$，但 $y$ 是离散 token 序列 → **无法直接对 $\theta$ 做反传**

必须用 RL（PPO）策略梯度：
$$
\nabla_\theta J(\theta) = \mathbb{E}\Big[ R(y)\nabla_\theta \log \pi_\theta(y\mid x)\Big]
$$

**一眼记住**：
RM 可微 ≠ 可 token 反传 → 必须用 RL；RM 只给标量奖励，RL 负责把奖励变成参数梯度

---

### Q10: Paged Attention vs FlashAttention

**原题**：
> 在基于Transformer的大语言模型推理中，为了减少KV-Cache的显存占用并提升长文本生成吞吐，一种被广泛采用的策略是
> - A. 使用FlashAttention重新计算部分注意力
> - B. 按token粒度对KV-Cache进行LRU淘汰
> - C. 在每一步解码后对KV向量做PCA降维
> - D. 将KV-Cache组织成固定大小的"页"（paged attention）

**正确答案**：D

**易错点**：
混淆 FlashAttention 和 Paged Attention 的目标

**核心公式/结论**：

| 对比 | FlashAttention | Paged Attention |
|------|----------------|-----------------|
| 目标 | 加速计算 | 降低 KV 显存 |
| KV cache | 全连续 | 分页管理 |
| 适合 | 训练/短上下文 | **长上下文推理** |

**一眼记住**：
FlashAttention = 算得快；PagedAttention = 省 KV + 长文本吞吐

---

### Q13: L2 正则下的梯度公式

**原题**：
> 假设有一个三层全连接神经网络，输入维度为 d，隐藏层维度为 h，输出层维度为 k。若使用 L2 正则化（权重衰减系数为 λ），并采用均方误差损失函数（MSE），则反向传播中输出的权重 W(3) 的梯度更新公式为？
> - A. ∇W(3)= 1/n X^T(y−ŷ) − λW(3)
> - B. ∇W(3)= 1/n X^T(y−ŷ) + λW(3)
> - C. ∇W(3)= 1/n X^T(y−ŷ) + (λ/2)W(3)
> - D. ∇W(3)= 1/n X^T(y−ŷ) · diag(W(3))

**正确答案**：B

**易错点**：
对 MSE + L2 的梯度推导不熟悉，不确定正则项梯度的符号

**核心公式/结论**：

MSE 数据项梯度：
$$
\nabla_W\mathcal{L}=\frac{1}{n}X^T(XW-y)=\frac{1}{n}X^T(\hat y-y)
$$

L2 正则项 $\frac{\lambda}{2}\|W\|^2$ 的梯度：
$$
\nabla = +\lambda W
$$

所以最终：
$$
\nabla W^{(3)}=\frac{1}{n}X^T(\hat y-y)+\lambda W^{(3)}
$$

**一眼记住**：
MSE 梯度 = $X^T(\hat y-y)$；L2 正则梯度 = $+\lambda W$（正号）

---

### Q15: 特征选择策略辨析

**原题**：
> 关于常见特征选择策略，下列哪项说法最为准确？
> - A. L1 正则化策略是在训练过程中使部分特征权重趋近于零，从而实现特征选择
> - B. 递归特征消除（RFE）通过分析特征与目标的统计相关性进行筛选，其过程独立于具体模型
> - C. 方差阈值法通过设定一个方差阈值，剔除变化很小的特征，但该方法依赖于模型训练后得到的特征重要性权重
> - D. 卡方检验需要训练模型来评估不同特征组合的预测性能

**正确答案**：A

**易错点**：
混淆各种特征选择方法是否依赖模型

**核心公式/结论**：

| 方法 | 是否依赖模型 | 说明 |
|------|-------------|------|
| L1 正则 | ✅ 依赖 | 训练时产生稀疏解 |
| RFE | ✅ 依赖 | train → ranking → eliminate |
| 方差阈值 | ❌ 不依赖 | 只看 $\mathrm{Var}(x_j)$ |
| 卡方检验 | ❌ 不依赖 | 统计检验 $\chi^2=\sum\frac{(O-E)^2}{E}$ |

**一眼记住**：
RFE 依赖模型；方差阈值、卡方检验不需要模型

---

### Q20: 基变换与过渡矩阵

**原题**：
> 设 B＝{b1，b2，b3} 是 R^3 的一个组基，其中 b1＝[1,0,1]^T，b2＝[0,1,1]^T，b3＝[1,1,0]^T。向量 v 在基 B 下的坐标表示为 [v]_B＝[2,-1,3]^T，则以下说法正确的是？
> - A. 基B为一组正交基
> - B. 从基B到标准基的过渡矩阵P的第三列为 [1,1,0]^T
> - C. 向量 v 在标准基下的表示为 [5,2,1]^T
> - D. 若 [w]_B＝[1,1,1]^T，则 v 与 w 的内积为6

**正确答案**：B、C

**易错点**：
1. 正交性判断：忘记检验 $b_i\cdot b_j=0$
2. 过渡矩阵公式：误以为是 $[v]_B = P v_E$

**核心公式/结论**：

正交检验：$b_i\cdot b_j=0\ (i\neq j)$
本题 $b_1\cdot b_2 = 0\cdot1+0\cdot1+1\cdot1=1\neq 0$ → ❌ 不正交

过渡矩阵定义（死记）：
$$
P=[b_1\ b_2\ b_3], \quad v_E=P[v]_B
$$

计算：$v = 2b_1 - b_2 + 3b_3 = 2(1,0,1)-(0,1,1)+3(1,1,0)=(5,2,1)$

**一眼记住**：
$v_E=P[v]_B$，P 的列是基向量；正交检验用点积为 0

---

## 📊 知识点统计（2025-10-29）

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 概率统计 | 5 | 2 | 60% |
| 线性代数 | 4 | 1 | 75% |
| 注意力与Transformer | 3 | 1 | 67% |
| 经典机器学习 | 4 | 1 | 75% |
| 深度学习基础 | 4 | 1 | 75% |
| 强化学习与对齐 | 1 | 1 | 0% |
| 优化与数值方法 | 1 | 0 | 100% |
| CNN | 1 | 0 | 100% |

---

## 🔥 薄弱环节 Top-3（2025-10-29）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 强化学习与对齐 | 1/1 | 0% | 样本少 |
| 2 | 概率统计 | 2/5 | 60% | 贝叶斯全概率、白噪声ACF |
| 3 | 注意力与Transformer | 1/3 | 67% | FlashAttn vs PagedAttn |

---

## 🧠 速记卡片

- **白噪声**：ACF 只有 k=0
- **Bayes**：先画结构再算，分母全概率展开
- **RM 可微 ≠ 可 token 反传** → 必须 RL
- **FlashAttention**：算得快
- **PagedAttention**：省 KV
- **MSE 梯度**：$X^T(\hat y-y)$
- **正交**：点积为 0
- **$v_E=P[v]_B$**：列是基向量

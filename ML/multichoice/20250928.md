# 2025-09-28 AI 方向题

## 一、单选题

### 1. 设 $S=\text{span}\left\{\begin{bmatrix}2 \\ 2 \\ 3\end{bmatrix}, \begin{bmatrix}8 \\ 5 \\ 6\end{bmatrix}\right\}$，$T=\text{span}\left\{\begin{bmatrix}1 \\ 0 \\ -1\end{bmatrix}, \begin{bmatrix}2 \\ 2 \\ 2\end{bmatrix}\right\}$，则 $S\cap T$ 的维度可能是？

- A. 1
- B. 0
- C. 2
- D. 3

**答案：A**

$$\dim(S\cap T)\ge \dim S+\dim T-3=1$$
若 $\dim(S\cap T)=2$ 则 $S=T$（但两组基向量互不包含，故不等）。

---

### 2. 在智能交通系统中，车辆到达某个交叉口的时间可以视为泊松过程。如果平均每小时有120辆车通过，那么任意连续两辆车之间的时间间隔大于1分钟的概率是多少？

- A. $e^{-1}$
- B. $e^{-2}$
- C. $e^{-4}$
- D. $e^{-3}$

**答案：B**

到达间隔 $T\sim \text{Exp}(\lambda)$，$\lambda=2/\text{min}$：
$$P(T>1)=e^{-2}$$

---

### 3. 一个团队构建的RAG问答系统在测试中表现不佳。用户提问：“在B-21轰炸机的研发合同中，诺斯罗普·格鲁曼公司承诺的初始作战能力（IOC）目标年份是哪一年？”
系统未能找到答案，尽管知识库中的一份文档里明确写着：“...该项目的主要承包商是诺斯罗普·格鲁曼公司。根据合同条款，B-21轰炸机的初始作战能力目标设定在2027年。”
工程师排查后发现，文档切分时，恰好将这两句话分在了两个相邻但独立的文本块（chunk）中：
- Chunk A: "...该项目的主要承包商是诺斯罗普·格鲁曼公司。"
- Chunk B: "根据合同条款，B-21轰炸机的初始作战能力目标设定在2027年。"
基于以上信息，请判断导致这次检索失败的最直接、最根本的技术原因是什么？

- A. 使用的Embedding模型质量太差，无法理解“诺斯罗普·格鲁曼”和“B-21轰炸机”之间的关联。
- B. 文档切分策略不当，破坏了关键信息的上下文完整性。Chunk A虽包含了关键实体，但与“年份”无关；Chunk B虽然有答案，但其自身文本不足以和包含“B-21”及“诺斯罗普·格鲁曼”的查询建立强语义关联。
- C. Tokenizer的词汇表不包含“诺斯罗普·格鲁曼”，导致该专有名词被错误地分割，影响了查询向量的生成。
- D. 向量搜索中的近似最近邻（ANN）算法精度过低，没能从海量向量中找到正确的Chunk B。

**答案：B**

不当切分把实体与答案分离，破坏语义完整性，导致检索无法强关联到含“2027年”的片段。

---

### 4. 在基于人类反馈的强化学习（RLHF）流程中，奖励模型（Reward Model, RM）扮演着至关重要的角色。它作为人类偏好的代理来指导语言模型的对齐。然而，过度优化奖励模型会导致一系列问题。以下哪个现象不属于“奖励黑客”（Reward Hacking）或奖励模型过优化（Over-optimization）的典型表现？

- A. 模型为了获得高分，倾向于生成更长、更啰嗦的回答，因为奖励模型可能将“详细”错误地关联为“更好”。
- B. 模型在多轮对话中，其回答的风格和价值观逐渐偏离了初始SFT阶段的状态，表现出与人类普遍价值观不符的倾向。
- C. 模型为了显得“有帮助”，即使在不知道答案的情况下，也会编造听起来非常plausible（貌似可信）但不正确的信息。
- D. 模型在面对它在预训练阶段从未见过的，全新的领域知识问题时，无法给出准确的回答。

**答案：D**

奖励黑客是过度优化RM导致与人类偏好偏离；D属于未见领域知识下的泛化不足。

---

### 5. 已知矩阵
$$
A=\begin{pmatrix} 1 & 0 & -1 \\ 2 & -1 & 1 \\ -1 & 2 & -5 \end{pmatrix}
$$
若下三角可逆矩阵 $P$ 和上三角可逆矩阵 $Q$ ，使得 $PAQ$ 为对角矩阵，则 $P,Q$ 可以分别取（ ）

- A. $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix},\ \ \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{pmatrix}$
- B. $\begin{pmatrix} 1 & 0 & 0 \\ 2 & -1 & 0 \\ -3 & 2 & 1 \end{pmatrix},\ \ \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{pmatrix}$
- C. $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 3 & 1 \end{pmatrix},\ \ \begin{pmatrix} 1 & 2 & -3 \\ 0 & -1 & 2 \\ 0 & 0 & 1 \end{pmatrix}$
- D. $\begin{pmatrix} 1 & 0 & 0 \\ 2 & -1 & 0 \\ -3 & 2 & 1 \end{pmatrix},\ \ \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$

**答案：B**

通过上三角列变换 $Q$ 与下三角行变换 $P$ 可将 $PAQ$ 化为对角矩阵。

---

### 6. 对不平衡二分类（正例1%，负例99%），下列哪种指标不会因「全部预测负例」而失效？

- A. AUC-ROC
- B. F1-score
- C. Precision@Top-10%
- D. Accuracy

**答案：A**

全部预测负例时 Accuracy、F1-score、Precision@Top-10% 均失效；AUC-ROC仍可评价排序区分能力。

---

### 7. 用线性回归在50个样本上得到，残差平方和 $SSE=18$，总平方和 $SST=50$，求 $R^2$

- A. 0.64
- B. 0.62
- C. 0.68
- D. 0.66

**答案：A**

$$R^2 = 1 - \frac{SSE}{SST}=1-\frac{18}{50}=0.64$$

---

### 8. 关于多头注意力（Multi-Head Attention）机制的核心设计目的，以下哪项说法最准确？

- A. 通过共享各头的Query与Key权重矩阵，减少参数数量，从而提高计算效率并降低模型复杂度。
- B. 通过在多个头中重复相同的注意力计算，增强对长距离依赖的建模稳定性，并加快模型训练收敛速度。
- C. 通过将输入映射到多个不同特征子空间并分别计算注意力，使模型能够同时捕捉不同子空间的信息，提升表达能力。
- D. 其本质是引入多个前馈神经网络，对输入序列进行并行非线性变换，增强模型的拟合能力。

**答案：C**

多头注意力将输入投影到多个子空间分别计算注意力，以捕捉不同依赖与特征。

---

### 9. 设二维随机向量 $(X,Y)$ 的概率密度函数为
$$
f(x,y)=\begin{cases}
Ax^2, & 0<|x|<y<1 \\
0, & 其它
\end{cases}
$$
则常数 $A$ 的取值 ( )，条件概率 $P(X \leq 0.25|Y=0.5)$ 的值为 ( )

- A. 6, 9/16
- B. 3, 9/16
- C. 3, 5/16
- D. 6, 5/16

**答案：A**

$$A\int_0^1\int_{-y}^{y}x^2\,dx\,dy=1\Rightarrow A=6$$
$$P(X\le0.25|Y=0.5)=\int_{-0.5}^{0.25}12x^2\,dx=\frac{9}{16}$$

---

### 10. 某工厂两条生产线 A（合格率 90%，产量 60%）和 B（合格率 80%，产量 40%）。随机抽取一件不合格品，其来自 A 线的概率为？

- A. 3/7
- B. 6/7
- C. 4/7
- D. 5/7

**答案：A**

$$P(A|\text{不合格})=\frac{0.6\times0.1}{0.6\times0.1+0.4\times0.2}=\frac{3}{7}$$

---

### 11. 循环神经网络（RNN）之所以特别适合处理文本、语音等序列数据，其最根本的结构性原因是？

- A. 它在网络的不同空间位置共享参数，类似于CNN。
- B. 它包含一个内部的循环，允许信息从输入的一个时间步持续传递到下一个时间步。
- C. 它将整个输入序列一次性全部输入到网络中进行并行计算。
- D. 它使用了比其他网络更复杂的激活函数。

**答案：B**

RNN 通过隐藏状态在时间维度上传递信息，适合处理序列依赖。

---

### 12. 在训练一个回归模型（如线性回归）来预测房价时，我们通常选择最小化均方误差（MSE）作为损失函数，这背后隐含了一个关于数据噪声的什么概率假设？

- A. 噪声服从泊松分布
- B. 噪声服从均匀分布
- C. 噪声服从指数分布
- D. 噪声服从高斯分布（正态分布）

**答案：D**

最小化 MSE 等价于在高斯噪声假设下的极大似然估计。

---

### 13. 用 Newton 迭代法求方程 $f(x)=x^2-3=0$ 的正根，初值 $x_0=2$，求第一次迭代后的近似值 $x_1$：

- A. 1.25
- B. 1.75
- C. 2.25
- D. 2.5

**答案：B**

$$x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)},\ \ x_1=2-\frac{4-3}{4}=1.75$$

---

### 14. 在神经网络中，以下哪种激活函数可以缓解梯度消失问题，并且在隐藏层中被广泛使用？

- A. Tanh
- B. 阶跃函数
- C. Sigmoid
- D. ReLU

**答案：D**

ReLU 正半轴梯度为常数、负半轴为 0，可缓解梯度消失，隐藏层常用。

---

### 15. 已知 $A$ 是 3 阶矩阵，$\mathrm{rank}(A)=1$，则 $A$ 的特征值是（）

- A. 一重、二重、三重特征值都有可能
- B. 至少是 0 的二重特征值
- C. 至多是 0 的二重特征值
- D. 必然是 0 的二重特征值

**答案：B**

零空间维数为 $3-1=2$，故 0 的几何重数为 2，代数重数至少为 2（可能为 3）。

---

## 二、多选题

### 16. 以下哪些方法适用于关键词提取任务

- A. 最大熵模型
- B. TextRank
- C. LDA（隐狄利克雷分布）
- D. TF-IDF

**答案：B、C、D**

- ✅ B：基于图排序的无监督方法，常用于从文本中抽取关键词。
- ✅ C：主题模型可据词-主题分布度量词的重要性，用于关键词提取。
- ✅ D：基于词频与逆文档频率衡量词的重要性，是经典的关键词抽取方法。

---

### 17. 在构建大模型时，以下哪些因素会影响其泛化能力？

- A. 训练数据的规模
- B. 正则化技术的应用
- C. 数据集的多样性
- D. 模型参数的数量
- E. 使用更大的学习率

**答案：A、B、C、D**

- ✅ A：更多高质量数据能降低方差、提升泛化。
- ✅ B：正则化抑制过拟合、改善泛化。
- ✅ C：覆盖更广分布可减少分布外误差、提升泛化。
- ✅ D：模型复杂度影响偏差-方差权衡。

---

### 18. 关于 Causal Mask 的作用，正确的是

- A. 是一个上三角为负无穷的矩阵
- B. 防止解码器在训练时看到未来的信息
- C. 仅用于文本生成任务
- D. 用于增强自注意力模型的局部性

**答案：A、B**

- ✅ A：是一个上三角为负无穷的矩阵。
- ✅ B：防止解码器在训练时看到未来的信息。

---

### 19. 设 $X_i$ 独立同分布，$\mathbb{E}[X_i]=\mu$，$\mathrm{Var}(X_i)=\sigma^2$。当 $n\to\infty$ 时，记 $\bar{X}_n=\frac1n \sum_{i=1}^{n}X_i$，$S_n=\sum_{i=1}^n X_i$。以下哪些是正确的渐近分布？

- A. $\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}\xrightarrow{d}N(0,1)$
- B. $\sqrt{n}(\bar{X}_n-\mu)/\sigma \xrightarrow{d} N(0,1)$
- C. $\frac{S_n - n\mu}{\sigma} \xrightarrow{d} N(0,1)$
- D. $X_n \xrightarrow{d} N(\mu,\sigma^2)$

**答案：A、B**

- ✅ A：由中心极限定理，样本均值标准化后渐近正态。
- ✅ B：与 A 等价的标准化形式，同由中心极限定理成立。

---

### 20. 激活函数在神经网络中起到非线性变换的作用，使模型能够拟合更加复杂的问题和场景。请问以下哪项是常用的激活函数？

- A. ReLU
- B. Softmax
- C. Linear
- D. Sigmoid

**答案：A、B、D**

- ✅ A：ReLU 是目前最常用的激活函数之一，能有效缓解梯度消失且计算简单。
- ✅ D：Sigmoid 虽在深层网络中不常用，但在一些二分类场景中仍然常用。
- ✅ B：Softmax 在多分类问题的输出层中非常常用。


下面按你标的题号来（都给**公式 + 直觉/类比 + 记忆法**），尽量机考背得住。

---

## (6) 全预测负例：F1 为啥“算不出来”？AUC 又怎么还能算？

### F1（严格数学上确实会卡住）

全预测负例时（假设数据里确实有正例）：

* $TP=0,\ FP=0,\ FN>0$
* Precision $P=\dfrac{TP}{TP+FP}=\dfrac{0}{0}$ **未定义**
* Recall $R=\dfrac{TP}{TP+FN}=0$

F1：
$$
F1=\frac{2PR}{P+R}
$$
因为 $P$ 未定义，所以**严格数学意义下 F1 不可定义**。

> **工程实现记忆**：大多数库会把“没有预测正例”( $TP+FP=0$ ) 时的 precision 置 0，于是 F1 也输出 0（相当于“你一个正例都没抓到”）。

### AUC-ROC（为什么还能算）

AUC 不依赖你选的阈值（比如 0.5），它看的是**分数排序能力**。

最常用等价定义（超好记）：
$$
\text{AUC}=\Pr\big(s(x^+)>s(x^-)\big)\ +\ \frac12\Pr\big(s(x^+)=s(x^-)\big)
$$
也就是：**随机抽一个正样本和一个负样本，正样本分数更大**的概率（并列算半分）。

* 即使你“阈值分类结果”全是负例，只要模型输出的**概率/打分**在正负样本间有排序差异，AUC 仍可算。
* 若模型对所有样本都输出同一分数（完全没排序信息），那 ROC 是对角线，**AUC=0.5**。

**物理直觉/类比**：
AUC 像“把正负样本当两类粒子，分数是能量”，AUC 就是“正粒子能量更高”的概率；阈值分类只是你拿一条能量切割线。

---

## (9) 条件概率里为什么会出现 **12**：从哪来的？

你写的：
$$
P(X\le0.25\mid Y=0.5)=\int_{-0.5}^{0.25}12x^2,dx
$$
关键是 **$12x^2$ 是条件密度 $f_{X|Y}(x|0.5)$**。

已知联合密度：
$$
f(x,y)=Ax^2,\quad 0<|x|<y<1
$$
且题解给了 $A=6$。

先算边缘密度：
$$
f_Y(y)=\int_{-y}^{y}6x^2,dx
=6\cdot\left[\frac{x^3}{3}\right]*{-y}^{y}
=6\cdot\frac{2y^3}{3}
=4y^3
$$
代 $y=0.5$：
$$
f_Y(0.5)=4(0.5)^3=4\cdot 0.125=0.5
$$
所以条件密度：
$$
f*{X|Y}(x|0.5)=\frac{f(x,0.5)}{f_Y(0.5)}
=\frac{6x^2}{0.5}=12x^2,\quad |x|<0.5
$$
这就是 **12** 的来源：**“切一片后再归一化”** 的归一化系数。

**记忆法**：
条件密度 = “这条横截面” ÷ “这条横截面的面积”。分母一小，分子就会被放大（这里 $0.5$ 让系数翻倍成 12）。

---

## (15) 为什么说 0 的代数重数“至少”是 2？

已知 $A$ 是 $3\times 3$ 且 $\mathrm{rank}(A)=1$。

### 先确定几何重数（一定的）

零空间维数（Nullity）：
$$
\dim\ker(A)=3-\mathrm{rank}(A)=2
$$
而 $\ker(A)$ 里的非零向量都是满足 $Av=0$ 的特征向量（特征值 0）。
所以 **0 的几何重数**：
$$
g_0=\dim\ker(A)=2
$$

### 代数重数为什么只能说“至少”

线代铁律（背这个就够了）：
$$
g_\lambda \le a_\lambda
$$
几何重数不超过代数重数，所以：
$$
a_0 \ge 2
$$
但 $a_0$ 可能是 2，也可能是 3。

**两个一眼例子（超有说服力）**

* $A=\mathrm{diag}(1,0,0)$：$\mathrm{rank}=1$，特征值 $1,0,0$ ⇒ $a_0=2$
* $A=\begin{pmatrix}0&1&0\0&0&0\0&0&0\end{pmatrix}$：$\mathrm{rank}=1$，但特征值全是 0 ⇒ $a_0=3$

**直觉**：rank=1 说明“最多把 3D 空间压到 1D 输出”，所以至少有 2 个方向被压扁到 0（几何重数=2），但“压扁到 0”的程度可能更彻底（所有特征值都变 0），于是代数重数能更大。

---

## (16) LDA 举个最简单例子 + TextRank 是啥？

### LDA（隐狄利克雷分布）——“文档=主题混合鸡尾酒”

设 2 个主题：

* 主题1：体育（球、比赛、球队…）
* 主题2：美食（米饭、火锅、味道…）

LDA 的核心生成式（背这两行够用）：
$$
\theta_d \sim \mathrm{Dir}(\alpha),\quad
z_{dn}\sim \mathrm{Mult}(\theta_d),\quad
w_{dn}\sim \mathrm{Mult}(\phi_{z_{dn}})
$$
含义：

* 每篇文档 $d$ 有一个“主题配比” $\theta_d$（比如 80%体育+20%美食）
* 每个词先抽主题 $z$，再从该主题的词分布里抽词 $w$

**一句话记忆**：
**Dirichlet 是“分布的分布”**：它生成一个概率向量（主题配比）。

### TextRank——“把词当网页做 PageRank”

步骤（机考版）：

1. 把候选词当图节点
2. 若两词在窗口内共现，就连边
3. 跑类似 PageRank 的迭代分数：
   $$
   S(v_i)=(1-d)+d\sum_{v_j\in \mathrm{In}(i)}\frac{S(v_j)}{|\mathrm{Out}(j)|}
   $$
   分记忆：
   **TextRank = PageRank on Text（词=网页，共现=链接）**

---

## (17) 学习率与泛化“无关”吗？那泛化更主要跟什么有关？

更准确说法：**学习率不是“稳定提升泛化”的因素**，但它会**间接影响**泛化（通过优化路径/落到什么极小值）。

* **泛化更主要、单调更可靠的因素**（题里 A/B/C/D 这类）：

  1. **数据规模**（越多越稳）
  2. **数据多样性**（覆盖分布）
  3. **正则化**（Weight decay、dropout、早停、数据增强）
  4. **模型容量**（偏差-方差权衡）

* **学习率**：更像“你怎么走到山谷里”，可能走到更“平坦”的谷（常被认为泛化更好），也可能直接走飞/震荡。
  所以在选择题里常把 “使用更大的学习率” 视为**不可靠/不一定提升泛化** 的选项。

**记忆法**：
泛化=“模型能不能适应新样本”，**最硬的三件套：数据量、数据多样性、正则化**；学习率主要是“训得动不动”。

---

## (18) “增强局部性”怎么做？为什么要增强？以及：这跟 Causal Mask 有关系吗？

先纠正：**Causal Mask 的目的不是增强局部性**，而是**禁止看未来**（自回归/解码器信息不泄露）。
它是“方向约束”，不是“距离约束”。

### 真正增强局部性的常见做法（距离约束）

1. **滑动窗口/局部注意力 mask**：只允许看 $i\pm w$ 范围
2. **块稀疏 attention**：分块，只连局部块+少量全局 token
3. **相对位置偏置/距离衰减**：距离越远 bias 越负
4. **Conv + Attention**：用卷积先抓局部，再 attention 抓全局

### 为什么要增强局部性？

* **很多信号本来就是局部模式强**（语言 n-gram、视觉邻域纹理、时序短期相关）
* **省算力/省显存**：全局注意力 $O(L^2)$，局部可降到 $O(Lw)$
* **更稳**：局部先学好，再补全局依赖

**物理类比**：
局部性像“近场相互作用更强”，先把近邻耦合学稳，再考虑长程耦合。

---

## (19) 把四个选项“列给你看”并逐条判定

题干：$X_i$ iid，$\mathbb{E}[X_i]=\mu$，$\mathrm{Var}(X_i)=\sigma^2$，$\bar X_n=\frac1n\sum X_i$，$S_n=\sum X_i$。

### A ✅

$$
\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}\xrightarrow{d}N(0,1)
$$
这是 CLT 的标准形式。

### B ✅（和 A 完全等价）

$$
\sqrt{n}(\bar{X}_n-\mu)/\sigma \xrightarrow{d} N(0,1)
$$
因为 $\dfrac{\bar X_n-\mu}{\sigma/\sqrt n}=\dfrac{\sqrt n(\bar X_n-\mu)}{\sigma}$。

### C ❌（少了 $\sqrt n$，尺度错）

正确的应该是：
$$
\frac{S_n-n\mu}{\sigma\sqrt{n}}\xrightarrow{d}N(0,1)
$$
因为 $\mathrm{Var}(S_n)=n\sigma^2$，标准化要除以 $\sigma\sqrt n$。

### D ❌（$X_n$ 本身不趋于正态）

$X_n$ 的分布就是 $X_1$ 的分布，除非题目额外给“$X_i$ 本来就是正态”，否则不能推出正态。

**记忆法**：

* 均值的波动量级是 $1/\sqrt n$，所以标准化一定出现 $\sqrt n$。
* 看到 $S_n$ 就想到方差 $n\sigma^2$ ⇒ 归一化用 $\sqrt n$。

---
你这三个点问得很关键，都是“**指标定义 vs 你实际输出的是什么**”造成的错觉。

---

## 1) AUC-ROC 的横坐标不是 TPR，是 **FPR**

ROC 曲线定义：

* 纵轴：**TPR**（召回率）
  $$\mathrm{TPR}=\frac{TP}{TP+FN}$$
* 横轴：**FPR**（假阳性率）
  $$\mathrm{FPR}=\frac{FP}{FP+TN}$$

**记忆法**：
ROC 的 x 轴是 **False**（错报），y 轴是 **True**（抓到）。

### 那“全预测负例”时 ROC 不就只有一个点 (0,0) 吗？

这是你在 **某一个固定阈值**（比如 0.5）下的分类结果：

* $TP=0, FP=0 \Rightarrow \mathrm{TPR}=0,\ \mathrm{FPR}=0$
  所以确实是一个点 **(0,0)**。

但 **AUC 不是用一个阈值的一个点算的**，AUC 是把阈值从高到低扫一遍，得到整条 ROC 曲线，然后算曲线下面积。

### 为什么 AUC 还能“有值”？

因为 AUC 用的是**模型打分/概率**的排序信息，而不是你最后那次“全负”的硬标签。

等价定义（最好背）：
$$
\mathrm{AUC}=\Pr(s(x^+)>s(x^-))+\frac12\Pr(s(x^+)=s(x^-))
$$

* 只要正样本的分数整体比负样本高一些，AUC 就 > 0.5
* 如果所有样本分数都一样（完全没排序能力），AUC = 0.5
* 如果你只给硬标签（全是 0），**没有分数**，那 AUC 在数学上就没信息了（很多工具会报错或当作常数分数 → AUC=0.5）

---

## 2) (15) rank=1 但所有特征值都是 0 合理吗？那 rank 不该是 0 吗？

**完全合理。**
关键点：**rank=0 当且仅当矩阵是零矩阵**。

* “所有特征值都是 0”只说明矩阵是**奇异的**（det=0），不代表矩阵等于 0。
* 非零矩阵也可能所有特征值都为 0，这类矩阵叫 **幂零矩阵 (nilpotent)**。

### 一个最短反例（背下来就行）

$$
A=\begin{pmatrix}
0&1&0\
0&0&0\
0&0&0
\end{pmatrix}
$$

* 明显 $A\neq 0$，所以 **rank 不能是 0**
* 只有一行/列有效，$\mathrm{rank}(A)=1$
* 但 $A^2=0$（幂零），因此它的全部特征值都是 0

**直觉**：
特征值描述的是“线性变换的伸缩因子”（沿某些方向的放大倍数）。
幂零矩阵的作用像“把向量往某个方向推一下”，但推完再推就全塌到 0（$A^k=0$），所以伸缩因子全是 0；可它第一次推的时候仍然改变了向量，因此 rank 仍然可以 > 0。

---

## 3) TextRank 最后是找“边最多”的词吗？图建完后怎么选词？

不是“边最多”（度最大）这么粗糙。TextRank 用的是 **PageRank / 特征向量中心性**：
一个词重要，不仅因为它连得多，还因为它连接的词也重要。

### TextRank 关键步骤（机考背法）

1. **建图**：词是节点；在窗口大小 $w$ 内共现就连边（可加权）
2. **迭代打分**（PageRank）：
   $$
   S(v_i)=(1-d)+d\sum_{v_j\in \mathrm{In}(i)}\frac{w_{ji}}{\sum_{v_k\in \mathrm{Out}(j)} w_{jk}};S(v_j)
   $$

* $d$ 常取 0.85
* $w_{ji}$ 是边权（共现次数/距离衰减等）

3. **取 Top-K 词**：按 $S(v)$ 排序取前 K 个
   4)（常见后处理）把文本中**相邻的关键词**拼成短语（如 “大语言模型”）

### 直觉类比

* “边最多”=只看社交账号粉丝数（度）
* TextRank=看“你被哪些大V关注”（中心性），更稳、更抗噪

**一句话记忆**：
TextRank = **PageRank on words**（词=网页，共现=链接，分数=重要性传播）

---

FPR（False Positive Rate）这玩意儿常见别名/中文如下（别和敏感度搞混就行）：

### FPR 的常用英文别名

* **False alarm rate**（误报率 / 虚警率）
* **Fall-out**（ROC 里很常见的叫法）
* （在统计检验语境）**Type I error rate** / **α**（第一类错误率 / 显著性水平，前提是把"把负类误判为正类"对应成 I 类错误）

### FPR 的常用中文

* **假阳性率**
* **误报率**
* **虚警率**
* **第一类错误率**（或 **I 类错误率**）

---

### 你提到的 "sensitivity" 是谁？

* **Sensitivity = TPR = Recall = 真阳性率 = 召回率**
  $$\mathrm{TPR}=\frac{TP}{TP+FN}$$
* FPR 不是 sensitivity。

**一眼记忆法（机考最稳）**：

* **TPR = Sensitivity（敏感度）= Recall（召回）**
* **TNR = Specificity（特异度）**
  $$\mathrm{TNR}=\frac{TN}{TN+FP}$$
* **FPR = 1 - Specificity**
  $$\mathrm{FPR}=\frac{FP}{FP+TN}=1-\mathrm{TNR}$$

---

## 📊 知识点标注表

| 题号 | 知识点 Tag1 | 知识点 Tag2 | 是否错题 |
|------|-------------|-------------|----------|
| Q1 | 线性代数 | - | ❌ |
| Q2 | 概率统计 | - | ❌ |
| Q3 | 大模型(LLM) | - | ❌ |
| Q4 | 强化学习与对齐 | - | ❌ |
| Q5 | 线性代数 | - | ❌ |
| Q6 | 评估指标 | - | ✅ 错 |
| Q7 | 经典机器学习 | - | ❌ |
| Q8 | 注意力与Transformer | - | ❌ |
| Q9 | 概率统计 | - | ✅ 错 |
| Q10 | 概率统计 | - | ❌ |
| Q11 | 深度学习基础 | - | ❌ |
| Q12 | 经典机器学习 | 概率统计 | ❌ |
| Q13 | 优化与数值方法 | - | ❌ |
| Q14 | 深度学习基础 | - | ❌ |
| Q15 | 线性代数 | - | ✅ 错 |
| Q16 | 大模型(LLM) | - | ✅ 错 |
| Q17 | 深度学习基础 | - | ✅ 错 |
| Q18 | 注意力与Transformer | - | ✅ 错 |
| Q19 | 概率统计 | - | ✅ 错 |
| Q20 | 深度学习基础 | - | ❌ |

---

## 📈 掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 概率统计 | 5 | 2 | 60% |
| 线性代数 | 3 | 1 | 67% |
| 深度学习基础 | 4 | 1 | 75% |
| 大模型(LLM) | 2 | 1 | 50% |
| 评估指标 | 1 | 1 | 0% |
| 注意力与Transformer | 2 | 1 | 50% |
| 经典机器学习 | 2 | 0 | 100% |
| 优化与数值方法 | 1 | 0 | 100% |
| 强化学习与对齐 | 1 | 0 | 100% |

---

## 🔥 薄弱环节 Top-3（20250928）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 评估指标 | 1/1 | 0% | 样本少 |
| 2 | 大模型(LLM) | 1/2 | 50% | TextRank/LDA |
| 3 | 注意力与Transformer | 1/2 | 50% | Causal Mask |

---

## 📚 核心知识点速查

### 📊 评估指标
| 公式/概念 | 说明 |
|-----------|------|
| $\mathrm{AUC}=\Pr(s(x^+)>s(x^-))$ | AUC = 正样本分数 > 负样本分数的概率 |
| $\mathrm{TPR}=\frac{TP}{TP+FN}$ | 真阳性率 = Sensitivity = Recall |
| $\mathrm{FPR}=\frac{FP}{FP+TN}$ | 假阳性率 = 1 - Specificity |
| $F1=\frac{2PR}{P+R}$ | 全预测负例时 P 未定义，F1 失效 |

### 📐 概率统计
| 公式/概念 | 说明 |
|-----------|------|
| $f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}$ | 条件密度 = 联合/边缘 |
| $f_Y(y)=\int f(x,y)dx$ | 边缘密度 |
| $\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}\xrightarrow{d}N(0,1)$ | 中心极限定理 |
| $\mathrm{Var}(S_n)=n\sigma^2$ | 和的方差需除 $\sigma\sqrt{n}$ |

### 🧮 线性代数
| 公式/概念 | 说明 |
|-----------|------|
| $g_\lambda \le a_\lambda$ | 几何重数 ≤ 代数重数 |
| $\dim\ker(A)=n-\mathrm{rank}(A)$ | 零空间维数 |
| 幂零矩阵 | $A^k=0$，特征值全为 0，但 rank 可 > 0 |

### 🤖 大模型(LLM)
| 公式/概念 | 说明 |
|-----------|------|
| LDA | $\theta_d \sim \mathrm{Dir}(\alpha)$，文档=主题混合 |
| TextRank | PageRank on words（词=网页，共现=链接） |

### 🎯 注意力与Transformer
| 公式/概念 | 说明 |
|-----------|------|
| Causal Mask | 上三角负无穷，禁止看未来（方向约束） |
| 局部性增强 | 滑动窗口/距离衰减（距离约束），与 Causal Mask 无关 |

---

## ✅ 错题整理（结构化）

### Q6: 不平衡分类指标（AUC-ROC）

**原题**：
> 对不平衡二分类（正例1%，负例99%），下列哪种指标不会因「全部预测负例」而失效？
> - A. AUC-ROC
> - B. F1-score
> - C. Precision@Top-10%
> - D. Accuracy

**正确答案**：A

**易错点**：
误以为 AUC 也需要阈值才能计算，但 AUC 基于分数排序能力，与阈值无关。

**核心公式/结论**：
$$\mathrm{AUC}=\Pr(s(x^+)>s(x^-))+\frac{1}{2}\Pr(s(x^+)=s(x^-))$$

**一眼记住**：
AUC 看「排序」不看「阈值」，全预测负例只是某一阈值的结果，不影响 AUC。

---

### Q9: 条件概率密度（12x² 系数来源）

**原题**：
> 设二维随机向量 $(X,Y)$ 的概率密度函数为 $f(x,y)=Ax^2,\ 0<|x|<y<1$，则常数 $A$ 的取值，条件概率 $P(X \leq 0.25|Y=0.5)$ 的值为

**正确答案**：A（$A=6$，概率 $9/16$）

**易错点**：
不清楚条件密度中 12 的来源：$f_{X|Y}(x|0.5)=\frac{6x^2}{0.5}=12x^2$

**核心公式/结论**：
$$f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)},\quad f_Y(y)=\int_{-y}^{y}6x^2\,dx=4y^3$$

**一眼记住**：
条件密度 = "切一片" ÷ "这片面积"，分母小就放大系数。

---

### Q15: 特征值代数重数（rank=1 矩阵）

**原题**：
> 已知 $A$ 是 3 阶矩阵，$\mathrm{rank}(A)=1$，则 $A$ 的特征值是（）
> - A. 一重、二重、三重特征值都有可能
> - B. 至少是 0 的二重特征值
> - C. 至多是 0 的二重特征值
> - D. 必然是 0 的二重特征值

**正确答案**：B

**易错点**：
误以为 rank=1 则特征值 0 的重数"恰好"是 2；实际可能是 3（幂零矩阵）。

**核心公式/结论**：
$$g_0=\dim\ker(A)=n-\mathrm{rank}(A)=2,\quad g_\lambda\le a_\lambda$$

**一眼记住**：
几何重数 ≤ 代数重数；rank=1 → 几何重数=2 → 代数重数 ≥ 2（可为 3）。

---

### Q16: 关键词提取方法（TextRank/LDA）

**原题**：
> 以下哪些方法适用于关键词提取任务
> - A. 最大熵模型
> - B. TextRank
> - C. LDA（隐狄利克雷分布）
> - D. TF-IDF

**正确答案**：B、C、D

**易错点**：
不熟悉 TextRank 和 LDA 的原理与用途。

**核心公式/结论**：
- TextRank: $S(v_i)=(1-d)+d\sum_{v_j}\frac{w_{ji}}{\sum_k w_{jk}}S(v_j)$（PageRank on words）
- LDA: $\theta_d \sim \mathrm{Dir}(\alpha)$，词 → 主题 → 重要性

**一眼记住**：
TextRank = 共现图 + PageRank；LDA = 主题模型，词的主题分布度量重要性。

---

### Q17: 泛化因素（学习率）

**原题**：
> 在构建大模型时，以下哪些因素会影响其泛化能力？
> - A. 训练数据的规模
> - B. 正则化技术的应用
> - C. 数据集的多样性
> - D. 模型参数的数量
> - E. 使用更大的学习率

**正确答案**：A、B、C、D

**易错点**：
误选 E（学习率）。学习率影响优化路径但不稳定提升泛化。

**核心公式/结论**：
泛化三件套：**数据量 + 数据多样性 + 正则化**

**一眼记住**：
学习率 = "怎么走到山谷"；泛化 = "模型能否适应新样本"。

---

### Q18: Causal Mask（目的 vs 局部性）

**原题**：
> 关于 Causal Mask 的作用，正确的是
> - A. 是一个上三角为负无穷的矩阵
> - B. 防止解码器在训练时看到未来的信息
> - C. 仅用于文本生成任务
> - D. 用于增强自注意力模型的局部性

**正确答案**：A、B

**易错点**：
误选 D，混淆"方向约束"（Causal Mask）与"距离约束"（局部性增强）。

**核心公式/结论**：
- Causal Mask: 上三角 $-\infty$，禁止看未来（方向约束）
- 局部性增强: 滑动窗口 / 距离衰减（距离约束）

**一眼记住**：
Causal Mask ≠ 局部性；Causal = 禁止看未来，局部 = 只看附近。

---

### Q19: 中心极限定理（标准化形式）

**原题**：
> 设 $X_i$ 独立同分布，以下哪些是正确的渐近分布？
> - A. $\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}\xrightarrow{d}N(0,1)$ ✅
> - B. $\sqrt{n}(\bar{X}_n-\mu)/\sigma \xrightarrow{d} N(0,1)$ ✅
> - C. $\frac{S_n - n\mu}{\sigma} \xrightarrow{d} N(0,1)$ ❌
> - D. $X_n \xrightarrow{d} N(\mu,\sigma^2)$ ❌

**正确答案**：A、B

**易错点**：
C 少了 $\sqrt{n}$（应除 $\sigma\sqrt{n}$）；D 错在 $X_n$ 分布不变。

**核心公式/结论**：
$$\frac{S_n-n\mu}{\sigma\sqrt{n}}\xrightarrow{d}N(0,1),\quad \mathrm{Var}(S_n)=n\sigma^2$$

**一眼记住**：
均值波动 $\sim 1/\sqrt{n}$，标准化必有 $\sqrt{n}$。

---

<details>
<summary>📎 追问详情（原始解答内容）</summary>

### (6) 全预测负例：F1 为啥"算不出来"？AUC 又怎么还能算？

**F1（严格数学上确实会卡住）**

全预测负例时（假设数据里确实有正例）：
* $TP=0,\ FP=0,\ FN>0$
* Precision $P=\dfrac{TP}{TP+FP}=\dfrac{0}{0}$ **未定义**
* Recall $R=\dfrac{TP}{TP+FN}=0$

F1：
$$F1=\frac{2PR}{P+R}$$
因为 $P$ 未定义，所以**严格数学意义下 F1 不可定义**。

> **工程实现记忆**：大多数库会把"没有预测正例"( $TP+FP=0$ ) 时的 precision 置 0，于是 F1 也输出 0。

**AUC-ROC（为什么还能算）**

AUC 不依赖你选的阈值（比如 0.5），它看的是**分数排序能力**。

最常用等价定义：
$$\text{AUC}=\Pr\big(s(x^+)>s(x^-)\big)\ +\ \frac12\Pr\big(s(x^+)=s(x^-)\big)$$

**物理直觉/类比**：AUC 像"把正负样本当两类粒子，分数是能量"，AUC 就是"正粒子能量更高"的概率。

---

### (9) 条件概率里为什么会出现 12

已知联合密度：$f(x,y)=6x^2,\quad 0<|x|<y<1$

边缘密度：
$$f_Y(y)=\int_{-y}^{y}6x^2\,dx=4y^3$$

代 $y=0.5$：$f_Y(0.5)=0.5$

条件密度：
$$f_{X|Y}(x|0.5)=\frac{6x^2}{0.5}=12x^2$$

**记忆法**：条件密度 = "这条横截面" ÷ "这条横截面的面积"。

---

### (15) 为什么说 0 的代数重数"至少"是 2？

**几何重数**：$g_0=\dim\ker(A)=3-1=2$

**代数重数**：$g_\lambda \le a_\lambda$，所以 $a_0 \ge 2$

**两个例子**：
* $A=\mathrm{diag}(1,0,0)$：$a_0=2$
* $A=\begin{pmatrix}0&1&0\\0&0&0\\0&0&0\end{pmatrix}$（幂零）：$a_0=3$

---

### (16) LDA 举个最简单例子 + TextRank 是啥？

**LDA**：每篇文档有"主题配比"$\theta_d$，每个词先抽主题再从该主题词分布里抽词。

**TextRank**：
1. 把候选词当图节点
2. 窗口内共现就连边
3. 跑 PageRank 迭代打分
4. 取 Top-K 词

---

### (17) 学习率与泛化"无关"吗？

更准确说法：学习率不是"稳定提升泛化"的因素，但会间接影响。

泛化更主要因素：数据规模、数据多样性、正则化、模型容量。

---

### (18) "增强局部性"怎么做？

**Causal Mask 的目的不是增强局部性**，而是**禁止看未来**。

真正增强局部性的做法：滑动窗口、块稀疏 attention、相对位置偏置、Conv + Attention。

---

### (19) 四个选项逐条判定

* A ✅：CLT 标准形式
* B ✅：与 A 等价
* C ❌：少了 $\sqrt n$
* D ❌：$X_n$ 分布不变

---

### AUC-ROC 的横坐标是 FPR

ROC 曲线：纵轴 TPR，横轴 FPR。

全预测负例时只是某一阈值的一个点 (0,0)，但 AUC 扫所有阈值。

---

### rank=1 但所有特征值都是 0 合理吗？

完全合理。幂零矩阵：$A\neq 0$ 但 $A^k=0$，特征值全为 0。

---

### TextRank 最后是找"边最多"的词吗？

不是。TextRank 用 PageRank/特征向量中心性，一个词重要还因为它连接的词也重要。

---

### FPR 别名

* False alarm rate（误报率）
* Fall-out
* Type I error rate / α
* 1 - Specificity

</details>

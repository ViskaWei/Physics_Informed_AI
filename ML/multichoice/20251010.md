# 2025-10-10 AI 方向题

## 一、单选题

### 1. 在大型语言模型的三阶段训练流程中，哪一个阶段的主要目标是让模型从一个只会“续写”文本的基座模型，转变为一个能理解并遵循人类指令格式的“对话助手”？

* A. 预训练 (Pre-training)
* B. 有监督微调 (Supervised Fine-Tuning, SFT)
* C. 奖励模型训练 (Reward Model Training)
* D. 基于人类反馈的强化学习 (RLHF)

**答案：B**

SFT 使用指令-回复的数据对，让仅会续写的基座模型学会按人类指令格式理解与作答。

---

### 2. 若输入序列长度 $s$，模型层数 $L$，隐藏层维度 $d$，当 $s \gg d$ 时，prefill 阶段计算量为

* A. $O(L*s*s*d)$
* B. $O(L*s*d*d)$
* C. $O(L*s*d)$
* D. $O(s*d+L*d*d)$

**答案：A**

Prefill 阶段主要为自注意力计算，复杂度与序列长度平方成正比：
$$O(L \times s^2 \times d)$$

---

### 3. 对于 $3\times224\times224$ 输入，卷积核 $7\times7$、$stride=2$、$padding=3$、$out_channels=64$，输出特征图的空间尺寸为？

* A. $128\times128$
* B. $256\times256$
* C. $112\times112$
* D. $224\times224$

**答案：C**

$$\left\lfloor \frac{224+2\times3-7}{2} \right\rfloor + 1 = 112$$
注：卷积核数量只影响通道数，不影响空间尺寸。

---

### 4. 连续掷一枚均匀硬币，首次出现 “正反” 序列（即一次正面后立刻反面）所需的期望掷币次数为

* A. 8
* B. 6
* C. 4
* D. 5

**答案：C**

用马尔可夫状态（初始/刚出正/刚出反）建立期望递推方程，先求 $E_1=2$、$E_2=4$，回代得 $E_0=4$。

---

### 5. 在探索两个连续变量的非线性关系时，应当采用以下哪种图作为可视化工具：

* A. 箱线图
* B. 柱状图
* C. 折线图
* D. 散点图

**答案：D**

散点图直接展示两连续变量的配对数据，可观察非线性关系与离群点。

---

### 6. 在自然语言处理的 N-gram 模型中，我们计算一个词出现的概率时会依赖于它前面的 N-1 个词，即 $P(w_i \mid w_{i-1}, w_{i-2}, \dots, w_{i-N+1})$，给定之前全部词的条件概率，等同于给定前 N 个词的条件概率。这种建模思想与完全历史依赖的条件概率形成对比，请问这种建模思想与概率论中的哪个概念最相关？

* A. 大数定律
* B. 贝叶斯定理
* C. 马尔可夫假设
* D. 全概率公式

**答案：C**

N-gram 假设当前词仅依赖前 $N-1$ 个词，即为马尔可夫假设。

---

### 7. 有以下伪代码，定义大小3×3×3,stride=2,padding=1的3D卷积核，计算卷积操作后输出tensor的形状

```text
1. conv = Convolution3D(in_channel=24,out_channel=32,kernel_size(3,3,3),stride=2, padding-1)
2. input tensor =create random tensor(batch=16,in_hannel=24,50,24,18)
3. output tensor = conv(input tensor)
```

* A. $16\times32\times23\times10\times7$
* B. $16\times32\times25\times12\times9$
* C. $16\times32\times26\times13\times10$
* D. $16\times32\times24\times11\times8$

**答案：B**

3D 卷积输出尺寸：
$$L_{\text{out}}=\left\lfloor \frac{L_{\text{in}}+2p-(k-1)-1}{s}+1\right\rfloor$$
（其中 $k=3,p=1,s=2$）对输入尺寸 50, 24, 18 得 25, 12, 9；批量与输出通道分别为 16 和 32。

---

### 8. 已知矩阵

$$
A=\begin{pmatrix}
1&0&-1\
2&-1&1\
-1&2&-5
\end{pmatrix}
$$
若下三角可逆矩阵 $P$ 和上三角可逆矩阵 $Q$，使得 $PAQ$ 为对角矩阵，则 $P,Q$ 可以分别取（）

* A. $\begin{pmatrix}1&0&0\0&1&0\0&0&1\end{pmatrix}$，$\begin{pmatrix}1&0&1\0&1&3\0&0&1\end{pmatrix}$
* B. $\begin{pmatrix}1&0&0\2&-1&0\-3&2&1\end{pmatrix}$，$\begin{pmatrix}1&0&0\0&1&0\0&0&1\end{pmatrix}$
* C. $\begin{pmatrix}1&0&0\0&1&0\1&3&1\end{pmatrix}$，$\begin{pmatrix}1&2&-3\0&-1&2\0&0&1\end{pmatrix}$
* D. $\begin{pmatrix}1&0&0\2&-1&0\-3&2&1\end{pmatrix}$，$\begin{pmatrix}1&0&1\0&1&3\0&0&1\end{pmatrix}$

**答案：D**

通过 LU 分解思想计算，可得 $P,Q$ 分别对应选项 D。

---

### 9. 给定以下6个二维数据点及其类别，其坐标表示为 $(x,y)$，类别标签用 [+] 表示正类，[−] 表示负类：

A(0,0)[+]，B(1,0)[+]，C(0,1)[−]，D(3,3)[−]，E(4,5)[−]，F(3,4)[−]
使用4-近邻算法（4-NN）和欧氏距离对样本 G(2,2) 进行分类预测。若采用多数投票法，则其预测类别是什么?

* A. 预测为负类（−）
* B. 正类（+）与负类（−）各2票，无法形成多数
* C. 预测为正类（+）
* D. 所有4个邻居到 G 的距离相等，必须使用加权投票

**答案：A**

到 G(2,2) 的最近四个点：D(3,3)[-]、B(1,0)[+]、C(0,1)[-]、F(3,4)[-]；投票结果：负类 3 票，正类 1 票。

---

### 10. 令 ${x_i : i=0,\dots,n}$ 是 $[a,b]$ 中的任意 $(n+1)$ 个不同点，且 $f\in C[a,b]$。那么，存在唯一个次数不超过 $n$ 的插值多项式 $p_n$，请选择下述正确的表达式，其中 $\omega(x)=(x-x_0)\cdots(x-x_n)$。

* A. $p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega(x_i)}$
* B. $p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega(x)}$
* C. $p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega'(x_i)}$
* D. $p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega'(x)}$

**答案：C**

拉格朗日插值公式为：
$$p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega'(x_i)}$$

---

### 11. 设

$$
A=\begin{pmatrix}-11&4\-30&11\end{pmatrix}
$$
则 $(A+E)(E-A+A^2-A^3+A^4-A^5+A^6)=(\ \ )$（注：$E$ 为单位矩阵）

* A. $A=\begin{pmatrix}12&4\30&10\end{pmatrix}$
* B. $A=\begin{pmatrix}-12&4\-30&10\end{pmatrix}$
* C. $A=\begin{pmatrix}10&4\30&12\end{pmatrix}$
* D. $A=\begin{pmatrix}-10&4\-30&12\end{pmatrix}$

**答案：D**

$$(I+A)\sum_{k=0}^{6}(-A)^k=I+(-A)^7=I+A^7$$
由 $A^2=I$，故 $A^7=A$，结果为 $I+A$。

---

### 12. 某电商公司构建推荐系统时，需对用户的 “月均消费金额” 特征进行预处理。该特征通常呈现严重右偏（即多数用户消费较低，少数用户消费极高）。考虑一个模拟的用户月均消费数据集，取值分别为（单位：元）[9, 99, 999, 9999]，整体分布右偏。为改善特征分布的对称性（即降低偏度），数据科学家考虑以下三种变换方式：

变换 1：最小 - 最大缩放（Min-Max Scaling）$x'=\frac{x-x_{min}}{x_{max}-x_{min}}$
变换 2：Z-score 标准化（Z-score Standardization）$x'=\frac{x-\mu}{\sigma}$
变换 3：对数变换（Log Transformation）$x'=\log_{10}(x+1)$
下列哪一种变换后的数据分布最对称（即偏度最小）？

* A. 变换 1
* B. 变换 3
* C. 变换 2
* D. 三种变换对称性相同

**答案：B**

对数变换能有效缓解右偏分布，使分布更接近对称。

---

### 13. 已知变量 X 和 Y 的两组数据分布如下：

组A：(1,20)、(2,18)、(3,16)、(4,14)、(5,12)、(6,10)、(7,8)、(8,6)、(9,4)、(10,2)
组B：组A所有数据 + 额外数据点 (11,20)
若两组数据的相关系数分别为 $r_1$（组A）和 $r_2$（组B），则下列关系正确的是（ ）

* A. $0>r_1>r_2$
* B. $r_1>r_2>0$
* C. $0>r_2>r_1$
* D. $r_1>r_2>0$

**答案：C**

组 A 点严格在直线 $y=22-2x$ 上，故 $r_1=-1$；加入离群点 (11,20) 使相关性减弱：
$$-1=r_1<r_2<0$$

---

### 14. 在一个基于 Transformer 的大型语言模型中，Tokenizer（如 BPE 分词器）和 Embedding 层是处理输入文本的最初两个步骤。关于这两个组件的功能和关系，以下哪个描述最为精确？

* A. Tokenizer 负责将文本分割成有意义的语义单元（tokens），而 Embedding 层则进一步将考虑不同 Token 之间的关联并将 Token 间的语义关系编码到高维空间中。
* B. Tokenizer 是一个基于固定规则或统计学习得到的、独立于下游模型的词汇映射系统；而 Embedding 层是一个包含可训练参数的神经网络层，其参数会在模型训练过程中不断优化。
* C. Tokenizer 的唯一目的是压缩文本以减少计算量，而 Embedding 层则负责将压缩后的 ID 恢复成密集的向量表示。
* D. Tokenizer 和 Embedding 层共同决定了模型的词汇量大小，并且在模型训练完成后，两者都可以被轻易替换以适应新的语言或领域。

**答案：B**

Tokenizer：负责词到 ID 的静态映射；Embedding 层：可训练参数层，用于学习词向量。

---

### 15. 如图所示的超平面 $Ax=b$ 到圆心距离等价于欠定方程组 $Ax=b$ 的最小范数解。

**问题如下：已知线性方程组 $y=Ax$ 中，$A\in\mathbb{R}^{m\times n}$ 是行满秩的胖矩阵（即 $m<n$），其解集为 ${x_p+z\mid z\in\mathcal{N}(A)}$（$x_p$ 为特解，$\mathcal{N}(A)$ 为 $A$ 的零空间）。若要在解集中找到二范数最小的解，该解为：**

* A. $x=(A^TA)^{-1}y$
* B. $x=(A^TA)^{-1}A^Ty$
* C. $x=A^Ty$
* D. $x=A^T(AA^T)^{-1}y$

**答案：D**

对行满秩的胖矩阵（欠定）最小二范数解为伪逆解：
$$x=A^+y=A^T(AA^T)^{-1}y$$

---

## 二、多选题

### 16. 最大池化 (Max Pooling) 在卷积神经网络中是一种常见的下采样操作，该操作的主要优点包括

* A. 能提供一定程度的平移不变性，使模型对输入的小变化不敏感
* B. 能精准保留输入特征的空间位置信息
* C. 能够有效抑制图像中的噪声
* D. 能保留图像中局部极值，用于提取显著的特征（如纹理、边缘）
* E. 能够显著减少特征图的空间维度和后续计算量

**答案：A、C、D、E**

---

### 17. 回归任务中可用的评估指标有？

* A. 均方误差（MSE）
* B. 均方根误差（RMSE）
* C. 平均绝对误差（MAE）
* D. $R^2$ 分数

**答案：A、B、C、D**

MSE/RMSE 强调较大误差惩罚；MAE 较稳健；$R^2$ 衡量模型解释方差的比例。

---

### 18. 在度量文本嵌入的相似度时，适合的处理方式包括哪些？

* A. 应用 PCA 降维
* B. 使用曼哈顿距离
* C. 使用点积并归一化
* D. 计算余弦相似度

**答案：C、D**

$$\cos(\theta)=\frac{x^\top y}{\lVert x\rVert\lVert y\rVert}$$

---

### 19. 对实矩阵 $X\in\mathbb{R}^{m\times d}$ 的奇异值分解 $X=U\Sigma V^T$，下列说法正确的是：

* A. $\Sigma$ 的对角元素按非递增顺序排列
* B. $\Sigma$ 的非零奇异值等于 $XX^T$ 的非零特征值的平方根
* C. $rank(X)$ 等于 $\Sigma$ 的非零对角元素个数
* D. $U, V$ 均为正交矩阵
* E. $U$ 的前 $r(=col(X))$ 列向量给出了一组 $X$ 列空间的正交基

**答案：A、B、C、D、E**

---

### 20. 以下模型与结构匹配正确的是

* A. T5 → Encoder-Decoder
* B. BART → Encoder-Decoder
* C. BERT → Decoder-only
* D. GPT-4 → Decoder-only

**答案：A、B、D**

下面按你标的题号逐个回答（都基于这套题的题面/答案区）：

---

## （1）C、D 的目标是什么？

这题的 A/B/C/D 分别是：预训练 / SFT / 奖励模型训练 / RLHF。

### C. 奖励模型训练（Reward Model, RM Training）目标

把“人类偏好”学成一个**可打分的函数**：
给定同一个 prompt 下的两条回复 (y^{(a)},y^{(b)})，人类选更喜欢的一条。奖励模型输出标量 (r(x,y))，训练让“更喜欢的那条”得分更高。

常见的对比/排序损失（Bradley–Terry）：
[
\Pr(y^{(a)} \succ y^{(b)})=\sigma!\big(r(x,y^{(a)})-r(x,y^{(b)})\big)
]
最大化偏好标签的对数似然即可。

> 一句话：**C 是“学会打分”**（学人类偏好打分器）。

### D. RLHF（用 RM 做奖励去强化学习）目标

在 RM 给分 (r(x,y)) 的基础上，**直接优化生成策略 (\pi_\theta(y|x))**，让模型更倾向产出高分回复，同时别偏离 SFT 太远（保持语言质量/稳定性）。

常见目标（带 KL 约束）：
[
\max_\theta\ \mathbb{E}*{y\sim\pi*\theta(\cdot|x)}[r(x,y)]-\beta,\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})
]

> 一句话：**D 是“按打分器去改行为”**（对齐风格/安全/偏好），常用 PPO 等。

---

## （4）有简单口算的公式吗？（“正反”= HT）

这题要等到首次出现 **HT**（一次正面后立刻反面），公平硬币期望是 **4**。

**口算记忆法（长度2的模式）**：

* **“异号”(HT 或 TH)**：期望 **4**
* **“同号”(HH 或 TT)**：期望 **6**（因为有重叠，会更慢）

超短推导（你只要记住这个递推就行）：
设 (E_0)=从头开始期望；(E_H)=“刚掷出H”的期望
[
E_H=1+\tfrac12\cdot0+\tfrac12 E_H\Rightarrow E_H=2
]
[
E_0=1+\tfrac12 E_H+\tfrac12 E_0\Rightarrow E_0=4
]

---

## （5）为什么不是 B /（你这题实际错在选了 C）

题 5 问“探索两个连续变量的**非线性关系**用什么图”。正确是 **散点图 D**。

为什么不是折线图/柱状图/箱线图？

* **散点图**：每个点就是一对 ((x,y))，天然展示相关/非线性/离群点。
* **折线图**：默认你要强调“顺序/时间/路径连接”，会**误导**读者以为有序列关系。
* **柱状/箱线**：更偏单变量分布或分类对比，不适合直接看两连续变量函数形态。

**一眼记住**：

> 两个连续变量看关系：**先散点**；有时间顺序才折线。

---

## （6）为什么不是 B？Markov 不是“无记忆、只看 (w_{i-1})”吗？

你记得的是**一阶马尔可夫**（只依赖上一个状态）。
但“马尔可夫假设”的核心是：

> **未来只依赖“当前状态”，与更早历史无关。**

关键点：**“状态怎么定义”是你选的。**

* N-gram 用的是 ((N-1)) 阶：
  [
  P(w_i\mid w_{<i})\approx P(w_i\mid w_{i-1},\dots,w_{i-N+1})
  ]
  这仍然是马尔可夫，只不过**状态 = 最近 (N-1) 个词**（把“记忆”打包进状态里）。

所以不是 B（贝叶斯定理），而是 C（马尔可夫假设）。

**一眼记住**：

> “无记忆”不是“只看前1个”，而是“只看**你定义的状态**”。

---

## （8）用 LR（=LU）演示：每一步清楚写出来

题 8 要找下三角可逆 (P) 和上三角可逆 (Q)，使 (PAQ) 变对角。

矩阵
[
A=\begin{pmatrix}
1&0&-1\
2&-1&1\
-1&2&-5
\end{pmatrix}
]

### Step 1：先做列变换（右乘 (Q)）

观察三列：
[
c_1=\begin{pmatrix}1\2\-1\end{pmatrix},\
c_2=\begin{pmatrix}0\-1\2\end{pmatrix},\
c_3=\begin{pmatrix}-1\1\-5\end{pmatrix}
]
你会发现：
[
c_3+c_1+3c_2=\begin{pmatrix}0\0\0\end{pmatrix}
]
也就是 **(c_3=-c_1-3c_2)**。

因此做列操作：

> (C_3 \leftarrow C_3 + C_1 + 3C_2)

这等价于右乘
[
Q=\begin{pmatrix}
1&0&1\
0&1&3\
0&0&1
\end{pmatrix}
]
得到
[
A Q=
\begin{pmatrix}
1&0&0\
2&-1&0\
-1&2&0
\end{pmatrix}
]

### Step 2：再做行变换（左乘 (P)）把前两列化成单位阵

对 (AQ) 做行操作：

1. (R_2 \leftarrow 2R_1 - R_2)
   [
   (2,-1,0)\ \mapsto\ 2(1,0,0)-(2,-1,0)=(0,1,0)
   ]

2. (R_3 \leftarrow R_3 + R_1 - 2R_2)（这里的 (R_2) 是更新后的）
   [
   (-1,2,0)+(1,0,0)-2(0,1,0)=(0,0,0)
   ]

最终：
[
P(AQ)=
\begin{pmatrix}
1&0&0\
0&1&0\
0&0&0
\end{pmatrix}
]
对应的
[
P=\begin{pmatrix}
1&0&0\
2&-1&0\
-3&2&1
\end{pmatrix}
]
（这就是答案 D 的 (P,Q)）

**一眼记住**：

> 右乘上三角 (Q) = “做列消元”；左乘下三角 (P) = “做行消元”。

---

## （10）记不住拉格朗日插值？

题干给了
[
\omega(x)=\prod_{j=0}^n (x-x_j)
]

正确式子（选 C）：
[
p_n(x)=\sum_{i=0}^n y_i\frac{\omega(x)}{(x-x_i)\omega'(x_i)}
]

**最强记忆点只有一个**：

> 分母一定要是“把 ((x-x_i)) 去掉后，在 (x_i) 处的常数”，也就是 (\omega'(x_i))。

因为：
[
\omega'(x_i)=\prod_{j\ne i}(x_i-x_j)
]
所以你也可以把它记成：
[
L_i(x)=\prod_{j\ne i}\frac{x-x_j}{x_i-x_j}
]

---

## （11）矩阵题不会

题 11：
[
(A+I)(I-A+A^2-A^3+A^4-A^5+A^6)
]
把括号看成等比和：令 (r=-A)
[
S=\sum_{k=0}^6 (-A)^k
]
则
[
(I+A)S=(I-(-A))S=I-(-A)^7=I+A^7
]

接着算 (A^2)（题里矩阵 (A=\begin{pmatrix}-11&4\-30&11\end{pmatrix})）：
[
A^2=
\begin{pmatrix}
121-120 & -44+44\
330-330 & -120+121
\end{pmatrix}
=============

\begin{pmatrix}
1&0\0&1
\end{pmatrix}=I
]
所以
[
A^7=A^{2\cdot3+1}=(A^2)^3A=IA=A
]
最终：
[
I+A=
\begin{pmatrix}
-10&4\-30&12
\end{pmatrix}
]
对应选 **D**。

---

## （13）相关系数是 (R^2) 吗？为什么小于 0？

题 13 问的是**相关系数 (r)**（Pearson correlation），不是 (R^2)。

* 组 A 的点严格在直线 (y=22-2x) 上，是**完美负相关**：
  [
  r_1=-1
  ]
  所以它当然 **小于 0**（因为斜率是负的）。

* 如果你做简单线性回归（带截距），那 (R^2=r^2)，因此组 A 的
  [
  R^2=1
  ]
  这和 (r=-1) **并不矛盾**。

加入离群点后，“没那么负相关”，所以
[
-1=r_1<r_2<0
]
题目选 C。

---

## （14）A 错在哪？

A 说 embedding 会“进一步考虑不同 token 间关联并编码语义关系”。
但 **Embedding 层本质就是 lookup 表**：每个 token id 映射到一个向量 (e_i)，彼此之间**不交互**。

token 之间的关系（上下文）主要由后面的 Transformer 层（自注意力等）建模。

**一眼记住**：

> Tokenizer = 文本 (\to) ID；Embedding = ID (\to) 向量；**关系在注意力里**，不在 embedding lookup 里。

---

## （15）不会：欠定 (m<n) 的最小二范数解

问题：(\min |x|_2) s.t. (Ax=y)，且 (A) **行满秩**（(m<n)，“胖矩阵”）。

拉格朗日：
[
\min_x \frac12|x|^2 + \lambda^\top(Ax-y)
]
对 (x) 求导：
[
x + A^\top\lambda=0 \Rightarrow x=-A^\top\lambda
]
代回约束：
[
A(-A^\top\lambda)=y \Rightarrow -(AA^\top)\lambda=y
\Rightarrow \lambda=-(AA^\top)^{-1}y
]
因此
[
x=A^\top(AA^\top)^{-1}y
]
选 **D**。

**一眼记住（胖矩阵最小范数）**：

> (m<n)：用 (AA^\top) 可逆
> [
> A^+=A^\top(AA^\top)^{-1}
> ]

---

## ✅ 错题整理（结构化）

> 整理日期: 2026-01-04

---

### 📊 知识点标注表

| 题号 | 知识点 Tag1 | 知识点 Tag2 | 是否错题 |
|------|-------------|-------------|----------|
| Q1 | 大模型(LLM) | 强化学习与对齐 | Unknown |
| Q2 | 注意力与Transformer | - | Unknown |
| Q3 | CNN | - | Unknown |
| Q4 | 概率统计 | - | Unknown |
| Q5 | 其他(可视化) | - | **✅错题** |
| Q6 | 概率统计 | - | Unknown |
| Q7 | CNN | - | Unknown |
| Q8 | 线性代数 | - | Unknown |
| Q9 | 经典机器学习 | - | Unknown |
| Q10 | 优化与数值方法 | - | Unknown |
| Q11 | 线性代数 | - | Unknown |
| Q12 | 经典机器学习 | - | Unknown |
| Q13 | 概率统计 | - | Unknown |
| Q14 | 大模型(LLM) | - | Unknown |
| Q15 | 线性代数 | - | Unknown |
| Q16 | CNN | - | Unknown |
| Q17 | 评估指标 | - | Unknown |
| Q18 | 经典机器学习 | - | Unknown |
| Q19 | 线性代数 | - | Unknown |
| Q20 | 大模型(LLM) | - | Unknown |

---

### 📈 知识点掌握度统计

| 知识点 | 总题数 | 错题数 | 正确率 |
|--------|--------|--------|--------|
| 大模型(LLM) | 4 | 0 | 100% |
| 线性代数 | 4 | 0 | 100% |
| 概率统计 | 3 | 0 | 100% |
| CNN | 3 | 0 | 100% |
| 经典机器学习 | 3 | 0 | 100% |
| 注意力与Transformer | 1 | 0 | 100% |
| 优化与数值方法 | 1 | 0 | 100% |
| 评估指标 | 1 | 0 | 100% |
| 其他(可视化) | 1 | 1 | 0% |

> ⚠️ 注：大部分题目错误状态为 Unknown（用户未明确标记做错），仅 Q5 明确做错

---

### 🔥 薄弱环节 Top-3（2025-10-10）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 其他(可视化) | 1/1 | 0% | 样本少 |
| 2 | - | - | - | 其他知识点无明确错题 |
| 3 | - | - | - | - |

---

## Q5: 非线性关系可视化

**原题**：
> 在探索两个连续变量的非线性关系时，应当采用以下哪种图作为可视化工具：
> * A. 箱线图
> * B. 柱状图
> * C. 折线图
> * D. 散点图

**正确答案**：D（散点图）

**易错点**：
误选 C（折线图）。折线图强调"顺序/时间/路径连接"，会误导读者以为有序列关系。

**核心公式/结论**：
- **散点图**：每个点就是一对 $(x,y)$，天然展示相关/非线性/离群点
- **折线图**：有时间/序列顺序才用
- **柱状/箱线**：单变量分布或分类对比

**一眼记住**：
> 两个连续变量看关系：**先散点**；有时间顺序才折线。

---

## 📚 核心知识点速查

### 1️⃣ 可视化选择
| 场景 | 图表类型 |
|------|----------|
| 两连续变量关系（含非线性） | 散点图 |
| 时间序列/有序数据 | 折线图 |
| 分类变量对比 | 柱状图 |
| 单变量分布/离群点 | 箱线图 |

### 2️⃣ 卷积输出尺寸公式
$$H_{out} = \left\lfloor \frac{H + 2P - K}{S} \right\rfloor + 1$$

### 3️⃣ "正反"序列期望掷币次数
- **异号** (HT 或 TH)：期望 **4**
- **同号** (HH 或 TT)：期望 **6**

口算递推：
$$E_H = 1 + \frac{1}{2} \cdot 0 + \frac{1}{2} E_H \Rightarrow E_H = 2$$
$$E_0 = 1 + \frac{1}{2} E_H + \frac{1}{2} E_0 \Rightarrow E_0 = 4$$

### 4️⃣ N-gram 与马尔可夫假设
- **马尔可夫假设**核心：未来只依赖"当前状态"，与更早历史无关
- N-gram 把"最近 N-1 个词"打包成状态，仍是马尔可夫
- **一眼记住**：无记忆 ≠ 只看前1个，而是只看你定义的状态

### 5️⃣ 欠定方程最小二范数解（胖矩阵 $m < n$）
$$x = A^+(y) = A^T(AA^T)^{-1}y$$

### 6️⃣ 拉格朗日插值公式
$$p_n(x) = \sum_{i=0}^n y_i \frac{\omega(x)}{(x-x_i)\omega'(x_i)}$$

其中 $\omega(x) = \prod_{j=0}^n (x-x_j)$，$\omega'(x_i) = \prod_{j \ne i}(x_i - x_j)$

### 7️⃣ 矩阵幂运算技巧
若 $A^2 = I$，则 $A^7 = A^{2 \cdot 3 + 1} = (A^2)^3 \cdot A = A$

等比求和公式：$(I+A) \sum_{k=0}^n (-A)^k = I - (-A)^{n+1}$

### 8️⃣ 相关系数 vs $R^2$
- **相关系数 $r$**：可正可负，$-1 \le r \le 1$
- **$R^2 = r^2$**：永远非负
- 完美负相关：$r = -1$，但 $R^2 = 1$

### 9️⃣ Tokenizer vs Embedding
- **Tokenizer**：文本 → ID（静态映射，基于规则/统计学习）
- **Embedding**：ID → 向量（可训练参数层）
- **关系建模**：在后续 Transformer 的自注意力层，不在 Embedding lookup

---

<details>
<summary>📎 追问详情（原始问答）</summary>

### （1）C、D 的目标是什么？

**C. 奖励模型训练**：把"人类偏好"学成可打分的函数
$$\Pr(y^{(a)} \succ y^{(b)}) = \sigma(r(x,y^{(a)}) - r(x,y^{(b)}))$$
一句话：**C 是"学会打分"**

**D. RLHF**：用 RM 给分的基础上，直接优化生成策略
$$\max_\theta \mathbb{E}_{y \sim \pi_\theta}[r(x,y)] - \beta \cdot \mathrm{KL}(\pi_\theta | \pi_{\text{ref}})$$
一句话：**D 是"按打分器去改行为"**

---

### （4）正反序列口算公式

设 $E_0$ = 从头开始期望；$E_H$ = "刚掷出H"的期望
$$E_H = 1 + \frac{1}{2} \cdot 0 + \frac{1}{2} E_H \Rightarrow E_H = 2$$
$$E_0 = 1 + \frac{1}{2} E_H + \frac{1}{2} E_0 \Rightarrow E_0 = 4$$

---

### （6）N-gram 与马尔可夫

"马尔可夫假设"核心：未来只依赖"当前状态"。关键是"状态怎么定义"。N-gram 用 (N-1) 阶，把"最近 N-1 个词"打包成状态。

---

### （8）LU 分解演示

矩阵 $A = \begin{pmatrix} 1 & 0 & -1 \\ 2 & -1 & 1 \\ -1 & 2 & -5 \end{pmatrix}$

**Step 1**：列变换 $C_3 \leftarrow C_3 + C_1 + 3C_2$，右乘 $Q = \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{pmatrix}$

**Step 2**：行变换消元，左乘 $P = \begin{pmatrix} 1 & 0 & 0 \\ 2 & -1 & 0 \\ -3 & 2 & 1 \end{pmatrix}$

**一眼记住**：右乘上三角 = 列消元；左乘下三角 = 行消元

---

### （10）拉格朗日插值

$$p_n(x) = \sum_{i=0}^n y_i \frac{\omega(x)}{(x-x_i)\omega'(x_i)}$$

记忆点：分母是"把 $(x-x_i)$ 去掉后在 $x_i$ 处的常数"

---

### （11）矩阵幂

$(I+A) \sum_{k=0}^6 (-A)^k = I - (-A)^7 = I + A^7$

计算 $A^2 = I$，所以 $A^7 = A$，最终 $I + A = \begin{pmatrix} -10 & 4 \\ -30 & 12 \end{pmatrix}$

---

### （13）相关系数

组 A 严格在直线 $y = 22 - 2x$ 上，完美负相关 $r_1 = -1$。
若做线性回归，$R^2 = r^2 = 1$，与 $r = -1$ 不矛盾。

---

### （14）Tokenizer vs Embedding

Embedding 层本质是 lookup 表，每个 token id 映射到向量，彼此不交互。
token 间关系由后面的 Transformer 自注意力层建模。

---

### （15）欠定方程最小二范数解

拉格朗日：$\min_x \frac{1}{2}\|x\|^2 + \lambda^T(Ax - y)$

对 $x$ 求导：$x + A^T\lambda = 0 \Rightarrow x = -A^T\lambda$

代回约束：$A(-A^T\lambda) = y \Rightarrow \lambda = -(AA^T)^{-1}y$

因此：$x = A^T(AA^T)^{-1}y$

</details>

# 2025-10-22 AI 方向题

## 一、单选题

### 1. 在深度学习中，若模型参数的更新依赖数值不稳定的矩阵求逆操作（如 $(X^T X)^{-1}$），则可能导致：

- A. 模型过拟合
- B. 参数估计误差大
- C. 模型欠拟合
- D. 训练加速过快

**答案：B**

数值不稳定（如 $(X^T X)^{-1}$ 条件数很大）会放大微小扰动，导致参数估计方差与误差增大。

---

### 2. 在构建一个用于预测用户是否流失的机器学习模型时，某数据科学家从原始数据中提取了 15 个特征，包括年龄、月消费金额、登录频率、服务投诉次数等。为了提升模型性能并减少过拟合，他决定使用递归特征消除（Recursive Feature Elimination, RFE）方法进行特征选择。已知他使用了一个训练好的逻辑回归模型作为评估器，并设定最终保留 5 个特征。以下关于 RFE 过程的描述中，哪一项是正确的？

- A. RFE 通过反复训练模型，每次移除当前模型中系数绝对值最小的特征，逐步缩小特征集，最终得到一个包含 5 个特征的子集。
- B. RFE 基于特征的方差阈值逐步剔除低方差特征，直到保留指定数量为止。
- C. RFE 根据各特征在逻辑回归中的 p 值显著性进行排序，每次移除最不显著的特征，直到保留 5 个具有统计显著性的特征。
- D. RFE 计算每个特征与目标变量之间的皮尔逊相关系数，保留相关性最高的 5 个特征。

**答案：A**

RFE 每轮依据评估器的重要性（如系数绝对值）剔除最弱特征，迭代直到保留指定数量。

---

### 3. 在自然语言处理中，比较两个词嵌入向量 $u=\begin{bmatrix}0.8\\0.6\end{bmatrix}$、$v=\begin{bmatrix}0.5\\0.866\end{bmatrix}$ 的语义相似性。以下结论正确的是？

- A. 点积=0.8，表明语义高度相似
- B. 点积结果受向量模影响，余弦相似度更可靠
- C. 余弦相似度≈0.99，表明语义几乎相同
- D. 向量 v 的模更大，因此更重要

**答案：B**

点积受向量模长影响；余弦相似度通过归一化更能反映方向相似性：
$$
\cos(u,v)=\frac{u^\top v}{\|u\|\|v\|}
$$

---

### 4. 当把 ReLU 换成 GELU 后，在相同初始化下，训练初期梯度 L2 范数通常会

- A. 不可预测
- B. 几乎不变
- C. 显著增大
- D. 显著减小

**答案：D**

GELU 的导数在大多数输入范围内小于或接近 1，初期梯度的 $L_2$ 范数通常更小。

---

### 5. 设 $X\sim \mathrm{Binomial}(n=10,p=0.3)$，则 $E[X^2]$ 为：

- A. 5.2
- B. 4.2
- C. 3.9
- D. 11.1

**答案：D**

$$
E[X^2]=\mathrm{Var}(X)+\left(E[X]\right)^2=np(1-p)+(np)^2=10(0.3)(0.7)+(3)^2=11.1
$$

---

### 6. 在大模型训练中，以下哪项技术最能有效缓解 “幻觉”（Hallucination）问题？

- A. 降低模型的温度参数（Temperature）
- B. 增加训练数据量
- C. 引入外部知识库（如检索增强生成，RAG）
- D. 使用更复杂的模型结构

**答案：C**

引入外部知识库（RAG）可在生成时检索事实依据，更直接缓解幻觉。

---

### 7. Swish 激活函数 $f(x)=x\cdot\sigma(\beta x)$，当 $\beta\to\infty$ 时逼近以下哪个函数

- A. tanh
- B. sigmoid
- C. 线性
- D. ReLU

**答案：D**

当 $\beta\to\infty$，$\sigma(\beta x)$ 趋近阶跃函数，因此
$$
f(x)=x\cdot\sigma(\beta x)\to \max(0,x)
$$

---

### 8. 在逻辑回归中，若目标函数为
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \Big[y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))\Big]
$$
以下哪种优化方法最可能陷入局部最优？

- A. 牛顿法
- B. 拟牛顿法（BFGS）
- C. 高斯 - 牛顿法
- D. 随机梯度下降（SGD）

**答案：D**

解析指出：逻辑回归对数似然为凸；在给定选项中，SGD 更可能因噪声与学习率导致停滞/震荡。

---

### 9. 设 A 为 $m\times n$ 矩阵，B 为 $n\times m$ 矩阵，则线性方程组 $(AB)x=0$

- A. 当 $n>m$ 时仅有零解
- B. 当 $n>m$ 时必有非零解
- C. 当 $m>n$ 时必有非零解
- D. 当 $m>n$ 时仅有零解

**答案：C**

$$
\mathrm{rank}(AB)\le \mathrm{rank}(B)\le n
$$
若 $m>n$，则 $\mathrm{rank}(AB)\le n<m$，不可能满秩，故必有非零解。

---

### 10. 关于检索增强生成技术（Retrieval-Augmented Generation, RAG）中，将 HNSW（Hierarchical Navigable Small World）等近似最近邻（ANN）向量索引用于查询阶段的主要优势是什么？

- A. 结合向量二值化技术，将存储开销压缩至 1-bit，极大节省内存占用。
- B. 在检索过程中自动优化嵌入表示，增强查询与文档的语义对齐能力。
- C. 显著降低查询延迟，同时仅轻微牺牲召回率，实现效率与效果的平衡。
- D. 利用分层图结构加速搜索路径收敛，提升高维空间中相似性排序的稳定性。

**答案：C**

HNSW 等 ANN 常用“轻微召回损失”换“显著低延迟/高吞吐”，实现效率与效果平衡。

---

### 11. 在标准的多头注意力（MHA）中，Q/K/V 矩阵的生成方式是

- A. 共享同一组线性变换权重
- B. 通过卷积生成
- C. 每个头独立生成 Q/K/V
- D. 仅 Q 矩阵需要多头拆分，K/V 共享

**答案：C**

标准 MHA 为每个头使用独立线性投影 $W_Q^{(h)},W_K^{(h)},W_V^{(h)}$ 生成 Q/K/V。

---

### 12. 设 $X_1,X_2,\ldots,X_n$ 为来自二项分布总体 $B(n,p)$ 的一组简单随机样本，且记 $\bar X$ 和 $S^2$ 为样本均值和样本方差，若 $\bar X+kS^2$ 为 $np^2$ 的一个无偏估计，则常数 $k=$

- A. -2
- B. 2
- C. 1
- D. -1

**答案：D**

解析给出：$E[\bar X]=np,\ E[S^2]=np(1-p)$，令
$$
E[\bar X+kS^2]=np+k\,np(1-p)=np^2
$$
解得 $k=-1$。

---

### 13. 在三维图形变换中，点 P(1,2,3) 先绕 Z 轴旋转 $\theta=90^\circ$，再平移 $d=(4,5,6)$。用齐次坐标表示，最终变换矩阵 T 与点坐标 $P'$ 分别为？（旋转变换矩阵：
$$
R_z(\theta)=
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 & 0\\
\sin\theta & \cos\theta & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}
$$
，平移矩阵：
$$
D(d)=
\begin{bmatrix}
1 & 0 & 0 & d_x\\
0 & 1 & 0 & d_y\\
0 & 0 & 1 & d_z\\
0 & 0 & 0 & 1
\end{bmatrix}
$$
）

- A. $T=DR_z,\ P'=(6,-1,9,1)^T$
- B. $T=DR_z,\ P'=(3,7,9,1)^T$
- C. $T=R_zD,\ P'=(2,6,9,1)^T$
- D. $T=R_zD,\ P'=(5,3,9,1)^T$

**答案：C**

先旋转：$P(1,2,3)\to(-2,1,3)$；再平移加 $d=(4,5,6)$ 得
$$
P'=(2,6,9,1)^T
$$

---

### 14. 假设样本数据 $x_1,x_2,\ldots,x_n$ 来自指数分布 $f(x;\lambda)=\lambda e^{-\lambda x},\ x\ge 0$。最大似然估计得到的 $\lambda$ 的估计量是多少？

- A. $\dfrac{\sum_{i=1}^n x_i}{n}$
- B. $\dfrac{1}{n}\sum_{i=1}^n x_i$
- C. $\sum_{i=1}^n x_i$
- D. $\dfrac{n}{\sum_{i=1}^n x_i}$

**答案：D**

指数分布 MLE：
$$
\hat\lambda=\frac{n}{\sum_{i=1}^n x_i}
$$

---

### 15. 一个均匀 6 面骰子，平均投掷多少次能投出连续的两个 6 点

- A. 37
- B. 36
- C. 48
- D. 42

**答案：D**

连续 $r$ 次成功的期望次数（解析给出）：
$$
E=\frac{1-p^r}{(1-p)p^r}
$$
取 $p=\frac16,\ r=2$ 得 $E=42$。

---

## 二、多选题

### 16. 设事件 A 和 B 是两个随机事件，下列结论哪些总是成立？

- A. 如果 $A\subseteq B$，则 $P(A\cap B)=P(A)$
- B. 若 A 与 B 互斥，则 A 与 B 不独立
- C. 如果 A 与 B 独立，则 A 与 B 的补事件也独立
- D. 如果 $P(A)=1$，则 A 为样本空间

**答案：A、C**

- ✅ A：若 $A\subseteq B$，则 $A\cap B=A$，因此 $P(A\cap B)=P(A)$
- ❌ B：题目问“总是成立”，该项不作为总成立结论
- ✅ C：独立性对补事件保持（补事件也独立）
- ❌ D：$P(A)=1$ 表示“几乎必然发生”，不等同于“样本空间”

---

### 17. Scaled Dot-Product Attention 中缩放因子 $\sqrt{d_k}$ 的作用是

- A. 补偿 query 和 key 向量维度的影响
- B. 防止点积数值过大导致梯度消失
- C. 使 Softmax 输出更均匀
- D. 减少计算量

**答案：A、B、C**

- ✅ A：点积分布方差随维度增大而变大，缩放用于补偿维度影响
- ✅ B：防止点积过大导致 softmax 饱和、梯度变小
- ✅ C：使 softmax 输出更均匀
- ❌ D：缩放不直接减少计算量  
$$
\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)
$$

---

### 18. 对于拥有数千唯一取值的高基数分类特征，若希望在不显著增加特征维度的前提下，充分利用类别与监督目标之间的统计关系，以下处理方式中，哪些做法合适？

- A. 将类别直接编码为连续整数标签（如 0, 1, 2, ...），并作为数值型特征输入模型。
- B. 对所有类别执行独热编码（One-Hot Encoding），并用全零向量表示训练中未出现的未知类别。
- C. 使用目标编码（Target Encoding），将每个类别映射为其在训练集中对应目标变量的均值（或正类比例），并辅以平滑或交叉验证策略防止过拟合。
- D. 引入可学习的嵌入层（Embedding Layer），将每个类别映射到低维稠密空间，在模型训练中联合学习优化嵌入表示。

**答案：C、D**

- ❌ A：题目强调“充分利用类别与监督目标之间的统计关系”且“不显著增加维度”，该项不符合
- ❌ B：独热编码会显著增加维度
- ✅ C：Target Encoding 直接利用目标统计关系，且可用平滑/CV 防过拟合
- ✅ D：Embedding 在低维稠密空间中学习类别表示，不显著增维

---

### 19. 关于泊松分布和指数分布的关系，以下哪些说法正确？

- A. 泊松分布的期望和方差不同，指数分布的期望和方差相等。
- B. 若事件发生次数服从均值为 $\lambda$ 的泊松分布，则两次事件间时间间隔服从均值为 $\lambda$ 的指数分布。
- C. 泊松分布和指数分布都是连续型分布。
- D. 泊松分布描述单位时间内事件发生次数，指数分布描述两次事件间的时间间隔。

**答案：B、D**

- ❌ A：解析指出：泊松分布期望与方差都等于 $\lambda$
- ✅ B：解析指出：两者在泊松过程中相关（次数与间隔对应）
- ❌ C：解析指出：泊松分布是离散型分布
- ✅ D：泊松分布描述单位时间内事件次数；指数分布描述事件间时间间隔

---

### 20. 在以下问答场景中，哪些是普通 RAG 难以处理、需要借助领域知识图谱实现 GraphRAG 的？

- A. 知识库包含分散的人物关系描述（如 “B 的父亲是 C”、“B 的母亲是 A”），用户提问 “A 的女儿是谁”，需结合指代消解与多跳逻辑推理得出答案。
- B. 用户查询与知识库中预定义的 FAQ 问题语义高度相似，系统通过向量检索匹配最接近的问答对，并直接返回对应答案。
- C. 系统存储大量领域论文及其引用网络，用户基于语义查询某研究方向的代表性文献。
- D. 用户希望从多篇娱乐报道中找出被高频提及的核心人物，系统需跨文档识别并归一化人物名称，统计共现频率以生成聚合性结论。

**答案：A、D**

- ✅ A：需要指代消解 + 多跳逻辑推理（关系推断）
- ❌ B：语义检索匹配 FAQ 属于普通 RAG 可处理
- ❌ C：语义查询代表性文献不必然要求知识图谱推理（题目给的标准答案未选）
- ✅ D：跨文档实体识别/归一化 + 共现统计，适合用知识图谱增强聚合推理

# 2025-10-22 AI 方向题

## 一、单选题

### 1. 在深度学习中，若模型参数的更新依赖数值不稳定的矩阵求逆操作（如 $(X^T X)^{-1}$），则可能导致：

- A. 模型过拟合
- B. 参数估计误差大
- C. 模型欠拟合
- D. 训练加速过快

**答案：B**

数值不稳定（如 $(X^T X)^{-1}$ 条件数很大）会放大微小扰动，导致参数估计方差与误差增大。

---

### 2. 在构建一个用于预测用户是否流失的机器学习模型时，某数据科学家从原始数据中提取了 15 个特征，包括年龄、月消费金额、登录频率、服务投诉次数等。为了提升模型性能并减少过拟合，他决定使用递归特征消除（Recursive Feature Elimination, RFE）方法进行特征选择。已知他使用了一个训练好的逻辑回归模型作为评估器，并设定最终保留 5 个特征。以下关于 RFE 过程的描述中，哪一项是正确的？

- A. RFE 通过反复训练模型，每次移除当前模型中系数绝对值最小的特征，逐步缩小特征集，最终得到一个包含 5 个特征的子集。
- B. RFE 基于特征的方差阈值逐步剔除低方差特征，直到保留指定数量为止。
- C. RFE 根据各特征在逻辑回归中的 p 值显著性进行排序，每次移除最不显著的特征，直到保留 5 个具有统计显著性的特征。
- D. RFE 计算每个特征与目标变量之间的皮尔逊相关系数，保留相关性最高的 5 个特征。

**答案：A**

RFE 每轮依据评估器的重要性（如系数绝对值）剔除最弱特征，迭代直到保留指定数量。

---

### 3. 在自然语言处理中，比较两个词嵌入向量 $u=\begin{bmatrix}0.8\\0.6\end{bmatrix}$、$v=\begin{bmatrix}0.5\\0.866\end{bmatrix}$ 的语义相似性。以下结论正确的是？

- A. 点积=0.8，表明语义高度相似
- B. 点积结果受向量模影响，余弦相似度更可靠
- C. 余弦相似度≈0.99，表明语义几乎相同
- D. 向量 v 的模更大，因此更重要

**答案：B**

点积受向量模长影响；余弦相似度通过归一化更能反映方向相似性：
$$
\cos(u,v)=\frac{u^\top v}{\|u\|\|v\|}
$$

---

### 4. 当把 ReLU 换成 GELU 后，在相同初始化下，训练初期梯度 L2 范数通常会

- A. 不可预测
- B. 几乎不变
- C. 显著增大
- D. 显著减小

**答案：D**

GELU 的导数在大多数输入范围内小于或接近 1，初期梯度的 $L_2$ 范数通常更小。

---

### 5. 设 $X\sim \mathrm{Binomial}(n=10,p=0.3)$，则 $E[X^2]$ 为：

- A. 5.2
- B. 4.2
- C. 3.9
- D. 11.1

**答案：D**

$$
E[X^2]=\mathrm{Var}(X)+\left(E[X]\right)^2=np(1-p)+(np)^2=10(0.3)(0.7)+(3)^2=11.1
$$

---

### 6. 在大模型训练中，以下哪项技术最能有效缓解 “幻觉”（Hallucination）问题？

- A. 降低模型的温度参数（Temperature）
- B. 增加训练数据量
- C. 引入外部知识库（如检索增强生成，RAG）
- D. 使用更复杂的模型结构

**答案：C**

引入外部知识库（RAG）可在生成时检索事实依据，更直接缓解幻觉。

---

### 7. Swish 激活函数 $f(x)=x\cdot\sigma(\beta x)$，当 $\beta\to\infty$ 时逼近以下哪个函数

- A. tanh
- B. sigmoid
- C. 线性
- D. ReLU

**答案：D**

当 $\beta\to\infty$，$\sigma(\beta x)$ 趋近阶跃函数，因此
$$
f(x)=x\cdot\sigma(\beta x)\to \max(0,x)
$$

---

### 8. 在逻辑回归中，若目标函数为
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \Big[y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))\Big]
$$
以下哪种优化方法最可能陷入局部最优？

- A. 牛顿法
- B. 拟牛顿法（BFGS）
- C. 高斯 - 牛顿法
- D. 随机梯度下降（SGD）

**答案：D**

解析指出：逻辑回归对数似然为凸；在给定选项中，SGD 更可能因噪声与学习率导致停滞/震荡。

---

### 9. 设 A 为 $m\times n$ 矩阵，B 为 $n\times m$ 矩阵，则线性方程组 $(AB)x=0$

- A. 当 $n>m$ 时仅有零解
- B. 当 $n>m$ 时必有非零解
- C. 当 $m>n$ 时必有非零解
- D. 当 $m>n$ 时仅有零解

**答案：C**

$$
\mathrm{rank}(AB)\le \mathrm{rank}(B)\le n
$$
若 $m>n$，则 $\mathrm{rank}(AB)\le n<m$，不可能满秩，故必有非零解。

---

### 10. 关于检索增强生成技术（Retrieval-Augmented Generation, RAG）中，将 HNSW（Hierarchical Navigable Small World）等近似最近邻（ANN）向量索引用于查询阶段的主要优势是什么？

- A. 结合向量二值化技术，将存储开销压缩至 1-bit，极大节省内存占用。
- B. 在检索过程中自动优化嵌入表示，增强查询与文档的语义对齐能力。
- C. 显著降低查询延迟，同时仅轻微牺牲召回率，实现效率与效果的平衡。
- D. 利用分层图结构加速搜索路径收敛，提升高维空间中相似性排序的稳定性。

**答案：C**

HNSW 等 ANN 常用“轻微召回损失”换“显著低延迟/高吞吐”，实现效率与效果平衡。

---

### 11. 在标准的多头注意力（MHA）中，Q/K/V 矩阵的生成方式是

- A. 共享同一组线性变换权重
- B. 通过卷积生成
- C. 每个头独立生成 Q/K/V
- D. 仅 Q 矩阵需要多头拆分，K/V 共享

**答案：C**

标准 MHA 为每个头使用独立线性投影 $W_Q^{(h)},W_K^{(h)},W_V^{(h)}$ 生成 Q/K/V。

---

### 12. 设 $X_1,X_2,\ldots,X_n$ 为来自二项分布总体 $B(n,p)$ 的一组简单随机样本，且记 $\bar X$ 和 $S^2$ 为样本均值和样本方差，若 $\bar X+kS^2$ 为 $np^2$ 的一个无偏估计，则常数 $k=$

- A. -2
- B. 2
- C. 1
- D. -1

**答案：D**

解析给出：$E[\bar X]=np,\ E[S^2]=np(1-p)$，令
$$
E[\bar X+kS^2]=np+k\,np(1-p)=np^2
$$
解得 $k=-1$。

---

### 13. 在三维图形变换中，点 P(1,2,3) 先绕 Z 轴旋转 $\theta=90^\circ$，再平移 $d=(4,5,6)$。用齐次坐标表示，最终变换矩阵 T 与点坐标 $P'$ 分别为？（旋转变换矩阵：
$$
R_z(\theta)=
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 & 0\\
\sin\theta & \cos\theta & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}
$$
，平移矩阵：
$$
D(d)=
\begin{bmatrix}
1 & 0 & 0 & d_x\\
0 & 1 & 0 & d_y\\
0 & 0 & 1 & d_z\\
0 & 0 & 0 & 1
\end{bmatrix}
$$
）

- A. $T=DR_z,\ P'=(6,-1,9,1)^T$
- B. $T=DR_z,\ P'=(3,7,9,1)^T$
- C. $T=R_zD,\ P'=(2,6,9,1)^T$
- D. $T=R_zD,\ P'=(5,3,9,1)^T$

**答案：C**

先旋转：$P(1,2,3)\to(-2,1,3)$；再平移加 $d=(4,5,6)$ 得
$$
P'=(2,6,9,1)^T
$$

---

### 14. 假设样本数据 $x_1,x_2,\ldots,x_n$ 来自指数分布 $f(x;\lambda)=\lambda e^{-\lambda x},\ x\ge 0$。最大似然估计得到的 $\lambda$ 的估计量是多少？

- A. $\dfrac{\sum_{i=1}^n x_i}{n}$
- B. $\dfrac{1}{n}\sum_{i=1}^n x_i$
- C. $\sum_{i=1}^n x_i$
- D. $\dfrac{n}{\sum_{i=1}^n x_i}$

**答案：D**

指数分布 MLE：
$$
\hat\lambda=\frac{n}{\sum_{i=1}^n x_i}
$$

---

### 15. 一个均匀 6 面骰子，平均投掷多少次能投出连续的两个 6 点

- A. 37
- B. 36
- C. 48
- D. 42

**答案：D**

连续 $r$ 次成功的期望次数（解析给出）：
$$
E=\frac{1-p^r}{(1-p)p^r}
$$
取 $p=\frac16,\ r=2$ 得 $E=42$。

---

## 二、多选题

### 16. 设事件 A 和 B 是两个随机事件，下列结论哪些总是成立？

- A. 如果 $A\subseteq B$，则 $P(A\cap B)=P(A)$
- B. 若 A 与 B 互斥，则 A 与 B 不独立
- C. 如果 A 与 B 独立，则 A 与 B 的补事件也独立
- D. 如果 $P(A)=1$，则 A 为样本空间

**答案：A、C**

- ✅ A：若 $A\subseteq B$，则 $A\cap B=A$，因此 $P(A\cap B)=P(A)$
- ❌ B：题目问“总是成立”，该项不作为总成立结论
- ✅ C：独立性对补事件保持（补事件也独立）
- ❌ D：$P(A)=1$ 表示“几乎必然发生”，不等同于“样本空间”

---

### 17. Scaled Dot-Product Attention 中缩放因子 $\sqrt{d_k}$ 的作用是

- A. 补偿 query 和 key 向量维度的影响
- B. 防止点积数值过大导致梯度消失
- C. 使 Softmax 输出更均匀
- D. 减少计算量

**答案：A、B、C**

- ✅ A：点积分布方差随维度增大而变大，缩放用于补偿维度影响
- ✅ B：防止点积过大导致 softmax 饱和、梯度变小
- ✅ C：使 softmax 输出更均匀
- ❌ D：缩放不直接减少计算量  
$$
\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)
$$

---

### 18. 对于拥有数千唯一取值的高基数分类特征，若希望在不显著增加特征维度的前提下，充分利用类别与监督目标之间的统计关系，以下处理方式中，哪些做法合适？

- A. 将类别直接编码为连续整数标签（如 0, 1, 2, ...），并作为数值型特征输入模型。
- B. 对所有类别执行独热编码（One-Hot Encoding），并用全零向量表示训练中未出现的未知类别。
- C. 使用目标编码（Target Encoding），将每个类别映射为其在训练集中对应目标变量的均值（或正类比例），并辅以平滑或交叉验证策略防止过拟合。
- D. 引入可学习的嵌入层（Embedding Layer），将每个类别映射到低维稠密空间，在模型训练中联合学习优化嵌入表示。

**答案：C、D**

- ❌ A：题目强调“充分利用类别与监督目标之间的统计关系”且“不显著增加维度”，该项不符合
- ❌ B：独热编码会显著增加维度
- ✅ C：Target Encoding 直接利用目标统计关系，且可用平滑/CV 防过拟合
- ✅ D：Embedding 在低维稠密空间中学习类别表示，不显著增维

---

### 19. 关于泊松分布和指数分布的关系，以下哪些说法正确？

- A. 泊松分布的期望和方差不同，指数分布的期望和方差相等。
- B. 若事件发生次数服从均值为 $\lambda$ 的泊松分布，则两次事件间时间间隔服从均值为 $\lambda$ 的指数分布。
- C. 泊松分布和指数分布都是连续型分布。
- D. 泊松分布描述单位时间内事件发生次数，指数分布描述两次事件间的时间间隔。

**答案：B、D**

- ❌ A：解析指出：泊松分布期望与方差都等于 $\lambda$
- ✅ B：解析指出：两者在泊松过程中相关（次数与间隔对应）
- ❌ C：解析指出：泊松分布是离散型分布
- ✅ D：泊松分布描述单位时间内事件次数；指数分布描述事件间时间间隔

---

### 20. 在以下问答场景中，哪些是普通 RAG 难以处理、需要借助领域知识图谱实现 GraphRAG 的？

- A. 知识库包含分散的人物关系描述（如 “B 的父亲是 C”、“B 的母亲是 A”），用户提问 “A 的女儿是谁”，需结合指代消解与多跳逻辑推理得出答案。
- B. 用户查询与知识库中预定义的 FAQ 问题语义高度相似，系统通过向量检索匹配最接近的问答对，并直接返回对应答案。
- C. 系统存储大量领域论文及其引用网络，用户基于语义查询某研究方向的代表性文献。
- D. 用户希望从多篇娱乐报道中找出被高频提及的核心人物，系统需跨文档识别并归一化人物名称，统计共现频率以生成聚合性结论。

**答案：A、D**

- ✅ A：需要指代消解 + 多跳逻辑推理（关系推断）
- ❌ B：语义检索匹配 FAQ 属于普通 RAG 可处理
- ❌ C：语义查询代表性文献不必然要求知识图谱推理（题目给的标准答案未选）
- ✅ D：跨文档实体识别/归一化 + 共现统计，适合用知识图谱增强聚合推理


下面把你标的 **(4)(8)(9)(13)** 按“能考、能背、能现场算”来讲：每题给 **数学推导 + 物理/直觉类比 + 记忆法**。

---

## （4）为什么 ReLU 换成 GELU 后，训练初期梯度 L2 范数通常 **减小**？

### 1）核心原因（用一行式子抓住）

反传到上一层的梯度会被激活函数导数“门控”：
[
|\nabla_{\text{in}}|*2 \approx | f'(x)\odot \nabla*{\text{out}}|_2
]
所以关键看 **(f'(x))** 的“平均平方”有多大（粗略：(\mathbb E[(f'(x))^2]) 越小，梯度能量越小）。

* **ReLU**
  [
  \text{ReLU}(x)=\max(0,x),\quad \text{ReLU}'(x)=\mathbf 1_{x>0}
  ]
  若初始化时 (x) 近似对称（常见近似 (x\sim \mathcal N(0,1))），则
  [
  \mathbb E[(\text{ReLU}'(x))^2]=P(x>0)=\tfrac12
  ]

* **GELU（精确定义）**
  [
  \text{GELU}(x)=x\Phi(x)
  ]
  其中 (\Phi(x)) 是标准正态 CDF，(\phi(x)) 是 PDF。导数很漂亮：
  [
  \text{GELU}'(x)=\Phi(x)+x\phi(x)
  ]
  对称输入下，(\text{GELU}'(x)) 在大量区域里 **小于 1**（尤其是负半轴），因此通常有
  [
  \mathbb E[(\text{GELU}'(x))^2] < \tfrac12
  ]
  → **梯度 L2 范数更小**（训练初期“更软”，不那么“冲”）。

### 2）你需要背的 GELU 近似公式（考场最常见）

很多实现用 tanh 近似：
[
\text{GELU}(x)\approx 0.5x\left(1+\tanh!\Big(\sqrt{\tfrac{2}{\pi}}(x+0.044715x^3)\Big)\right)
]
（这个不一定要求手推导数；记住“GELU 是 (x) 乘一个软门”就够了。）

### 3）同类“软门”激活：Swish / SiLU（你点名要）

* **Swish（一般形式）**
  [
  \text{Swish}*\beta(x)=x,\sigma(\beta x)
  ]
  [
  \frac{d}{dx}\text{Swish}*\beta(x)=\sigma(\beta x)+\beta x,\sigma(\beta x)\big(1-\sigma(\beta x)\big)
  ]
* **SiLU** 就是 (\beta=1) 的 Swish：
  [
  \text{SiLU}(x)=x\sigma(x)
  ]

### 4）物理/直觉类比（好记）

* **ReLU = 理想二极管/硬开关**：要么全通（斜率 1），要么断路（斜率 0）→ 梯度“能量”比较大且很硬。
* **GELU/Swish = 带温度的软阀门**：负半轴不是完全断，而是“漏一点”；正半轴也不是永远 1 的硬斜率 → 平均下来 **更“省力”**，梯度 L2 往往更小。

**一眼记住**：

> ReLU：硬门（0/1）
> GELU：概率门（(\Phi(x))）
> Swish：sigmoid 门（(\sigma(\beta x))）

---

## （8）拟牛顿法是什么？A/B/C 都来一个“超简单例子”

题里 A=牛顿法，B=拟牛顿(BFGS)，C=高斯-牛顿。

### A. 牛顿法（Newton）

更新公式（最经典）：
[
x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)}
]
**例子：** (f(x)=(x-3)^2)
[
f'(x)=2(x-3),\quad f''(x)=2
]
从任意 (x_0)：
[
x_1=x_0-\frac{2(x_0-3)}{2}=3
]
**一句话：** 二阶信息“直接告诉你曲率”，二次函数一跳到最优。

---

### B. 拟牛顿（Quasi-Newton，代表：BFGS）

核心：**不用显式 Hessian (f'')，用“梯度变化”去学曲率**。

1D 情况你可以把它记成“割线法版二阶”：
[
f''(x)\approx \frac{g_1-g_0}{x_1-x_0},\quad g_i=f'(x_i)
]
然后像牛顿一样走：
[
x_{2}=x_1-\frac{g_1}{\frac{g_1-g_0}{x_1-x_0}}
]

**例子同样用** (f(x)=(x-3)^2)，选两点 (x_0=0,x_1=1)：

* (g_0=f'(0)=-6)
* (g_1=f'(1)=-4)
* 曲率估计：(\frac{-4-(-6)}{1-0}=2)
* 更新：(x_2=1-\frac{-4}{2}=3)

**一句话：** 拟牛顿 = “用两次梯度差分把二阶学出来”。

（在高维里 BFGS 的标准更新你只要认识符号即可：用 (s=x_{k+1}-x_k)、(y=g_{k+1}-g_k) 更新一个“逆 Hessian 近似”。）

---

### C. 高斯-牛顿（Gauss-Newton）

只针对 **最小二乘**：
[
\min_\theta \frac12|r(\theta)|^2
]
令 (J=\frac{\partial r}{\partial \theta})，更新：
[
\theta_{k+1}=\theta_k-(J^\top J)^{-1}J^\top r
]
（它相当于把 Hessian 近似成 (J^\top J)，忽略二阶残差项。）

**例子：拟合一条直线** (y\approx a x + b)
残差 (r_i(a,b)=a x_i+b-y_i)，雅可比一行是 ([x_i,,1])。
高斯-牛顿一步就是经典正规方程：
[
\begin{bmatrix}a\b\end{bmatrix}
=(X^\top X)^{-1}X^\top y
]
**一句话：** 高斯-牛顿 = “最小二乘专用的二阶近似”。

---

### 你这题（逻辑回归）怎么记（冲刺版）

逻辑回归目标是 **凸的**，严格说“不存在局部最优”。题库解释选 SGD（D）想表达的是：

> SGD 受噪声/学习率影响更可能在最优附近震荡或收敛慢，看起来“停在次优”。

**记忆法：**

* 牛顿/BFGS：二阶（快、稳、每步贵）
* SGD：一阶（省、噪、步长不对就“晃”）

---

## （9）还是没懂：为什么 (m>n) 时 ((AB)x=0) 必有非零解？

你只要抓住 **rank 上界**：

* (A\in\mathbb R^{m\times n})，(B\in\mathbb R^{n\times m})
  所以 (AB\in\mathbb R^{m\times m})，解 (x\in\mathbb R^m)。

关键不等式：
[
\operatorname{rank}(AB)\le \operatorname{rank}(B)\le n
]
因为 (B) 的秩最多等于它的行数 (n)。

**当 (m>n)：**
[
\operatorname{rank}(AB)\le n<m
]
所以 (AB) 不可能满秩 ⇒ 一定奇异 ⇒ 齐次方程一定有非零解。

更“量化”的一句（用秩-零度定理）：
[
\text{nullity}(AB)=m-\operatorname{rank}(AB)\ge m-n>0
]
所以非零解空间至少有 (m-n) 维。

**超直观类比（物理版）：**
(B: \mathbb R^m\to \mathbb R^n) 是把 **m 维信息压到 n 维**（压缩）。
当 (m>n)，必然有一些方向被压没（映到同一个点），于是再乘 (A) 回到 (\mathbb R^m) 时，整体 (AB) 不可能“可逆”，必有方向被打成 0。

**最小例子给你“眼见为实”：**
取 (m=3,n=2)：
[
B=\begin{bmatrix}1&0&0\0&1&0\end{bmatrix},\
A=\begin{bmatrix}1&0\0&1\0&0\end{bmatrix}
\Rightarrow
AB=\begin{bmatrix}1&0&0\0&1&0\0&0&0\end{bmatrix}
]
那 ((AB)x=0) 的非零解就是 (x=(0,0,1)^\top)。

**记忆法：**

> (m>n) = “先降维再升维”，不可能满秩 ⇒ 必有非零零空间。

---

## （13）为什么有人写 (T=RD) 而不是 (T=DR)？不是先旋转吗？

先给你一个**不怕顺序坑的稳赢法（考场推荐）**：直接算点！

### 1）坐标法（最稳）

绕 (z) 轴 (90^\circ)：
[
(x,y,z)\mapsto (-y,,x,,z)
]
所以 (P(1,2,3)\to(-2,1,3))。
再平移 (d=(4,5,6))：
[
(-2,1,3)+(4,5,6)=(2,6,9)
]
所以 (P'=(2,6,9,1)^\top) —— 你只要在选项里找这个就行。

### 2）矩阵乘法顺序为什么会“反”？

因为看你用 **列向量** 还是 **行向量**：

* **列向量（最常见，点是 (4\times 1)）**：
  先旋转再平移：
  [
  p' = D,(R,p) = (DR),p
  ]
  **结论：先做的在右边，后做的在左边。**

* **行向量（点是 (1\times 4)）**：
  [
  p' = (p,R),D = p,(RD)
  ]
  **结论：先做的在左边，后做的在右边。**

而且两种约定下，“平移矩阵里 (d_x,d_y,d_z)”的位置也会变：

* 列向量：平移在**最后一列**
* 行向量：平移在**最后一行**

**记忆法（10 秒）：**

* 列向量：**右边先做**（从右往左读）
* 行向量：**左边先做**

> 冲刺阶段我建议：**一律用坐标法算 (P')**，矩阵顺序只用来“解释”，别拿它当唯一判题依据。

---

下面把你问的两块（**16** 的两个判断、**9** 的 B/C 争议）一次讲透：给 **反例 / 严格证明 / 直觉 / 记忆法**（都能在考场 10 秒复现）。

---

## （16-1）“若 A 与 B 互斥，则 A 与 B 不独立”——**不是总成立**，只有在 (P(A),P(B)>0) 时成立

### ✅ 一般情况（你平时遇到的）：互斥 + 都非零概率 ⇒ **一定不独立**

互斥：(A\cap B=\varnothing\Rightarrow P(A\cap B)=0)

若还满足 (P(A)>0,\ P(B)>0)，则
[
P(A)P(B)>0 \neq 0 = P(A\cap B)
]
所以 **不独立**。

### ✅ 反例（你要的）：互斥但**独立**

只要其中一个事件概率为 0（或 1 的补事件概率为 0），就可能既互斥又独立。

**最简单反例：**

* 样本空间 (\Omega={1,2})，均匀
* 取 (A=\varnothing)（不可能事件，(P(A)=0)）
* 取 (B={1})（(P(B)=\tfrac12)）

则

* (A\cap B=\varnothing)（互斥）
* (P(A\cap B)=0)，而 (P(A)P(B)=0\cdot \tfrac12=0)

所以 **独立成立**。

### 💡 记忆法（冲刺版）

> **互斥 ≠ 不独立（例外：概率 0）**
> **互斥且都“可能发生”（(P>0)） ⇒ 一定不独立。**

---

## （16-2）“独立性对补事件保持”——**是真的**（而且四种组合都独立）

已知 (A) 与 (B) 独立：
[
P(A\cap B)=P(A)P(B)
]

### ✅ 证明 1：(A) 与 (B^c) 也独立

[
P(A\cap B^c)=P(A)-P(A\cap B)=P(A)-P(A)P(B)=P(A)(1-P(B))=P(A)P(B^c)
]

### ✅ 证明 2：(A^c) 与 (B) 也独立

同理：
[
P(A^c\cap B)=P(B)-P(A\cap B)=P(B)-P(A)P(B)=(1-P(A))P(B)=P(A^c)P(B)
]

### ✅ 证明 3：(A^c) 与 (B^c) 也独立（考场最常用）

[
P(A^c\cap B^c)=1-P(A)-P(B)+P(A\cap B)
]
代入独立性：
[
=1-P(A)-P(B)+P(A)P(B)=(1-P(A))(1-P(B))=P(A^c)P(B^c)
]

### 💡 记忆法

> **独立 ⇒ 补也独立**（“独立”对取补封闭）
> 四个都独立：(A,B)，(A,B^c)，(A^c,B)，(A^c,B^c)。

---

## （9）你说“正确答案是 B 不是 C”——从线代结论看：**C 才是必然成立**，B 不是

题面：(A\in \mathbb{R}^{m\times n},\ B\in \mathbb{R}^{n\times m})，所以 (AB\in \mathbb{R}^{m\times m})，解 ((AB)x=0)（(x\in\mathbb{R}^m)）。

### ✅ 为什么 **C（(m>n) 必有非零解）**一定对？

关键是秩上界：
[
\operatorname{rank}(AB)\le \operatorname{rank}(B)\le n
]
若 (m>n)，则
[
\operatorname{rank}(AB)\le n < m
]
所以 (AB) **不可能满秩** ⇒ 必定奇异 ⇒ 齐次方程必有非零解。

用秩-零度定理一句话封神：
[
\dim\ker(AB)=m-\operatorname{rank}(AB)\ge m-n>0
]
所以非零解空间至少 (m-n) 维。

### ❌ 为什么 **B（(n>m) 必有非零解）**不一定对？给你一个一眼反例

取 (m=2,\ n=3)（满足 (n>m)），构造
[
A=\begin{bmatrix}1&0&0\0&1&0\end{bmatrix},\quad
B=\begin{bmatrix}1&0\0&1\0&0\end{bmatrix}
]
则
[
AB=\begin{bmatrix}1&0\0&1\end{bmatrix}=I_2
]
于是 ((AB)x=0 \Rightarrow x=0) **只有零解**。
所以 **“(n>m) 必有非零解”是错的**（不保证）。

### 💡 物理/直觉类比（最好记）

把 (B:\mathbb{R}^m\to \mathbb{R}^n) 当“通道/测量”：

* 若 (m>n)：**从高维压到低维**（信息必丢），一定有某些方向被压没 ⇒ 一定有非零 (x) 让 (Bx=0)（更别说再乘 (A)）
* 若 (n>m)：是“升维”，**不必丢信息**，完全可能做到可逆（比如上面的 (AB=I)）

### 记忆法（10 秒版）

> 看最终是 (m\times m) 矩阵 (AB)。
> **只要中间瓶颈维度 (n<m)，秩就卡在 (n)，必不满秩 ⇒ 必有非零解。**
> 所以必然条件是 **(m>n)**（对应选项 C）。

---

`'16': A '17': B '18': C '19': B '20': A`

---

## ✅ 错题整理（结构化）

> **整理日期**: 2026-01-04  
> **统计**: 20 题，5 题错（多选漏选），正确率 75%

---

### 📊 知识点标注表

| 题号 | 知识点 Tag | 是否错题 |
|------|------------|----------|
| 1 | 深度学习基础 | ✅ |
| 2 | 经典机器学习 | ✅ |
| 3 | 深度学习基础 | ✅ |
| 4 | 深度学习基础 | ✅ |
| 5 | 概率统计 | ✅ |
| 6 | 大模型(LLM) | ✅ |
| 7 | 深度学习基础 | ✅ |
| 8 | 优化与数值方法 | ✅ |
| 9 | 线性代数 | ✅ |
| 10 | 大模型(LLM) | ✅ |
| 11 | 注意力与Transformer | ✅ |
| 12 | 概率统计 | ✅ |
| 13 | 线性代数 | ✅ |
| 14 | 概率统计 | ✅ |
| 15 | 概率统计 | ✅ |
| 16 | 概率统计 | ❌ 漏选C |
| 17 | 注意力与Transformer | ❌ 漏选A,C |
| 18 | 经典机器学习 | ❌ 漏选D |
| 19 | 概率统计 | ❌ 漏选D |
| 20 | 大模型(LLM) | ❌ 漏选D |

---

### 📈 掌握度统计

| 知识点 | 总数 | 错数 | 正确率 |
|--------|------|------|--------|
| 注意力与Transformer | 2 | 1 | 50% |
| 经典机器学习 | 2 | 1 | 50% |
| 概率统计 | 6 | 2 | 67% |
| 大模型(LLM) | 3 | 1 | 67% |
| 深度学习基础 | 4 | 0 | 100% |
| 优化与数值方法 | 1 | 0 | 100% |
| 线性代数 | 2 | 0 | 100% |

---

### 🔥 薄弱环节 Top-3（20251022）

| 排名 | 知识点 | 错/总 | 正确率 | 备注 |
|------|--------|-------|--------|------|
| 1 | 注意力与Transformer | 1/2 | 50% | 样本少 |
| 2 | 经典机器学习 | 1/2 | 50% | 样本少 |
| 3 | 概率统计 | 2/6 | 67% | 独立性、泊松分布 |

---

## 📚 核心知识点速查

### 1. 概率统计

**互斥 vs 独立**
$$
\text{互斥}: P(A \cap B) = 0 \quad\text{（不能同时发生）}
$$
$$
\text{独立}: P(A \cap B) = P(A)P(B) \quad\text{（发生互不影响）}
$$

- **互斥 + 都非零概率 ⇒ 一定不独立**（因为 $P(A)P(B) > 0 \neq 0 = P(A \cap B)$）
- **例外**: 若 $P(A)=0$ 或 $P(B)=0$，可能既互斥又独立

**独立性对补事件保持**（四种组合都独立）
$$
P(A \cap B^c) = P(A) - P(A \cap B) = P(A)(1 - P(B)) = P(A)P(B^c)
$$
$$
P(A^c \cap B^c) = (1-P(A))(1-P(B)) = P(A^c)P(B^c)
$$

**泊松 vs 指数分布**
| 分布 | 描述 | 期望 | 方差 |
|------|------|------|------|
| 泊松 $\text{Poisson}(\lambda)$ | 单位时间事件**次数** | $\lambda$ | $\lambda$ |
| 指数 $\text{Exp}(\lambda)$ | 两次事件**间隔时间** | $1/\lambda$ | $1/\lambda^2$ |

> ⚠️ 泊松 $E = \text{Var} = \lambda$；指数 $E \neq \text{Var}$

### 2. 注意力与Transformer

**Scaled Dot-Product 缩放因子 $\sqrt{d_k}$ 的三个作用**
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

1. ✅ **补偿维度影响**: 点积方差 $\propto d_k$，缩放后方差稳定
2. ✅ **防止梯度消失**: 点积过大 → softmax 饱和 → 梯度趋近 0
3. ✅ **使 softmax 更均匀**: 避免 one-hot 分布
4. ❌ 不减少计算量

### 3. 经典机器学习

**高基数分类特征处理**（不显著增维 + 利用目标统计关系）
- ❌ 整数编码（无语义关系）
- ❌ One-Hot（维度爆炸）
- ✅ **Target Encoding**: 类别 → 目标均值，配合平滑/CV 防过拟合
- ✅ **Embedding Layer**: 低维稠密表示，端到端学习

### 4. 大模型(LLM)

**GraphRAG 适用场景**（普通 RAG 难以处理）
- ✅ **多跳逻辑推理**: "A的女儿是谁" → 需结合 "B的父亲是C" + "B的母亲是A"
- ✅ **跨文档实体归一化**: 识别不同文档中的同一人物，统计共现
- ❌ 语义相似 FAQ 匹配 → 普通向量检索即可
- ❌ 论文语义搜索 → 普通向量检索即可

### 5. 深度学习基础（追问整理）

**GELU vs ReLU 梯度**
$$
\text{GELU}(x) = x \cdot \Phi(x), \quad \text{GELU}'(x) = \Phi(x) + x\phi(x)
$$

- ReLU: 硬门（0/1），梯度能量大
- GELU: 概率门 $\Phi(x)$，梯度能量更小 → 训练初期梯度 $L_2$ 范数**减小**

**Swish 极限**
$$
\text{Swish}_\beta(x) = x \cdot \sigma(\beta x) \xrightarrow{\beta \to \infty} \max(0, x) = \text{ReLU}(x)
$$

### 6. 线性代数（追问整理）

**矩阵秩与齐次方程解**

设 $A \in \mathbb{R}^{m \times n}$，$B \in \mathbb{R}^{n \times m}$，则 $AB \in \mathbb{R}^{m \times m}$

$$
\text{rank}(AB) \le \text{rank}(B) \le n
$$

- **若 $m > n$**: $\text{rank}(AB) \le n < m$ → $AB$ 不满秩 → $(AB)x = 0$ **必有非零解**
- 直觉: 从 $m$ 维压到 $n$ 维再回来，信息必丢

**齐次坐标变换顺序**（列向量约定）
$$
P' = D \cdot R \cdot P \quad\text{（先旋转 $R$，后平移 $D$）}
$$

> **右边先做**：从右往左读矩阵顺序

---

## Q16: 事件独立性与互斥性

**原题**：
> 设事件 A 和 B 是两个随机事件，下列结论哪些总是成立？
> - A. 如果 $A\subseteq B$，则 $P(A\cap B)=P(A)$
> - B. 若 A 与 B 互斥，则 A 与 B 不独立
> - C. 如果 A 与 B 独立，则 A 与 B 的补事件也独立
> - D. 如果 $P(A)=1$，则 A 为样本空间

**正确答案**：A、C

**你的选择**：A（漏选 C）

**易错点**：
- B 选项的陷阱：互斥 + **都非零概率** 才不独立；若 $P(A)=0$，可能既互斥又独立
- C 选项容易漏：独立性对补事件封闭（四种组合都独立）

**核心公式/结论**：
$$
P(A^c \cap B^c) = 1 - P(A) - P(B) + P(A \cap B) = (1-P(A))(1-P(B)) = P(A^c)P(B^c)
$$

**一眼记住**：
> **独立 ⇒ 补也独立**；互斥 ≠ 不独立（例外：概率 0）

---

## Q17: Scaled Dot-Product Attention 缩放因子

**原题**：
> Scaled Dot-Product Attention 中缩放因子 $\sqrt{d_k}$ 的作用是
> - A. 补偿 query 和 key 向量维度的影响
> - B. 防止点积数值过大导致梯度消失
> - C. 使 Softmax 输出更均匀
> - D. 减少计算量

**正确答案**：A、B、C

**你的选择**：B（漏选 A、C）

**易错点**：
- 只记住"防梯度消失"，忘记更本质的原因是"维度影响方差"
- 忽略了 softmax 分布均匀性的作用

**核心公式/结论**：
$$
\text{Var}(q^\top k) = d_k \cdot \text{Var}(q_i) \cdot \text{Var}(k_i) \approx d_k
$$
缩放后方差回到 1，避免 softmax 饱和。

**一眼记住**：
> 缩放 = 补偿维度 + 防饱和 + 均匀分布（**不减少计算量**）

---

## Q18: 高基数分类特征处理

**原题**：
> 对于拥有数千唯一取值的高基数分类特征，若希望在不显著增加特征维度的前提下，充分利用类别与监督目标之间的统计关系，以下处理方式中，哪些做法合适？
> - A. 将类别直接编码为连续整数标签
> - B. 对所有类别执行独热编码
> - C. 使用目标编码（Target Encoding）
> - D. 引入可学习的嵌入层（Embedding Layer）

**正确答案**：C、D

**你的选择**：C（漏选 D）

**易错点**：
- 只想到 Target Encoding，忘记 Embedding 也是低维表示
- Embedding Layer 虽然需要训练，但维度可控且能学习复杂表示

**核心公式/结论**：
- Target Encoding: $x_{\text{category}} \to \mathbb{E}[y | \text{category}]$
- Embedding: $x_{\text{category}} \to \mathbf{e} \in \mathbb{R}^d$ (低维稠密)

**一眼记住**：
> 高基数 + 不增维 + 利用目标 → **Target Encoding** 或 **Embedding**

---

## Q19: 泊松分布与指数分布的关系

**原题**：
> 关于泊松分布和指数分布的关系，以下哪些说法正确？
> - A. 泊松分布的期望和方差不同，指数分布的期望和方差相等
> - B. 若事件发生次数服从均值为 λ 的泊松分布，则两次事件间时间间隔服从均值为 λ 的指数分布
> - C. 泊松分布和指数分布都是连续型分布
> - D. 泊松分布描述单位时间内事件发生次数，指数分布描述两次事件间的时间间隔

**正确答案**：B、D

**你的选择**：B（漏选 D）

**易错点**：
- A 选项反了：泊松 $E = \text{Var} = \lambda$
- C 选项：泊松是**离散**分布（计数）
- D 选项直接描述了两者的本质区别，容易被当作"太显然"而忽略

**核心公式/结论**：
| 泊松 | 指数 |
|------|------|
| $P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$ | $f(x) = \lambda e^{-\lambda x}$ |
| $E[X] = \lambda$ | $E[X] = 1/\lambda$ |
| $\text{Var}(X) = \lambda$ | $\text{Var}(X) = 1/\lambda^2$ |
| 离散（次数） | 连续（时间） |

**一眼记住**：
> 泊松 = 离散 + $E = \text{Var}$；指数 = 连续 + $E \neq \text{Var}$

---

## Q20: GraphRAG 适用场景

**原题**：
> 在以下问答场景中，哪些是普通 RAG 难以处理、需要借助领域知识图谱实现 GraphRAG 的？
> - A. 知识库包含分散的人物关系描述，用户提问需结合指代消解与多跳逻辑推理得出答案
> - B. 用户查询与知识库中预定义的 FAQ 问题语义高度相似，系统通过向量检索匹配
> - C. 系统存储大量领域论文及其引用网络，用户基于语义查询某研究方向的代表性文献
> - D. 用户希望从多篇娱乐报道中找出被高频提及的核心人物，系统需跨文档识别并归一化人物名称

**正确答案**：A、D

**你的选择**：A（漏选 D）

**易错点**：
- D 选项的"跨文档实体识别/归一化 + 共现统计"是知识图谱的典型应用
- B、C 都是纯语义检索，普通 RAG 即可

**核心公式/结论**：
GraphRAG 适用场景：
1. 多跳推理（A→B→C 的关系链）
2. 实体归一化（不同表述指向同一实体）
3. 聚合统计（需要结构化知识支持）

**一眼记住**：
> GraphRAG = 多跳推理 + 实体归一 + 聚合统计

---

<details>
<summary>📎 追问详情（Q4/Q8/Q9/Q13/Q16）</summary>

### Q4: GELU vs ReLU 梯度详解

**核心原因（用一行式子抓住）**

反传到上一层的梯度会被激活函数导数"门控"：

$$|\nabla_{\text{in}}|_2 \approx | f'(x)\odot \nabla_{\text{out}}|_2$$

所以关键看 $(f'(x))$ 的"平均平方"有多大。

* **ReLU**
$$\text{ReLU}(x)=\max(0,x),\quad \text{ReLU}'(x)=\mathbf 1_{x>0}$$
若初始化时 $x$ 近似对称（常见近似 $x\sim \mathcal N(0,1)$），则
$$\mathbb E[(\text{ReLU}'(x))^2]=P(x>0)=\tfrac12$$

* **GELU（精确定义）**
$$\text{GELU}(x)=x\Phi(x)$$
其中 $\Phi(x)$ 是标准正态 CDF，$\phi(x)$ 是 PDF。导数：
$$\text{GELU}'(x)=\Phi(x)+x\phi(x)$$
对称输入下，$\text{GELU}'(x)$ 在大量区域里 **小于 1**（尤其是负半轴），因此通常有
$$\mathbb E[(\text{GELU}'(x))^2] < \tfrac12$$
→ **梯度 L2 范数更小**

**物理/直觉类比**：
* **ReLU = 理想二极管/硬开关**：要么全通（斜率 1），要么断路（斜率 0）
* **GELU/Swish = 带温度的软阀门**：负半轴不是完全断，而是"漏一点"

---

### Q8: 牛顿法 vs 拟牛顿 vs 高斯-牛顿

**A. 牛顿法（Newton）**
$$x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)}$$
例子：$f(x)=(x-3)^2$，一跳到最优。

**B. 拟牛顿（BFGS）**
不用显式 Hessian，用"梯度变化"去学曲率：
$$f''(x)\approx \frac{g_1-g_0}{x_1-x_0}$$

**C. 高斯-牛顿**
只针对最小二乘：
$$\theta_{k+1}=\theta_k-(J^\top J)^{-1}J^\top r$$

**记忆法**：
* 牛顿/BFGS：二阶（快、稳、每步贵）
* SGD：一阶（省、噪、步长不对就"晃"）

---

### Q9: 为什么 $m>n$ 时 $(AB)x=0$ 必有非零解

关键不等式：
$$\operatorname{rank}(AB)\le \operatorname{rank}(B)\le n$$

当 $m>n$：
$$\operatorname{rank}(AB)\le n<m$$
所以 $AB$ 不可能满秩 ⇒ 必有非零解。

**物理类比**：
$B: \mathbb R^m\to \mathbb R^n$ 是把 **m 维信息压到 n 维**（压缩）。
当 $m>n$，必然有一些方向被压没。

**反例（$n>m$ 不一定有非零解）**：
取 $m=2, n=3$：
$$A=\begin{bmatrix}1&0&0\\0&1&0\end{bmatrix}, B=\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix} \Rightarrow AB=I_2$$
只有零解。

---

### Q13: 齐次坐标变换顺序

**坐标法（最稳）**：
绕 $z$ 轴 $90°$：$(x,y,z)\mapsto (-y,x,z)$
所以 $P(1,2,3)\to(-2,1,3)$。
再平移 $d=(4,5,6)$：
$$(-2,1,3)+(4,5,6)=(2,6,9)$$

**矩阵乘法顺序**（列向量）：
$$p' = D \cdot (R \cdot p) = (DR) \cdot p$$
**先做的在右边，后做的在左边。**

---

### Q16: 互斥与独立详解

**互斥 + 都非零概率 ⇒ 一定不独立**

互斥：$A\cap B=\varnothing \Rightarrow P(A\cap B)=0$

若还满足 $P(A)>0,\ P(B)>0$，则
$$P(A)P(B)>0 \neq 0 = P(A\cap B)$$
所以 **不独立**。

**反例（互斥但独立）**：
取 $A=\varnothing$（$P(A)=0$），$B=\{1\}$
则 $P(A\cap B)=0$，而 $P(A)P(B)=0$，独立成立。

**独立性对补事件保持的证明**：
$$P(A\cap B^c)=P(A)-P(A\cap B)=P(A)-P(A)P(B)=P(A)(1-P(B))=P(A)P(B^c)$$

</details>

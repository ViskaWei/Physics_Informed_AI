# Att ç±»é¢˜ç›®æ±‡æ€» [7/7 å®Œæˆ] âœ…

> ğŸ“Š **è¿›åº¦**: 7/7 å®Œæˆ (100%) ğŸ‰  
> ğŸ”„ **æœ€åæ›´æ–°**: 2026-01-05  
> ğŸ“ **åˆ†ç±»**: att (Attentionã€ViTã€LoRAã€Multi-Headã€Sparse Attentionã€Self-Attention)

---

## ğŸ“‹ é¢˜ç›®æ€»è§ˆ

> ğŸ”¥ **é‡åˆ·ä¼˜å…ˆçº§**: 4 > 3 = 2 > 7 > 6 > 5 > 1ï¼ˆæŒ‰é‡è¦ç¨‹åº¦æ’åºï¼‰

| å‡ºé¢˜æ—¥æœŸ | # | Pç¼–å· | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | å®Œæˆæ—¥æœŸ |
|----------|---|-------|------|------|------|----------|
| 2025-11-20 | 1 | P4481 | ViT Patch Embeddingå±‚å®ç° | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-10-22 | 2 | P4275 | åŸºäºç©ºé—´è¿ç»­å—çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-10-15 | 3 | P4227 | åŠ¨æ€æ³¨æ„åŠ›æ©ç è°ƒåº¦é—®é¢˜ | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-09-28 | 4 | P3843 | Masked Multi-Head Self-Attention å®ç° | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-09-17 | 5 | P3712 | å¤§æ¨¡å‹Attentionæ¨¡å—å¼€å‘ | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-09-12 | 6 | P3658 | æ”¯æŒLoRAçš„Attentionå®ç° | ä¸­ç­‰ | âœ… | 2026-01-02 |
| 2025-09-04 | 7 | P3562 | ä¼ æ„Ÿå™¨æ•°æ®åˆ†æï¼ˆSelf-Attention+FCï¼‰ | ä¸­ç­‰ | âœ… | 2026-01-05 |

ğŸ† **å…¨éƒ¨å®Œæˆï¼** éš¾åº¦/æœ‰ä»·å€¼/äºŒåˆ·é‡ç‚¹ï¼š 4 > 3 = 2 > 7 > 6 > 5 > 1
---

## ğŸ”§ é€šç”¨æ¨¡æ¿

```python
# Scaled Dot-Product Attention
import numpy as np

def attention(Q, K, V, mask=None):
    d_k = Q.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)
    if mask is not None:
        scores = np.where(mask, scores, -1e9)
    weights = softmax(scores)
    return weights @ V

def softmax(x):
    e = np.exp(x - x.max(axis=-1, keepdims=True))
    return e / e.sum(axis=-1, keepdims=True)
```

---

## é¢˜ç›®1: ViT Patch Embeddingå±‚å®ç°ï¼ˆP4481ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬2é¢˜-p4481](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬2é¢˜-p4481)

### é¢˜ç›®æè¿°

Vision Transformer(ViT) æ˜¯è§†è§‰é¢†åŸŸåº”ç”¨éå¸¸å¹¿æ³›çš„åŸºç¡€ç½‘ç»œç»“æ„ï¼Œç»å…¸çš„ ViT ç»“æ„åŒ…å«äº† Patchï¼†Position embeddingã€Transformer embeddingã€Transformer Encoder ç­‰å¤šä¸ªå…³é”®æ¨¡å—ç»„æˆã€‚è¿™å‡ ä¸ªæ¨¡å—ä¸­ï¼Œå°†å›¾åƒåˆ†å‰²ä¸ºå›ºå®šå¤§å°çš„ patch å¹¶è¿›è¡Œçº¿æ€§åµŒå…¥æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œä¹Ÿå³ Patch Embedding å±‚ï¼Œå…¶ä¸»è¦å®ç°æ­¥éª¤ä¸ºï¼š

**Step 1**ï¼šå°†è¾“å…¥å›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªéé‡å çš„ patchï¼Œä¹Ÿå³å°†å›¾ç‰‡åˆ‡åˆ†ä¸º NÃ—N ä¸ª patchï¼Œå¦‚ 3Ã—3 ä¸ª 2D å›¾åƒå—ï¼›

**Step 2**ï¼šå°†æ¯ä¸ª patch å±•å¹³ä¸ºå‘é‡ï¼Œä¹Ÿå³å°†æ¯ä¸ªåˆ‡åˆ†åçš„ 2D Patch å±•å¹³ä¸º 1D å‘é‡;

**Step 3**ï¼šå¯¹å±•å¹³çš„ patch è¿›è¡Œçº¿æ€§å˜æ¢(åµŒå…¥)ï¼Œä¹Ÿå³å¯¹æ¯ä¸ªå±•å¹³åçš„ 1D å‘é‡åšä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ E å’Œåç½®å‘é‡ B è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå…¬å¼ä¸ºï¼š$Z=X*E+b$

**Step 4**ï¼šæ·»åŠ å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼›

è¯·æ ¹æ®ä»¥ä¸Šæç¤ºæ­¥éª¤ï¼Œå®ç° Patch Embedding å±‚ã€‚

**ç‰¹åˆ«æ³¨æ„**ï¼šæœ¬å®ç°è¿‡ç¨‹ä¸­ï¼Œæ— æ³•ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚ pytorchã€tensorflow ç­‰

**è¾“å…¥æè¿°**ï¼šè¾“å…¥å‚æ•°åŒ…æ‹¬ `img_size`ã€`patch_size`ã€`channel`ã€`embedding_dim`ï¼Œåˆ†åˆ«è¡¨ç¤ºï¼š
- å›¾åƒå°ºå¯¸ï¼ˆå›¾åƒé•¿ã€å®½é»˜è®¤ç›¸ç­‰ï¼‰`img_size`
- patch å¤§å° `patch_size`
- å›¾åƒé€šé“æ•° `channels`
- åµŒå…¥ç»´åº¦ `embedding_dim`

**è¾“å‡ºæè¿°**ï¼šè¾“å‡º `patch_embedding` åçš„ç»´åº¦ä¿¡æ¯ `embedding_shape`ï¼Œå…¶ä¸­éœ€è¦åŒ…å« cls tokenï¼Œå…·ä½“å¯è§æ ·ä¾‹ã€‚

### æ ·ä¾‹
```
è¾“å…¥ï¼š
448 32 3 384

è¾“å‡ºï¼š
197 384
```
```
è¾“å…¥ï¼š
224 16 3 768

è¾“å‡ºï¼š
197 768
```

### æ€è·¯

Patch Embedding çš„æœ¬è´¨æ˜¯ä¸€ä¸ªåˆ†å— + å±•å¼€ + çº¿æ€§å˜æ¢çš„è¿‡ç¨‹ï¼Œå¯ä»¥ç†è§£ä¸ºå¯¹å›¾åƒåšä¸€æ¬¡"å·ç§¯æ ¸ä¸º patchï¼Œæ­¥é•¿ä¸º patch_size çš„å·ç§¯ + reshape"ï¼Œå†åŠ ä¸Šä¸€ä¸ª cls tokenã€‚è¿™é‡Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—è¾“å‡ºå‘é‡åºåˆ—çš„ç»´åº¦ï¼Œè€Œä¸æ˜¯å…·ä½“åšçŸ©é˜µè¿ç®—ã€‚

1. **è®¡ç®— patch çš„ä¸ªæ•°**ï¼šå›¾åƒè¢«å‡åŒ€åˆ‡æˆå¤§å°ä¸º patch_size Ã— patch_size çš„ä¸é‡å  patch
   - æ¯ä¸€ç»´ä¸Šçš„ patch ä¸ªæ•°ï¼š$N = \frac{\text{img\_size}}{\text{patch\_size}}$
   - æ€» patch æ•°ç›®ï¼š$\text{num\_patches} = N \times N = \left(\frac{\text{img\_size}}{\text{patch\_size}}\right)^2$

2. **å±•å¼€å¹¶çº¿æ€§å˜æ¢**ï¼šæ¯ä¸ª patch çš„åŸå§‹ç»´åº¦ä¸º $\text{patch\_dim} = \text{patch\_size} \times \text{patch\_size} \times \text{channel}$ï¼Œç»è¿‡çº¿æ€§åµŒå…¥åæ¯ä¸ª patch å˜æˆä¸€ä¸ªé•¿åº¦ä¸º embedding_dim çš„å‘é‡ã€‚

3. **æ·»åŠ  CLS Token**ï¼šViT ä¼šé¢å¤–æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ çš„ cls tokenï¼Œå…¶ç»´åº¦å’Œå•ä¸ª patch çš„åµŒå…¥ç›¸åŒï¼Œä¸º (embedding_dim,)ã€‚æ‹¼æ¥åˆ°åºåˆ—å‰é¢ä¹‹åï¼Œåºåˆ—é•¿åº¦å˜ä¸ºï¼š$\text{num\_tokens} = \text{num\_patches} + 1$

4. **æœ€ç»ˆè¾“å‡ºç»´åº¦**ï¼š$\text{embedding\_shape} = (\text{num\_patches} + 1, \text{embedding\_dim})$

### å¤æ‚åº¦

- **æ—¶é—´å¤æ‚åº¦**ï¼šO(1)ï¼Œåªéœ€è¦è¿›è¡Œç®€å•çš„é™¤æ³•è¿ç®—
- **ç©ºé—´å¤æ‚åº¦**ï¼šO(1)ï¼Œåªéœ€è¦å­˜å‚¨å‡ ä¸ªå˜é‡

### æˆ‘çš„ä»£ç 
```python
I, P, CH, E = map(int, input().split())
num_patch = ((I-1) // P + 1)
print(num_patch **2 + 1, E)
```

---

## é¢˜ç›®2: åŸºäºç©ºé—´è¿ç»­å—çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆP4275ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬3é¢˜-p4275](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬3é¢˜-p4275)

### é¢˜ç›®æè¿°

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ ï¼Œæ ‡å‡† Attention çš„è®¡ç®—å¼€é”€ä»¥ $O(n^2)$ å¢é•¿ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ä¸ºæå‡é•¿åºåˆ—å¤„ç†æ•ˆç‡ï¼Œæå‡ºä¸€ç§åŸºäºç©ºé—´è¿ç»­å—çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚

**å…·ä½“æµç¨‹**ï¼š
1. ä¸€ä¸ªé•¿åº¦ä¸º n çš„å†å² token åºåˆ—ï¼Œæ¯ä¸ª token è¡¨ç¤ºä¸º 1 ä¸ª d ç»´ç‰¹å¾å‘é‡ $x_j \in \mathbb{R}^d$
2. æŒ‰å›ºå®šå—å¤§å° bï¼Œå°†åºåˆ—åˆ’åˆ†ä¸º $m = \lceil n/b \rceil$ ä¸ªç©ºé—´è¿ç»­å—ï¼ˆæœ€åä¸€ä¸ªå—å¯ä¸æ»¡ï¼‰$B_1, B_2, ..., B_m$
3. å¯¹æ¯ä¸ªå— $B_k$ï¼š
   - è®¡ç®—å¹³å‡æ± åŒ–å‘é‡ï¼š$\mathbf{h}_k = \frac{1}{|B_k|} \sum_{x \in B_k} \mathbf{x}$
   - ä½¿ç”¨ä¸€ä¸ªä¸¤å±‚å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰è¿›è¡Œéçº¿æ€§å‹ç¼©ï¼ˆéšè—ç»´åº¦ $d_l = 1$ï¼‰ï¼š$\mathbf{c}_k = W_2 \cdot \sigma(W_1 \cdot \mathbf{h}_k + b_1) + b_2$
     - å…¶ä¸­ $W_1 \in \mathbb{R}^{1 \times d}$ï¼Œ$W_2 \in \mathbb{R}^{d \times 1}$ï¼Œè¾“å‡º $c_k \in \mathbb{R}^d$
     - $b_1 = 2$ï¼Œ$b_2 = 1$
     - $\sigma(x) = \max(0, x)$ï¼ˆå³ ReLU æ¿€æ´»å‡½æ•°ï¼‰
4. ç»™å®šæŸ¥è¯¢å‘é‡ $\mathbf{q} \in \mathbb{R}^d$ï¼ˆé¢˜ç›®ä¸­å›ºå®šä¸ºå…¨ 1 å‘é‡ï¼š$q_i = 1$ï¼‰ï¼Œè®¡ç®—æ¯ä¸ªå‹ç¼©å—çš„æ³¨æ„åŠ›å¾—åˆ†ï¼š$a_k = \frac{\mathbf{q} \cdot \mathbf{c}_k}{\sqrt{d}}$
5. å°†åºåˆ— A åˆ’åˆ†ä¸ºæ°å¥½ 2 ä¸ªè¿ç»­éç©ºå­æ•°ç»„ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–è¿™ä¸¤ä¸ªå­æ•°ç»„å’Œä¸­çš„æœ€å°å€¼ S
6. æœ€ç»ˆè¾“å‡ºè¯¥æœ€å¤§åŒ–çš„æœ€å°å€¼ S çš„æ•´æ•°åŒ–å¾—åˆ†ï¼Œå³ $round(100 \cdot S)$

**è¾“å…¥æè¿°**ï¼šç¬¬ 1 è¡Œï¼šn d bï¼›æ¥ä¸‹æ¥ n è¡Œï¼šæ¯è¡Œ d ä¸ªæ•°ï¼Œè¡¨ç¤º $x_i$ï¼›å€’æ•°ç¬¬ 2 è¡Œï¼šd ä¸ªæ•°ï¼Œè¡¨ç¤º $W_1$ï¼›æœ€å 1 è¡Œï¼šd ä¸ªæ•°ï¼Œè¡¨ç¤º $W_2$

**è¾“å‡ºæè¿°**ï¼šè¿”å›ä¸€ä¸ªæ•´æ•°ï¼Œå³ä¸Šè¿°æ­¥éª¤ 5 çš„æ•´æ•°åŒ–å¾—åˆ†

### æ ·ä¾‹
```
è¾“å…¥ï¼š
3 1 1
2.0
4.0
6.0
1.0
2.0

è¾“å‡ºï¼š
1700
```
```
è¾“å…¥ï¼š
3 2 1
2.0 1.0
3.0 2.0
4.0 3.0
1.0 0.5
2.0 1.0

è¾“å‡ºï¼š
1732

Input:
6 2 2
5.000000 -2.000000
7.000000 4.000000
-1.000000 5.000000
-3.000000 2.000000
3.000000 6.000000
-2.000000 3.000000
3.000000 -2.000000
-3.000000 -2.000000
Out:
-6081
```

### æ€è·¯

æ•´ä½“å¯åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

1. **æ•°å€¼æ„é€ **ï¼šåˆ†å— + æ± åŒ– + MLP + æ‰“åˆ†
   - è®¾åºåˆ—é•¿åº¦ä¸º nã€ç»´åº¦ä¸º dã€å—å¤§å°ä¸º bï¼Œå—æ•° $m = \lceil n/b \rceil$
   - ç¬¬ k ä¸ªå—çš„å‡å€¼æ± åŒ–ï¼š$h_k = \frac{1}{|B_k|}\sum_{x \in B_k} x \in \mathbb{R}^d$
   - ä¸¤å±‚ MLPï¼ˆéšè—ç»´åº¦ä¸º 1ï¼‰ï¼š$t_k = W_1 \cdot h_k + b_1$ï¼Œ$r_k = \sigma(t_k) = \max(0, t_k)$ï¼Œ$c_k = W_2 \cdot r_k + b_2 \in \mathbb{R}^d$
   - å› ä¸º $q = \mathbf{1}$ï¼Œæ³¨æ„åŠ›å¾—åˆ†ï¼š$a_k = \frac{\sum_{i=1}^{d} c_k^{(i)}}{\sqrt d}$

2. **æœ€ä¼˜åˆ’åˆ†**ï¼šå‰ç¼€å’Œ + è´ªå¿ƒ
   - ç›®æ ‡æ˜¯ $\max_{1 \le s \le m-1} \min(\sum_{i=1}^{s}a_i, \sum_{i=s+1}^{m}a_i)$
   - è®°æ€»å’Œ $T = \sum_{i=1}^{m}a_i$ï¼Œå‰ç¼€å’Œ $P_s = \sum_{i=1}^{s}a_i$
   - æœ€ä¼˜ s ä½¿å¾—ä¸¤æ®µå°½é‡"å‡è¡¡"ï¼Œå³ $P_s$ æœ€æ¥è¿‘ $T/2$
   - å®ç°ä¸Šåªéœ€ä¸€æ¬¡çº¿æ€§æ‰«æï¼šç»´æŠ¤å‰ç¼€å’Œï¼Œé€ä¸ªè®¡ç®— $\min(P_s, T-P_s)$ çš„æœ€å¤§å€¼å³å¯

### å¤æ‚åº¦

- **æ—¶é—´å¤æ‚åº¦**ï¼š$O(n \cdot d)$
  - è®¡ç®—æ‰€æœ‰å—å‡å€¼ä¸ MLPï¼šéå†æ¯ä¸ª token å„ç»´åº¦ï¼Œ$O(n \cdot d)$
  - è®¡ç®—æ‰“åˆ†å¹¶å¯»æ‰¾æœ€ä¼˜åˆ‡åˆ†ç‚¹ï¼š$O(m)$ï¼Œå…¶ä¸­ $m = \lceil n/b \rceil \le n$
- **ç©ºé—´å¤æ‚åº¦**ï¼š$O(d + m)$ï¼Œå¯é™åˆ° $O(d)$ï¼ˆè¾¹ç®—è¾¹ç´¯è®¡ï¼Œä¸å¿…å­˜æ•´åˆ—ï¼‰

### æˆ‘çš„ä»£ç 
```python
import numpy as np
N, D, B = map(int, input().split()); M = (N-1) // B + 1;
X = np.array([list(map(float, input().split())) for _ in range(N)])
W1 = np.array(list(map(float, input().split()))); W2 = np.array(list(map(float, input().split()))); 
b1 = 2; b2 = 1; A = [0] * M
for m in range(M):
    Bk = X[m*B: (m+1) * B]  # B x D
    hk = Bk.mean(axis=0) # D
    ck = W2 * max(W1 @ hk + b1, 0) + b2
    A[m] = ck.sum() / np.sqrt(D)
total = sum(A); maxx = -float('inf'); prefix = 0;
for i in range(M-1):
    prefix += A[i]
    other = total - prefix
    maxx = max(maxx,  min(prefix, other))
print(f"{maxx * 100:.0f}")
```

---

## é¢˜ç›®3: åŠ¨æ€æ³¨æ„åŠ›æ©ç è°ƒåº¦é—®é¢˜ï¼ˆP4227ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬2é¢˜-p4227](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬2é¢˜-p4227)

### é¢˜ç›®æè¿°

ä½ æ­£åœ¨è®¾è®¡ä¸€ç§è·¨æ¨¡æ€çŸ¥è¯†çš„å¤§æ¨¡å‹ç²¾å‡†åº¦æœºåˆ¶ï¼Œç»™å®šä¸€ä¸ªé•¿åº¦ä¸º n çš„è¾“å…¥ token åºåˆ—ï¼Œæ¯ä¸ªä½ç½® j æ‹¥æœ‰ä¸€ä¸ª d ç»´ç‰¹å¾å‘é‡ $X_j \in \mathbb{R}^d$ å’Œä¸€ä¸ªæ­£æ•´æ•°è®¡ç®—å®¹é‡ $c_j$ï¼Œè¡¨ç¤ºè¯¥ä½ç½®æœ€å¤šå¯æ¥æ”¶æ¥è‡ªå‰ j ä½ç½®çš„ä¿¡æ¯è¿æ¥æ•°ã€‚

**ç³»ç»Ÿéœ€å®Œæˆä»¥ä¸‹æ­¥éª¤**ï¼š

1. **RMSNorm å½’ä¸€åŒ–**ï¼šå¯¹æ‰€æœ‰ç‰¹å¾å‘é‡è¿›è¡Œ RMSNorm å½’ä¸€åŒ–ï¼ˆæœ¬é¢˜å– $\gamma = 1, \epsilon = 0$ï¼‰ï¼š
   - æ¯ä¸ªç‰¹å¾å‘é‡è®°ä¸º $x_i \in \mathbb{R}^d$ï¼Œå…¶ç¬¬ k ä¸ªåˆ†é‡ä¸º $x_i[k]$
   - RMSNorm å®šä¹‰ä¸ºï¼š$\hat{X_i} = \frac{x_i}{\sqrt{\frac{1}{d}\sum_{k=1}^{d}x_i[k]^2 + \epsilon}} \cdot \gamma$

2. **æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—**ï¼šè®¡ç®—æ¯å¯¹ä½ç½® $i < j$ çš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œä½¿ç”¨æ ‡å‡†ç¼©æ”¾ç‚¹ç§¯å…¬å¼ï¼ˆåŸºäº RMSNorm å½’ä¸€åŒ–å‘é‡ï¼‰ï¼š
   - $A_{ij} = \frac{\hat{x_i} \cdot \hat{x_j}}{\sqrt{d}}$

3. **æ©ç çŸ©é˜µæ„é€ **ï¼šæ„é€ ä¸‹ä¸‰è§’æ³¨æ„åŠ›æ©ç çŸ©é˜µ $M \in \{0,1\}^{n \times n}$ï¼Œæ»¡è¶³å…¥åº¦çº¦æŸï¼š
   - $\forall j \in [0, n), \sum_{i=0}^{j-1} M_{ij} \leq c_j$

4. **ç›®æ ‡å‡½æ•°æœ€å¤§åŒ–**ï¼šæœ€å¤§åŒ–å…¨å±€æ³¨æ„åŠ›ä¿¡æ¯æ€»é‡ï¼Œå®šä¹‰ä¸ºæ‰€æœ‰æ¿€æ´»è¿æ¥çš„å¹³æ–¹æ³¨æ„åŠ›å¾—åˆ†ä¹‹å’Œï¼š
   - $S = \sum_{j=0}^{n-1} \sum_{i=0}^{j-1} M_{ij} \cdot A_{ij}^2$

5. **è¾“å‡ºæ•´æ•°åŒ–å¾—åˆ†**ï¼šæœ€ç»ˆè¿”å›å°†æœ€å¤§åŒ– S ä¹˜ä»¥ 100 åå››èˆäº”å…¥å¾—åˆ°çš„æ•´æ•°ï¼š$round(100 \cdot S)$

**è¾“å…¥æè¿°**ï¼šç¬¬ 1 è¡Œï¼šn dï¼›æ¥ä¸‹æ¥ n è¡Œï¼šæ¯è¡Œ d ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤º $x_j$ï¼›æœ€å 1 è¡Œï¼šn ä¸ªæ­£æ•´æ•°ï¼Œè¡¨ç¤º $c_j$

**è¾“å‡ºæè¿°**ï¼šè¿”å›ä¸€ä¸ªæ•´æ•°ï¼Œå³ä¸Šè¿°æ­¥éª¤ 5 çš„æ•´æ•°åŒ–å¾—åˆ†

### æ ·ä¾‹
```
è¾“å…¥ï¼š
4 2
2.0 2.0
3.0 0.0
0.0 4.0
1.0 1.0
1 2 1 3

è¾“å‡ºï¼š
600
```
```
è¾“å…¥ï¼š
3 2
1.0 0.0
0.0 1.0
1.0 1.0
1 1 2

è¾“å‡ºï¼š
200
```

### æ€è·¯

æœ¬é¢˜çš„æ ¸å¿ƒæ˜¯åœ¨èµ„æºçº¦æŸä¸‹æœ€å¤§åŒ–æ³¨æ„åŠ›ä¿¡æ¯æ€»é‡ã€‚é—®é¢˜å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **RMSNorm å½’ä¸€åŒ–**ï¼šå¯¹äºæ¯ä¸ª d ç»´ç‰¹å¾å‘é‡ï¼Œè®¡ç®—å…¶å‡æ–¹æ ¹å€¼ï¼Œç„¶åå°†å‘é‡çš„æ¯ä¸ªåˆ†é‡é™¤ä»¥è¯¥å‡æ–¹æ ¹å€¼ã€‚

2. **è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†**ï¼šå¯¹äºä»»æ„ä¸¤ä¸ªä½ç½® i å’Œ jï¼ˆå…¶ä¸­ $i < j$ï¼‰ï¼Œä½¿ç”¨å½’ä¸€åŒ–åçš„å‘é‡è¿›è¡Œç¼©æ”¾ç‚¹ç§¯è¿ç®—ï¼Œå¾—åˆ°æ³¨æ„åŠ›å¾—åˆ† $A_{ij}$ï¼Œå¹¶è®¡ç®—å…¶å¹³æ–¹å€¼ $A_{ij}^2$ã€‚

3. **è´ªå¿ƒé€‰æ‹©**ï¼šå¯¹äºæ¯ä¸ªä½ç½® jï¼Œéœ€è¦ä»å‰é¢çš„æ‰€æœ‰ä½ç½®ä¸­é€‰æ‹©æœ€å¤š $c_j$ ä¸ªä½ç½®å»ºç«‹è¿æ¥ã€‚ä¸ºäº†æœ€å¤§åŒ–ç›®æ ‡å‡½æ•° Sï¼Œåº”å½“é‡‡ç”¨è´ªå¿ƒç­–ç•¥ï¼šå¯¹äºæ¯ä¸ªä½ç½® jï¼Œå°†æ‰€æœ‰å‰ç½®ä½ç½®æŒ‰ç…§ $A_{ij}^2$ çš„å€¼ä»å¤§åˆ°å°æ’åºï¼Œç„¶åé€‰æ‹©å‰ $c_j$ ä¸ªæœ€å¤§çš„å€¼ã€‚

4. **è´ªå¿ƒç­–ç•¥çš„æ­£ç¡®æ€§**ï¼šç›®æ ‡å‡½æ•° S æ˜¯æ‰€æœ‰æ¿€æ´»è¿æ¥çš„ $A_{ij}^2$ ä¹‹å’Œï¼Œæ¯ä¸ªä½ç½®çš„é€‰æ‹©æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤å±€éƒ¨æœ€ä¼˜è§£ï¼ˆæ¯ä¸ªä½ç½®é€‰æ‹©æœ€å¤§çš„ $c_j$ ä¸ªå€¼ï¼‰å¿…ç„¶èƒ½å¯¼è‡´å…¨å±€æœ€ä¼˜è§£ã€‚

### å¤æ‚åº¦

- **æ—¶é—´å¤æ‚åº¦**ï¼š$O(n^2 \cdot d)$
  - RMSNorm å½’ä¸€åŒ–ï¼š$O(n \cdot d)$
  - è®¡ç®—æ‰€æœ‰æ³¨æ„åŠ›å¾—åˆ†å¹³æ–¹ï¼š$O(n^2 \cdot d)$
  - è´ªå¿ƒé€‰æ‹©ï¼š$O(n^2)$ï¼ˆæ¯ä¸ªä½ç½®æœ€å¤š n ä¸ªå‰ç½®ä½ç½®ï¼Œéœ€è¦æ’åºï¼‰
- **ç©ºé—´å¤æ‚åº¦**ï¼š$O(n^2)$ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µ

### æˆ‘çš„ä»£ç 
```python
import numpy as np
N, D = map(int, input().split())
Xj = np.array([list(map(float, input().split())) for _ in range(N)])
Cj = np.array(list(map(int, input().split())))
M = 1 - np.triu(np.ones((N,N)))
dom = np.sqrt((Xj**2).mean(axis=1, keepdims=True))
Xnorm = np.divide(Xj, dom+1e-12)
A = Xnorm @ Xnorm.T / np.sqrt(D); A2 = M * A**2
S = 0
for i in range(1, N):
    max_idx = min(i, Cj[i])
    S+= np.partition(A2[i][:i], -max_idx)[-max_idx:].sum()
print(f"{100*S:.0f}")
```

---

## é¢˜ç›®4: Masked Multi-Head Self-Attention å®ç°ï¼ˆP3843ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬3é¢˜-p3843](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬3é¢˜-p3843)

### é¢˜ç›®æè¿°

åœ¨ Transformer æ¨¡å‹ä¸­ï¼ŒMulti-Head Self-Attention æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºæ•æ‰åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚ä½ éœ€è¦ä»å¤´å®ç°ä¸€ä¸ª Masked Multi-Head Self-Attention å‡½æ•°ï¼Œæ”¯æŒè‡ªæ³¨æ„åŠ›ï¼ˆå³ queriesã€keys å’Œ values æ¥è‡ªåŒä¸€è¾“å…¥åºåˆ—ï¼‰ï¼Œå¹¶å¤„ç†ç¼–ç ï¼ˆmaskï¼‰ä»¥é˜²æ­¢æœªæ¥ä½ç½®çš„ä¿¡æ¯æ³„éœ²ï¼ˆå¸¸è§äº Decoder ä¸­ï¼‰ã€‚

**å…·ä½“è¦æ±‚**ï¼š

1. **æ”¯æŒå¤šå¤´æ³¨æ„åŠ›**ï¼šå°†æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œåˆ†æˆå¤šä¸ª"å¤´"ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
2. **è®¡ç®—è¿‡ç¨‹**ï¼š
   - ç”Ÿæˆ Qã€Kã€V çŸ©é˜µï¼šå¯¹è¾“å…¥åºåˆ— Xï¼ˆç»´åº¦ï¼š[batch_size, seq_len, d_model]ï¼‰é€šè¿‡ 3 ä¸ªçº¿æ€§å±‚åˆ†åˆ«ç”ŸæˆæŸ¥è¯¢ï¼ˆQuery, Qï¼‰ã€é”®ï¼ˆKey, Kï¼‰ã€å€¼ï¼ˆValue, Vï¼‰çŸ©é˜µï¼š$Q = X \cdot W_Q$ï¼Œ$K = X \cdot W_K$ï¼Œ$V = X \cdot W_V$
   - å°† Qã€Kã€V æ‹†åˆ†ä¸ºå¤šä¸ªå¤´ï¼šåˆ†å‰²ä¸º num_heads ä¸ªå¹¶è¡Œçš„å­çŸ©é˜µï¼ˆæ¯ä¸ªå¤´çš„ç»´åº¦ä¸º $d_k = d_{model} / num_{heads}$ï¼‰
   - å¯¹äºæ¯ä¸ªå¤´ï¼Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š$attention\_scores = (Q \cdot K^T) / \sqrt{d_k}$
   - æä¾› maskï¼ˆä¸€ä¸ª (batch_size, seq_len, seq_len) çš„å¸ƒå°”æ•°ç»„ï¼Œå…¶ä¸­ True è¡¨ç¤ºéœ€è¦æ©ç çš„ä½ç½®ï¼‰ï¼Œåˆ™å°† masked ä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼ˆ-infï¼‰
   - å¯¹æ©ç åçš„åˆ†æ•°åº”ç”¨ softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
   - è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼š$attention = softmax\_scores \cdot V$
   - æ‹¼æ¥å¤šå¤´è¾“å‡ºï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§æŠ•å½±å¾—åˆ°æœ€ç»ˆç»“æœï¼š$output = concat(attention_1, ..., attention_{num\_heads}) \cdot W_O$

**è¾“å…¥æè¿°**ï¼šä»¥";"åˆ†éš”ï¼Œåˆ†åˆ«ä¸º num_heads, X, Qã€Kã€Vï¼Œ$W_O$

**è¾“å‡ºæè¿°**ï¼šè¾“å‡ºä¸ºæœ€ç»ˆç»“æœ outputï¼Œè¾“å‡ºä¿ç•™ä¸¤ä½æœ‰æ•ˆå°æ•°ï¼Œå¹¶ä¸”ä¸º List

### æ ·ä¾‹
```
è¾“å…¥ï¼š
2;[[[ 1.92, 1.48], [0.67, -1.23], [0.35, -0.68]], [[-1.11, 0.09], [-0.3, -0.39], [-0.59, -0.06]]];[[1.0, 2.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]]

è¾“å‡ºï¼š
[[[14.64, 14.64], [-5.36, -5.36], [-4.44, -4.44]], [[-2.79, -2.79], [-3.04, -3.04], [-2.79, -2.79]]]
```
```
è¾“å…¥ï¼š
2;[[[ 1.92, 1.48], [0.67, -1.23], [0.35, -0.68]], [[-1.11, 0.09], [-0.3, -0.39], [-0.59, -0.06]]];[[1.0,1.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]];[[1.0, 1.0], [2.0, 2.0]]

è¾“å‡ºï¼š
[[[14.64, 14.64], [-5.37, -5.37], [-4.62, -4.62]], [[-2.79, -2.79], [-3.03, -3.03], [-2.77, -2.77]]]
è¾“å…¥ï¼š
2;[[[1.17, 0.14], [-0.18, -0.17]], [[0.73, 0.15], [-1.95, 0.02]]];[[-0.07, -1.26], [0.64, 1.41]];[[-1.77, -0.78], [-1.03, 0.42]];[[0.82, 1.50], [0.32, 1.25]];[[0.39, 0.09], [2.00, -1.66]]
è¾“å‡ºï¼š
[[[4.25, -3.11], [1.65, -1.17]], [[2.82, -2.07], [-6.08, 4.69]]]

2;[[[0.81, 1.47, 0.82, 0.40], [-1.74, -0.73, 1.43, -1.08], [1.88, 1.30, 0.04, 1.28], [0.35, 1.32, -1.12, 0.12], [-1.05, -0.60, -1.78, -0.10]], [[0.74, -1.81, 1.72, 0.10], [0.89, -1.37, 1.10, 1.71], [-1.06, -1.51, -0.37, -1.96], [0.06, -0.34, -0.51, 0.10], [0.19, 0.52, -1.03, -1.50]], [[-0.26, 0.22, 0.77, 0.37], [-0.75, -1.97, 1.18, -0.02], [0.10, -1.93, -0.76, -1.73], [-0.80, 0.27, 0.07, -1.53], [-0.83, 1.99, 0.33, 0.14]]];[[-1.35, -0.93, 1.49, 0.97], [-0.34, -0.34, 0.62, -1.45], [-0.54, 1.67, 1.23, -0.75], [0.55, 0.87, -0.94, 1.91]];[[-1.85, 1.17, -0.59, 0.37], [1.09, 1.84, 1.01, 0.45], [0.24, -1.49, -0.43, 1.44], [0.67, 0.47, 1.68, 1.75]];[[0.49, 1.17, 1.10, -0.03], [-0.40, 1.99, -1.51, 1.71], [1.87, -0.74, -0.86, -1.23], [-0.65, -1.48, -1.72, -0.14]];[[-1.10, 1.23, 1.24, 0.45], [-0.72, 0.94, -0.28, -0.28], [-1.21, 1.88, -0.24, -0.96], [1.28, -0.56, 0.98, -0.67]]

[[[2.00, -2.07, 2.64, 1.40], [-6.48, 5.07, -2.11, 1.79], [4.00, -4.28, 6.29, 2.65], [-0.05, -0.76, 4.49, 2.84], [-4.33, 1.91, 1.62, 4.15]], [[-10.68, 7.81, 0.82, 4.77], [-2.57, -1.69, 0.13, 5.74], [-8.34, 9.96, -0.23, -2.00], [-7.13, 3.96, -1.34, 4.11], [-7.67, 9.68, 4.17, -0.79]], [[1.16, -2.97, 1.35, 2.97], [-4.75, 2.32, -1.79, 2.87], [-5.63, 4.47, 1.43, 2.13], [-10.22, 12.57, -0.16, -3.01], [-11.78, 15.12, -4.82, -5.75]]]

```

### æ€è·¯

æœ¬é¢˜è¦æ±‚æ‰‹å†™ã€Œå¸¦å› æœæ©ç ã€çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆDecoder å¸¸ç”¨ï¼‰ï¼Œæ•´ä½“æµç¨‹ï¼š

1. **çº¿æ€§æ˜ å°„ç”Ÿæˆ Q/K/V**ï¼š$Q = X W_Q, K = X W_K, V = X W_V$ï¼Œç»´åº¦ï¼š[B, S, d_model]

2. **åˆ†å¤´**ï¼šå°†æœ€åä¸€ç»´ d_model å‡åˆ†ä¸º num_heads ä¸ªå¤´ï¼Œæ¯å¤´ç»´åº¦ $d_k = d_{model} / num_{heads}$ï¼Œå¹¶é‡æ’ä¸º $Q_h, K_h, V_h \in [B, H, S, d_k]$

3. **æ¯å¤´è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼š$\text{scores} = \frac{Q_h K_h^\top}{\sqrt{d_k}} \in [B,H,S,S]$

4. **å› æœæ©ç ï¼ˆé˜²æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰**ï¼šæ„é€ ä¸‹ä¸‰è§’ Maskï¼ˆ[S,S]ï¼Œä¸‹ä¸‰è§’ä¸º 1ï¼Œä¸Šä¸‰è§’ä¸º 0ï¼‰ï¼Œå¹¿æ’­åˆ° [B,H,S,S]ã€‚å°†ä¸Šä¸‰è§’ï¼ˆä¸å…è®¸å…³æ³¨çš„ï¼‰ä½ç½®ç½®ä¸º $-\infty$ï¼š$\text{masked\_scores} = \text{where}(mask=0, -\infty, \text{scores})$

5. **Softmax å¾—æ³¨æ„åŠ›æƒé‡**ï¼šæŒ‰æœ€åä¸€ç»´ S åšå½’ä¸€åŒ–ï¼Œæ•°å€¼ç¨³å®šï¼šå‡å»è¡Œæœ€å¤§å€¼ï¼Œ$\alpha = \text{softmax}(\text{masked\_scores})$

6. **èšåˆå¾—åˆ°æ¯å¤´è¾“å‡º**ï¼š$\text{head} = \alpha V_h \in [B,H,S,d_k]$

7. **æ‹¼æ¥å„å¤´å¹¶åšè¾“å‡ºæŠ•å½±**ï¼šå…ˆå°†å„å¤´åœ¨ $d_k$ ç»´æ‹¼æ¥å› $d_{model}$ï¼š[B,S,HÂ·$d_k$] = [B,S,$d_{model}$]ï¼Œå†ä¹˜ä»¥ $W_O$ï¼š$\text{output} = \text{concat(heads)} W_O \in [B,S,d_{model}]$

### å¤æ‚åº¦

è®¾æ‰¹æ¬¡ Bã€åºåˆ—é•¿åº¦ Sã€æ¨¡å‹ç»´åº¦ $D = d_{model}$ã€å¤´æ•° Hã€æ¯å¤´ç»´åº¦ $d_k = D/H$ã€‚

- **æ—¶é—´å¤æ‚åº¦**ï¼š$O(B \cdot S \cdot D^2 + B \cdot S^2 \cdot D)$
  - çº¿æ€§æ˜ å°„ï¼š$O(B \cdot S \cdot D^2)$
  - æ³¨æ„åŠ› QK^Tï¼šæ¯å¤´ $O(S^2 \cdot d_k)$ï¼Œæ€»è®¡ $O(B \cdot H \cdot S^2 \cdot d_k) = O(B \cdot S^2 \cdot D)$
  - ä¹˜ V èšåˆï¼šåŒé˜¶ $O(B \cdot S^2 \cdot D)$
  - è¾“å‡ºæŠ•å½±ï¼š$O(B \cdot S \cdot D^2)$

- **ç©ºé—´å¤æ‚åº¦**ï¼š$O(B \cdot S \cdot D + B \cdot H \cdot S^2)$ï¼Œä¸»è¦å­˜å‚¨ Q/K/Vã€æ³¨æ„åŠ›åˆ†æ•°ä¸æƒé‡

### æˆ‘çš„ä»£ç 
```python
import numpy as np
from ast import literal_eval
H, XX, Wq, Wk, Wv, Wo= list(map(lambda x: np.array(literal_eval(x), dtype=float), input().strip().split(';'))); 
H = int(H); B, L, D = XX.shape; Dk = D // H 
QQ, KK, VV = [np.transpose((XX @ WW).reshape(B, L, H, Dk), (0,2,1,3)) for WW in [Wq, Wk, Wv]]
scores = (QQ @ np.transpose(KK, (0,1,3,2))/ np.sqrt(Dk)); 

MM = np.tril(np.ones((L, L)))[None, None, :, :]; 
scores = np.where(MM == 1, scores, -np.inf)

exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
softmax = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True); 
A = (np.transpose(softmax @ VV, (0,2,1,3)).reshape(B, L, D) @ Wo).round(2)
s = np.array2string(A, precision=2, separator=', ', formatter = {'float_kind': lambda x: f"{x:.2f}" if abs(x) >=0.005 else "0.00"})
print(' '.join(s.split()))
```

---

## é¢˜ç›®5: å¤§æ¨¡å‹Attentionæ¨¡å—å¼€å‘ï¼ˆP3712ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬2é¢˜-p3712](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬2é¢˜-p3712)

### é¢˜ç›®æè¿°

å·²çŸ¥å¤§æ¨¡å‹å¸¸ç”¨çš„ Attention æ¨¡å—å®šä¹‰å¦‚ä¸‹ï¼š

$$Y = \text{softmax}\left(\frac{QK^T}{\sqrt{h}}\right)V$$

æ­¤å¤„è€ƒè™‘äºŒç»´æƒ…å†µï¼Œå…¶ä¸­ï¼š
- $Q, K, V = XW_1, XW_2, XW_3 \in \mathbb{R}^{n \times h}$
- $X \in \mathbb{R}^{n \times m}$
- $W_1, W_2, W_3 \in \mathbb{R}^{m \times h}$

**æ³¨æ„**ï¼š
- ä¸ºç®€ä¾¿èµ·è§ï¼Œæ‰€æœ‰è¾“å…¥åˆå§‹åŒ–ä¸ºå…¨ 1 çŸ©é˜µï¼Œæ‰€æœ‰æƒé‡çŸ©é˜µåˆå§‹åŒ–ä¸ºä¸Šä¸‰è§’å…¨ 1 çŸ©é˜µ
- å¯¹ä»»æ„çŸ©é˜µ M çš„ softmax è®¡ç®—ç®€åŒ–ä¸ºï¼š$\text{softmax}(M)_{ij} = \frac{M_{ij}}{M_i}$ï¼Œå…¶ä¸­ $M_i = \sum_j M_{ij}$

**è¾“å…¥æè¿°**ï¼šè¾“å…¥ä¸ºç»´åº¦å‚æ•° n, m å’Œ hï¼Œå‚æ•°é—´ä½¿ç”¨ç©ºæ ¼éš”å¼€ï¼Œå‡ä¸ºå°äº 100 çš„æ­£æ•´æ•°

**è¾“å‡ºæè¿°**ï¼šè¾“å‡ºä¸ºç»“æœçŸ©é˜µ $Y \in \mathbb{R}^{n \times h}$ çš„æ‰€æœ‰å…ƒç´ ä¹‹å’Œï¼Œåœ¨å››èˆäº”å…¥åä¿ç•™æ•´æ•°

### æ ·ä¾‹
```
è¾“å…¥ï¼š
3 3 3

è¾“å‡ºï¼š
18
```
```
è¾“å…¥ï¼š
2 3 1

è¾“å‡ºï¼š
2
```
```
è¾“å…¥ï¼š
91 100 71

è¾“å‡ºï¼š
232596
```

### æ€è·¯

æŒ‰é¢˜æ„ç”¨"æš´åŠ›æ¨¡æ‹Ÿ"å®Œæ•´èµ°ä¸€éè®¡ç®—å›¾ï¼š

1. **æ„é€ çŸ©é˜µ**ï¼šæ„é€  X ä¸º nÃ—m çš„å…¨ 1ï¼›æ„é€  $W_1$ã€$W_2$ã€$W_3$ ä¸º mÃ—h çš„ä¸Šä¸‰è§’å…¨ 1ï¼ˆä¸Šä¸‰è§’çŸ©é˜µï¼šä¸»å¯¹è§’çº¿åŠä»¥ä¸Šçš„å…ƒç´ ä¸º 1ï¼Œä»¥ä¸‹ä¸º 0ï¼‰

2. **è®¡ç®— Q, K, V**ï¼š$Q = X \cdot W_1$ï¼Œ$K = X \cdot W_2$ï¼Œ$V = X \cdot W_3$ï¼ˆæ™®é€šä¸‰é‡å¾ªç¯çŸ©é˜µä¹˜æ³•ï¼‰

3. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼š$M = (Q \cdot K^T) / \sqrt{h}$

4. **ç®€åŒ– softmax**ï¼šæŠŠ M çš„æ¯ä¸€è¡Œåšå½’ä¸€åŒ–ï¼š$A[i][j] = M[i][j] / (\text{è¯¥è¡Œå…ƒç´ å’Œ})$

5. **è®¡ç®—è¾“å‡º**ï¼š$Y = A \cdot V$

6. **æ±‚å’Œå¹¶è¾“å‡º**ï¼šå°† Y å…¨éƒ¨å…ƒç´ æ±‚å’Œï¼Œå››èˆäº”å…¥è¾“å‡ºæ•´æ•°

### å¤æ‚åº¦

- **æ—¶é—´å¤æ‚åº¦**ï¼š$O(n \cdot m \cdot h + n^2 \cdot h)$
  - è®¡ç®— Qã€Kã€Vï¼š$O(n \cdot m \cdot h)$
  - è®¡ç®— $M = Q \cdot K^T$ï¼š$O(n^2 \cdot h)$
  - è¡Œå½’ä¸€åŒ–ï¼š$O(n^2)$
  - è®¡ç®— $Y = A \cdot V$ï¼š$O(n^2 \cdot h)$

- **ç©ºé—´å¤æ‚åº¦**ï¼š$O(n \cdot h + n^2)$ï¼ˆä¿å­˜ Qã€Kã€Vã€A æˆ– M ç­‰ä¸­é—´ç»“æœï¼‰

### æˆ‘çš„ä»£ç 
```python
import numpy as np
L, D, H = map(int, input().split())
X = np.ones((L, D)); W = np.triu(np.ones((D, H))); Q = K = V = X @ W
M = Q @ K.T/ np.sqrt(H)
A = np.divide(M , M.sum(axis=1) + 1e-12)
Y = A @ V
print(f"{Y.sum():.0f}")
```

---

## é¢˜ç›®6: æ”¯æŒLoRAçš„Attentionå®ç°ï¼ˆP3658ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æº**: [core46#ç¬¬3é¢˜-p3658](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md#ç¬¬3é¢˜-p3658)

### é¢˜ç›®æè¿°

ç›¸å¯¹äºå…¨é‡å¾®è°ƒï¼ŒLoRA å¾®è°ƒæå‡ºäº†ä¸€ç§ä½ç§©åˆ†è§£çš„æ–¹æ³•ï¼Œåªéœ€åœ¨åŸæ¨¡å‹å‚æ•°åŸºç¡€ä¸Šå¢åŠ å°‘é‡çš„å¯è®­ç»ƒå‚æ•°ï¼Œå¤§å¹…é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºåŸå§‹çš„é¢„è®­ç»ƒæƒé‡çŸ©é˜µ Wï¼ŒLoRA åšä»¥ä¸‹æ”¹è¿›ï¼š

$$W' = W + B \times A$$

å…¶ä¸­ï¼š
- W ä¸ºåŸå§‹æƒé‡ï¼ˆå†»ç»“ä¸å˜ï¼‰
- $B \in \mathbb{R}^{d \times r}$ å’Œ $A \in \mathbb{R}^{r \times d}$ ä¸ºæ–°å¢çš„ä½ç§©çŸ©é˜µï¼Œ$r \ll d$ï¼Œç§© r ä¸€èˆ¬å¾ˆå°
- å¾®è°ƒæ—¶åªæ›´æ–° Aã€B è¿™ä¸¤ä¸ªçŸ©é˜µï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒçš„å‚æ•°æ•°é‡

è¯·å®ç°æ”¯æŒ LoRA çš„ Attention è®¡ç®—å‡½æ•° `LoRA_Attention(x, W_q, W_k, W_v, A, B)`ã€‚ä¸ºç®€åŒ–å®ç°ï¼Œä»…éœ€æ”¯æŒ Attention ä¸­ Q çš„ LoRA ç»“æ„å®ç°å³å¯ã€‚å®ç°æ—¶è¯·ä½¿ç”¨ float64 ä½ç²¾åº¦ã€‚

**è¾“å…¥æè¿°**ï¼š
- ç¬¬ 1 è¡Œï¼šb, d, rï¼Œå…¶ä¸­ b ä¸º batch sizeï¼Œd ä¸ºç‰¹å¾çš„é•¿åº¦ï¼Œr ä¸º LoRA çŸ©é˜µçš„ç§©ï¼Œ$b \geq 1, d \geq 1, r \geq 0$
- ç¬¬ 2 è¡Œï¼šè¾“å…¥ xï¼Œé•¿åº¦ä¸º $b \times d$
- ç¬¬ 3-5 è¡Œï¼š$W_q, W_k, W_v$ï¼Œé•¿åº¦ä¸º $d \times d$
- è‹¥ $r > 0$ï¼Œåˆ™ï¼š
  - ç¬¬ 6 è¡Œï¼šAï¼Œé•¿åº¦ä¸º $r \times d$
  - ç¬¬ 7 è¡Œï¼šBï¼Œé•¿åº¦ä¸º $d \times r$

**è¾“å‡ºæè¿°**ï¼šLoRA Attention è®¡ç®—çš„ç»“æœï¼Œè¾“å‡ºä¿ç•™å››ä½å°æ•°ï¼Œä¸è¶³å››ä½å°æ•°çš„è¡¥ 0

### æ ·ä¾‹
```
è¾“å…¥ï¼š
2 5 3
-0.58 -0.52 -0.02 0.56 0.79 0.06 -0.64 -0.04 -0.20 -0.38
0.24 -0.72 -0.66 0.96 0.02 -0.43 -0.24 0.19 -0.85 -0.35 0.69 -0.09 0.99 0.21 -0.06 0.55 0.57 0.97 0.58 -0.16 0.64 0.02 -0.71 0.53 -0.90
0.07 -0.16 -0.47 -0.32 -0.92 0.13 -0.74 -0.87 0.05 0.33 0.37 0.75 0.57 0.14 -0.62 0.67 -0.62 -0.85 0.09 -0.90 0.22 0.97 -0.68 0.61 0.48
0.39 -0.74 0.84 0.21 0.44 -0.59 -0.07 -0.84 -0.70 0.86 -0.12 -0.06 0.45 -0.43 -0.09 -0.73 0.56 -0.62 0.36 -0.87 -0.97 -0.48 0.71 0.07 -0.28
0.25 0.58 -0.04 -0.94 0.45 -0.60 0.89 0.94 0.35 -0.76 -0.47 -0.40 0.10 0.23 0.25
-0.18 -0.11 0.60 0.37 0.75 0.51 -0.76 -0.39 -0.81 -0.88 -0.43 -0.88 0.15 -0.46 -0.24

è¾“å‡ºï¼š
0.3499 0.0803 0.0376 -0.1791 0.3952 0.4112 0.2240 -0.0239 -0.2177 0.4478
```
```
è¾“å…¥ï¼š
1 3 2
0.58 -0.65 -0.63
-0.74 -0.71 0.65 0.70 -0.14 0.01 -0.84 0.20 0.25
-0.60 0.51 -0.12 -0.35 0.57 -0.38 -0.44 -0.82 0.53
0.14 0.03 -0.27 0.10 -0.12 0.85 -0.55 0.10 -0.43
0.65 0.32 -0.42 -0.62 -0.88 -0.70
-0.66 0.49 0.09 -0.21 0.48 0.41

è¾“å‡ºï¼š
0.2318 -0.3995 -0.1131
```

### æ€è·¯

1. **LoRA æ€è·¯**ï¼š
   - åŸå§‹æƒé‡ $W_q$ å†»ç»“
   - æ–°å¢ä½ç§©çŸ©é˜µ $A \in \mathbb{R}^{r \times d}, B \in \mathbb{R}^{d \times r}$ï¼Œå½¢æˆï¼š$W_q' = W_q + B \times A$
   - è‹¥ $r = 0$ï¼Œç›´æ¥ç”¨åŸå§‹ $W_q$

2. **Attention è®¡ç®—æ­¥éª¤**ï¼š
   - è®¡ç®— $Q = XW_q'^T$ï¼Œ$K = XW_k^T$ï¼Œ$V = XW_v^T$
   - æ‰“åˆ†å¹¶ç¼©æ”¾ï¼š$S = \frac{QK^T}{\sqrt{d}}$
   - å¯¹æ¯ä¸€è¡Œåšç¨³å®š softmaxï¼ˆå‡å»è¡Œæœ€å¤§å€¼ï¼‰
   - è¾“å‡ºï¼š$O = \text{softmax}(S) \cdot V$

3. **å®ç°è¦ç‚¹**ï¼š
   - float64 ç²¾åº¦ï¼Œé¿å…æº¢å‡º
   - softmax æ—¶å¯¹æ¯è¡Œå‡å»æœ€å¤§å€¼
   - è¾“å‡ºæ‹‰å¹³ï¼Œä¿ç•™å››ä½å°æ•°ï¼Œ-0.0000 ç‰¹åˆ¤ä¸º 0.0000

### å¤æ‚åº¦

- **æ—¶é—´å¤æ‚åº¦**ï¼š$O(b \cdot d^2 + b^2 \cdot d)$
  - LoRA æƒé‡è®¡ç®—ï¼š$O(d \cdot r \cdot d) = O(d^2 \cdot r)$ï¼ˆå½“ $r \ll d$ æ—¶å¯å¿½ç•¥ï¼‰
  - è®¡ç®— Qã€Kã€Vï¼š$O(b \cdot d^2)$
  - è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š$O(b^2 \cdot d)$
  - Softmax å’Œè¾“å‡ºï¼š$O(b^2 \cdot d)$

- **ç©ºé—´å¤æ‚åº¦**ï¼š$O(b \cdot d + b^2)$ï¼Œä¸»è¦å­˜å‚¨ Qã€Kã€V å’Œæ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ

### æˆ‘çš„ä»£ç 
```python
import numpy as np
B, D, R = map(int, input().split())
X = np.array(list(map(float, input().split()))).reshape(B, D) # 2x5
Wq = np.array(list(map(float, input().split()))).reshape(D, D)
Wk = np.array(list(map(float, input().split()))).reshape(D, D) # 5x5
Wv = np.array(list(map(float, input().split()))).reshape(D, D)
if R > 0:
    A1 = np.array(list(map(float, input().split()))).reshape(R, D) # 3 x 5
    A2 = np.array(list(map(float, input().split()))).reshape(D, R) # 5 x 3
Q = X @ (Wq + A2 @ A1 if R > 0 else 0).T; K = X @ Wk.T; V = X @ Wv.T; QK = Q @ K.T / np.sqrt(D); 
def softmax(x, axis=-1):
    x = x.astype(np.float64)                # é¢˜ç›®å¸¸è¦æ±‚ float64
    x = x - np.max(x, axis=axis, keepdims=True)  # æ•°å€¼ç¨³å®š
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
A = softmax(QK); O = (A @ V).reshape(-1)
print(" ".join([f"{xx:.4f}" for xx in O]))
```

---

## é¢˜ç›®7: ä¼ æ„Ÿå™¨æ•°æ®åˆ†æï¼ˆP3562ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: Self-Attention + FC æ¨ç†
- **æº**: [core46#ç¬¬3é¢˜-p3562](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- ç½‘ç»œç»“æ„ï¼šä¸¤å±‚ Self-Attention â†’ ä¸¤å±‚ FC
- è¾“å…¥ï¼šLÃ—D çš„åºåˆ—
- Attentionï¼š$\text{softmax}(\frac{QK^T}{\sqrt{d}})V$
- æ— éçº¿æ€§æ¿€æ´»å‡½æ•°

### å…³é”®è§„åˆ™
1. æ¯å±‚æœ‰ Wq, Wk, Wvï¼ˆDÃ—Dï¼‰
2. FC æœ‰æƒé‡ W å’Œåç½® b
3. è¾“å‡ºæ ¼å¼ï¼šé€—å·åˆ†éš”ï¼Œä¿ç•™ 2 ä½å°æ•°

### æ ·ä¾‹
```
è¾“å…¥:
4,1
1.00,-3.00,9.50,6.50
-0.20
0.45
...

è¾“å‡º:
0.04,0.04,0.05,0.05
```

### æ€è·¯
1. è§£æè¾“å…¥ï¼ˆé€—å·åˆ†éš”ï¼‰
2. ä¸¤å±‚ Attention + FC
3. softmax éœ€è¦æ•°å€¼ç¨³å®šï¼ˆå‡æœ€å¤§å€¼ï¼‰

### å¤æ‚åº¦
- æ—¶é—´: O(LÂ² Â· D + L Â· DÂ²)
- ç©ºé—´: O(LÂ² + L Â· D)

### æˆ‘çš„ä»£ç 
```python
import sys
import numpy as np
import math

def softmax_rows(M):
    mx = np.max(M, axis=1, keepdims=True)
    E = np.exp(M - mx)
    return E / np.sum(E, axis=1, keepdims=True)

def attn(X, Wq, Wk, Wv, D):
    Q, K, V = X @ Wq, X @ Wk, X @ Wv
    S = (Q @ K.T) / math.sqrt(D)
    return softmax_rows(S) @ V

def main():
    lines = [sys.stdin.readline().strip() for _ in range(12)]
    L, D = map(int, lines[0].split(','))

    def parse(idx, cnt):
        return np.array(list(map(float, lines[idx].split(',')))), idx + 1

    idx = 1
    seq, idx = parse(idx, L*D)
    seq = seq.reshape(L, D)

    Wq1, idx = parse(idx, D*D); Wq1 = Wq1.reshape(D, D)
    Wk1, idx = parse(idx, D*D); Wk1 = Wk1.reshape(D, D)
    Wv1, idx = parse(idx, D*D); Wv1 = Wv1.reshape(D, D)
    Wfc1, idx = parse(idx, D*D); Wfc1 = Wfc1.reshape(D, D)
    bfc1, idx = parse(idx, D)

    Wq2, idx = parse(idx, D*D); Wq2 = Wq2.reshape(D, D)
    Wk2, idx = parse(idx, D*D); Wk2 = Wk2.reshape(D, D)
    Wv2, idx = parse(idx, D*D); Wv2 = Wv2.reshape(D, D)
    Wfc2, idx = parse(idx, D*D); Wfc2 = Wfc2.reshape(D, D)
    bfc2, idx = parse(idx, D)

    Y1 = attn(seq, Wq1, Wk1, Wv1, D)
    Z1 = Y1 @ Wfc1 + bfc1
    Y2 = attn(Z1, Wq2, Wk2, Wv2, D)
    Z2 = Y2 @ Wfc2 + bfc2

    out = Z2.flatten()
    print(",".join(f"{x:.2f}" for x in out))

if __name__ == "__main__":
    main()
```

---

## ğŸ“Œ æ˜“é”™ç‚¹æ€»ç»“

1. **Self-Attention ç¼©æ”¾**ï¼šé™¤ä»¥ $\sqrt{d}$ï¼Œä¸æ˜¯ $d$
2. TODO

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- æºæ–‡ä»¶ï¼š`../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md`
- ç´¢å¼•ï¼š`../ai_core46_index.md`

---

## ğŸ“ ä»£ç ç­”æ¡ˆ

### é¢˜ç›®1: ViT Patch Embeddingå±‚å®ç°ï¼ˆP4481ï¼‰

```python
# è®¡ç®— Patch Embedding è¾“å‡ºç»´åº¦çš„å‡½æ•°
def get_embedding_shape(img_size, patch_size, channel, embedding_dim):
    # æ¯ä¸€ç»´ä¸Šçš„ patch ä¸ªæ•°
    num_per_dim = img_size // patch_size
    # æ€»çš„ patch ä¸ªæ•°
    num_patches = num_per_dim * num_per_dim
    # åŠ ä¸Šä¸€ä¸ª cls token
    num_tokens = num_patches + 1
    # è¿”å› (åºåˆ—é•¿åº¦, åµŒå…¥ç»´åº¦)
    return num_tokens, embedding_dim

def main():
    # è¯»å–è¾“å…¥ï¼šimg_size patch_size channel embedding_dim
    img_size, patch_size, channel, embedding_dim = map(int, input().split())
    # è°ƒç”¨å‡½æ•°è®¡ç®—ç»“æœ
    tokens, dim = get_embedding_shape(img_size, patch_size, channel, embedding_dim)
    # æŒ‰é¢˜ç›®è¦æ±‚è¾“å‡º
    print(tokens, dim)

if __name__ == "__main__":
    main()
```

### é¢˜ç›®2: åŸºäºç©ºé—´è¿ç»­å—çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆP4275ï¼‰

```python
import sys
import math
import numpy as np

# æ ¸å¿ƒåŠŸèƒ½ï¼šæ ¹æ®é¢˜æ„è®¡ç®—æœ€ç»ˆæ•´æ•°åŒ–å¾—åˆ†
def solve(n: int, d: int, b: int, X: np.ndarray, W1: np.ndarray, W2: np.ndarray) -> int:
    m = (n + b - 1) // b  # å—æ•°
    A = []  # å‹ç¼©æ³¨æ„åŠ›å¾—åˆ†åºåˆ—

    sqrt_d = math.sqrt(d)

    # é€å—è®¡ç®— a_k
    for k in range(m):
        start = k * b
        end = min((k + 1) * b, n)
        block = X[start:end]  # è¯¥å—çš„æ‰€æœ‰ tokenï¼Œå½¢çŠ¶ (len, d)

        # å¹³å‡æ± åŒ– h_k
        h_k = block.mean(axis=0)

        # ä¸¤å±‚ MLPï¼št = W1Â·h + b1ï¼Œr = ReLU(t)ï¼Œc = W2*r + b2(é€ç»´åŠ 1)
        t = float(W1.dot(h_k)) + 2.0
        r = max(0.0, t)
        c = W2 * r + 1.0  # å¹¿æ’­åŠ  1
        a_k = float(c.sum()) / sqrt_d
        A.append(a_k)

    # çº¿æ€§æ‰«æå¯»æ‰¾æœ€ä¼˜åˆ‡åˆ†ç‚¹ï¼Œä½¿ min(å·¦å’Œ, å³å’Œ) æœ€å¤§
    T = sum(A)
    best = -1e100
    pref = 0.0
    for s in range(1, m):  # å¿…é¡»åˆ‡æˆä¸¤ä¸ªéç©ºæ®µ
        pref += A[s - 1]
        best = max(best, min(pref, T - pref))

    S = best
    return int(round(S * 100.0))

def main():
    data = sys.stdin.read().strip().split()
    it = iter(data)

    # è¯»å…¥ n d b
    n = int(next(it)); d = int(next(it)); b = int(next(it))

    # è¯»å…¥ n è¡Œï¼Œæ¯è¡Œ d ä¸ªæµ®ç‚¹
    xs = [ [float(next(it)) for _ in range(d)] for _ in range(n) ]
    X = np.array(xs, dtype=float)

    # è¯»å…¥ W1, W2ï¼ˆå„ d ä¸ªæ•°ï¼‰
    W1 = np.array([float(next(it)) for _ in range(d)], dtype=float)
    W2 = np.array([float(next(it)) for _ in range(d)], dtype=float)

    ans = solve(n, d, b, X, W1, W2)
    print(ans)

if __name__ == "__main__":
    main()
```

### é¢˜ç›®3: åŠ¨æ€æ³¨æ„åŠ›æ©ç è°ƒåº¦é—®é¢˜ï¼ˆP4227ï¼‰

```python
import numpy as np

def solve(n, d, vectors, capacities):
    # æ­¥éª¤1ï¼šå¯¹æ‰€æœ‰ç‰¹å¾å‘é‡è¿›è¡ŒRMSNormå½’ä¸€åŒ–
    normalized = []
    for vec in vectors:
        # è®¡ç®—å‡æ–¹æ ¹å€¼
        rms = np.sqrt(np.mean(np.array(vec) ** 2))
        # å½’ä¸€åŒ–
        normalized.append(np.array(vec) / rms)

    # æ­¥éª¤2ï¼šè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†çš„å¹³æ–¹
    A_squared = [[0.0] * n for _ in range(n)]
    for i in range(n):
        for j in range(i + 1, n):
            # è®¡ç®—ç‚¹ç§¯
            dot_product = np.dot(normalized[i], normalized[j])
            # ç¼©æ”¾å¹¶è®¡ç®—å¹³æ–¹
            A_ij = dot_product / np.sqrt(d)
            A_squared[i][j] = A_ij ** 2

    # æ­¥éª¤3ï¼šè´ªå¿ƒé€‰æ‹©ï¼Œæœ€å¤§åŒ–å…¨å±€æ³¨æ„åŠ›ä¿¡æ¯æ€»é‡S
    S = 0.0
    for j in range(1, n):
        # æ”¶é›†ä½ç½®jçš„æ‰€æœ‰å‰ç½®ä½ç½®çš„æ³¨æ„åŠ›å¾—åˆ†å¹³æ–¹
        scores = []
        for i in range(j):
            scores.append(A_squared[i][j])

        # é™åºæ’åºï¼Œé€‰æ‹©æœ€å¤§çš„c_jä¸ª
        scores.sort(reverse=True)
        S += sum(scores[:capacities[j]])

    # æ­¥éª¤4ï¼šè¾“å‡ºæ•´æ•°åŒ–å¾—åˆ†
    return round(100 * S)

if __name__ == "__main__":
    # è¯»å–nå’Œd
    n, d = map(int, input().split())

    # è¯»å–ç‰¹å¾å‘é‡
    vectors = []
    for _ in range(n):
        vec = list(map(float, input().split()))
        vectors.append(vec)

    # è¯»å–è®¡ç®—å®¹é‡
    capacities = list(map(int, input().split()))

    # è®¡ç®—å¹¶è¾“å‡ºç»“æœ
    result = solve(n, d, vectors, capacities)
    print(result)
```

### é¢˜ç›®4: Masked Multi-Head Self-Attention å®ç°ï¼ˆP3843ï¼‰

```python
import sys
import numpy as np
from ast import literal_eval

def to_str(arr):
    """é€’å½’æŠŠåµŒå¥— list è½¬æˆå­—ç¬¦ä¸²ï¼Œæ•°å€¼å›ºå®šä¸¤ä½å°æ•°ä¸”æ— å¼•å·ï¼›æŠŠ -0.00 è§„æ•´ä¸º 0.00"""
    if isinstance(arr, list):
        return "[" + ", ".join(to_str(x) for x in arr) + "]"
    else:
        # æ•°å€¼åˆ†æ”¯
        v = float(arr)
        s = f"{v:.2f}"
        # è§„æ•´ -0.00 -> 0.00
        if s == "-0.00":
            s = "0.00"
        return s

def softmax_stable(x, axis=-1):
    # æ•°å€¼ç¨³å®š softmax
    m = np.max(x, axis=axis, keepdims=True)
    ex = np.exp(x - m)
    return ex / np.sum(ex, axis=axis, keepdims=True)

def multi_head_self_attention(X, WQ, WK, WV, WO, num_heads):
    B, S, D = X.shape
    assert D % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
    d_k = D // num_heads

    # 1) çº¿æ€§æ˜ å°„
    Q = X @ WQ     # [B,S,D]
    K = X @ WK
    V = X @ WV

    # 2) åˆ†å¤´ -> [B,H,S,d_k]
    def split_heads(t):
        t = t.reshape(B, S, num_heads, d_k)     # [B,S,H,d_k]
        return np.transpose(t, (0, 2, 1, 3))    # [B,H,S,d_k]
    Qh, Kh, Vh = split_heads(Q), split_heads(K), split_heads(V)

    # 3) æ³¨æ„åŠ›åˆ†æ•° [B,H,S,S]
    scores = (Qh @ np.transpose(Kh, (0,1,3,2))) / np.sqrt(d_k)

    # 4) å› æœæ©ç ï¼šå…è®¸å…³æ³¨è‡ªå·±åŠä¹‹å‰ä½ç½® => ä¸‹ä¸‰è§’ä¸º1ï¼Œå…¶ä½™ä¸º0
    mask = np.tril(np.ones((S, S), dtype=np.float32))  # [S,S]
    mask = mask[None, None, :, :]                      # [1,1,S,S] å¹¿æ’­åˆ° [B,H,S,S]
    scores = np.where(mask == 1, scores, -np.inf)

    # 5) softmax
    attn = softmax_stable(scores, axis=-1)  # [B,H,S,S]

    # 6) åŠ æƒæ±‚å’Œ
    heads = attn @ Vh                       # [B,H,S,d_k]

    # 7) æ‹¼å› + è¾“å‡ºæŠ•å½±
    heads = np.transpose(heads, (0, 2, 1, 3))      # [B,S,H,d_k]
    concat = heads.reshape(B, S, D)                # [B,S,D]
    out = concat @ WO                              # [B,S,D]

    return out

def main():
    raw = sys.stdin.read().strip()
    # æŒ‰åˆ†å·åˆ†å‰²ï¼šnum_heads;X;Q;K;V;W_O
    parts = [p.strip() for p in raw.split(';')]
    if len(parts) != 6:
        raise ValueError("è¾“å…¥åº”åŒ…å«6æ®µå‚æ•°ï¼šnum_heads;X;Q;K;V;W_O")

    num_heads = int(parts[0])
    X = np.array(literal_eval(parts[1]), dtype=float)
    WQ = np.array(literal_eval(parts[2]), dtype=float)
    WK = np.array(literal_eval(parts[3]), dtype=float)
    WV = np.array(literal_eval(parts[4]), dtype=float)
    WO = np.array(literal_eval(parts[5]), dtype=float)

    out = multi_head_self_attention(X, WQ, WK, WV, WO, num_heads)
    out = np.around(out, 2)                 # ä¿ç•™ä¸¤ä½å°æ•°
    print(to_str(out.tolist()))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®5: å¤§æ¨¡å‹Attentionæ¨¡å—å¼€å‘ï¼ˆP3712ï¼‰

```python
import sys
import ast
import numpy as np

def solve(n, m, h):
    # 1) æ„é€  X å…¨ 1ï¼ŒW ä¸Šä¸‰è§’å…¨ 1
    X = np.ones((n, m), dtype=float)
    W = np.triu(np.ones((m, h), dtype=float))  # W1=W2=W3 ç›¸åŒ

    # 2) è®¡ç®— Q, K, Vï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
    Q = X @ W
    K = X @ W
    V = X @ W

    # 3) è®¡ç®— M=(QÂ·K^T)/sqrt(h)
    M = (Q @ K.T) / np.sqrt(float(h))

    # 4) "ç®€åŒ– softmax"ï¼šæŒ‰è¡Œé™¤ä»¥è¡Œå’Œ
    row_sum = M.sum(axis=1, keepdims=True)
    A = M / (row_sum + 1e-12)

    # 5) è®¡ç®— Y=AÂ·V å¹¶æ±‚å’Œ
    Y = A @ V
    total = float(Y.sum())

    # 6) å››èˆäº”å…¥è¾“å‡ºæ•´æ•°
    return int(np.rint(total))

def main():
    s = sys.stdin.read().strip()
    try:
        val = ast.literal_eval(s)
        if isinstance(val, (list, tuple)) and len(val) == 3:
            n, m, h = map(int, val)
        else:
            n, m, h = map(int, s.split())
    except Exception:
        n, m, h = map(int, s.split())

    print(solve(n, m, h))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®6: æ”¯æŒLoRAçš„Attentionå®ç°ï¼ˆP3658ï¼‰

```python
import sys
import numpy as np

def softmax(x):
    """
    è®¡ç®—softmaxå‡½æ•°
    """
    x = x.astype(np.float64)
    max_vals = np.max(x, axis=1, keepdims=True)
    exp_vals = np.exp(x - max_vals)
    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)

def LoRA_Attention(x, wq, wk, wv, A, B):
    """
    å®ç°å¸¦æœ‰LoRAé€‚é…å™¨çš„çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶
    """
    d = x.shape[1]

    # åº”ç”¨LoRAé€‚é…å™¨ï¼ˆå¦‚æœæä¾›ï¼‰
    if A is not None and B is not None and A.size > 0 and B.size > 0:
        effective_wq = wq + B @ A
    else:
        effective_wq = wq

    # è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼
    Q = x @ effective_wq.T
    K = x @ wk.T
    V = x @ wv.T

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scale_factor = 1.0 / np.sqrt(d)
    attention_scores = (Q @ K.T) * scale_factor

    # åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
    attention_weights = softmax(attention_scores)

    # è®¡ç®—è¾“å‡º
    output = attention_weights @ V
    return output

def format_output(values):
    """
    æ ¼å¼åŒ–è¾“å‡ºï¼Œç¡®ä¿-0.0000æ˜¾ç¤ºä¸º0.0000
    """
    formatted_values = []
    for value in values:
        formatted = f"{value:.4f}"
        if formatted == "-0.0000":
            formatted = "0.0000"
        formatted_values.append(formatted)
    return formatted_values

def main():
    # è¯»å–è¾“å…¥æ•°æ®
    data = list(map(float, sys.stdin.read().strip().split()))
    it = iter(data)

    # è¯»å–ç»´åº¦å‚æ•°
    b = int(next(it))
    d = int(next(it))
    r = int(next(it))

    # è¯»å–è¾“å…¥çŸ©é˜µ
    x = np.array([next(it) for _ in range(b * d)]).reshape(b, d)

    # è¯»å–æƒé‡çŸ©é˜µ
    wq = np.array([next(it) for _ in range(d * d)]).reshape(d, d)
    wk = np.array([next(it) for _ in range(d * d)]).reshape(d, d)
    wv = np.array([next(it) for _ in range(d * d)]).reshape(d, d)

    # è¯»å–LoRAé€‚é…å™¨å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if r > 0:
        A = np.array([next(it) for _ in range(r * d)]).reshape(r, d)
        B = np.array([next(it) for _ in range(d * r)]).reshape(d, r)
    else:
        A = None
        B = None

    # è®¡ç®—è¾“å‡º
    output = LoRA_Attention(x, wq, wk, wv, A, B)

    # æ ¼å¼åŒ–å’Œæ‰“å°ç»“æœ
    flat_output = output.reshape(-1)
    formatted_output = format_output(flat_output)
    print(" ".join(formatted_output))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®7: ä¼ æ„Ÿå™¨æ•°æ®åˆ†æï¼ˆP3562ï¼‰

```python
import sys # 8,55
for i, line in enumerate(sys.stdin):
    it = iter(line.strip().split(','))
    if i == 0: 
        L, D = int(next(it)),int(next(it))
    if i == 1:
        X = [[float(next(it)) for _ in range(D)] for _ in range(L)]
    if i == 2:
        Wq1 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 3:
        Wk1 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i ==4:
        Wv1 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 5:
        W1 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 6:
        b1 = [float(next(it)) for _ in range(D)] 
    if i == 7:
        Wq2 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 8:
        Wk2 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 9:
        Wv2 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 10:
        W2 = [[float(next(it)) for _ in range(D)] for _ in range(D)]
    if i == 11:
        b2 = [float(next(it)) for _ in range(D)] 
import numpy as np
def softmax(x):
    exps = np.exp(x-np.max(x,axis=-1, keepdims=True)) 
    return exps/exps.sum(axis=-1, keepdims=True)
X,Wq1,Wk1,Wv1,W1,b1,Wq2,Wk2,Wv2,W2,b2 = list(map(np.array, [X,Wq1,Wk1,Wv1,W1,b1,Wq2,Wk2,Wv2,W2,b2]))
Q1,K1,V1 = X @ Wq1,X @ Wk1,X @ Wv1; A=softmax(Q1 @ K1.T/np.sqrt(D)) @ V1;
H1 = A @ W1 + b1;
Q2,K2,V2 = H1 @ Wq2,H1 @ Wk2,H1 @ Wv2; A2=softmax(Q2 @ K2.T/np.sqrt(D)) @ V2;
H2 = A2 @ W2 + b2
print(",".join(f"{a:.2f}" for a in H2.ravel()))
```

# Reg ç±»é¢˜ç›®æ±‡æ€» [4/6 å®Œæˆ]

> ğŸ“Š **è¿›åº¦**: 4/6 å®Œæˆ (67%)  
> ğŸ”„ **æœ€åæ›´æ–°**: 2026-01-06  
> ğŸ“ **åˆ†ç±»**: reg (çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€æ•…éšœé¢„æµ‹)

---

## ğŸ“‹ é¢˜ç›®æ€»è§ˆ

> ğŸ”¥ **é‡åˆ·ä¼˜å…ˆçº§**: 1 > 3 > 2 > 5 > 4 > 6ï¼ˆæŒ‰éš¾åº¦å’Œé‡è¦ç¨‹åº¦æ’åºï¼‰

| å‡ºé¢˜æ—¥æœŸ | # | Pç¼–å· | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | å®Œæˆæ—¥æœŸ |
|----------|---|-------|------|------|------|----------|
| 2025-12-17 | 1 | P4532 | ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹æ‰‹æœºå”®ä»· â­ | ä¸­ç­‰ | âœ… | 2026-01-05 |
| 2025-11-06 | 2 | P4447 | åŒ»ç–—è¯Šæ–­æ¨¡å‹çš„è®­ç»ƒä¸æ›´æ–° | ä¸­ç­‰ | âœ… | 2026-01-05 |
| 2025-10-29 | 3 | P4344 | å•†å“è´­ä¹°é¢„æµ‹ï¼ˆé€»è¾‘å›å½’+L2æ­£åˆ™ï¼‰ | ä¸­ç­‰ | âœ… | 2026-01-05 |
| 2025-10-10 | 4 | P3872 | åŸºäºé€»è¾‘å›å½’çš„æ„å›¾åˆ†ç±»å™¨ | ä¸­ç­‰ | âœ… | 2026-01-05 |
| 2025-09-18 | 5 | P3719 | æ•°æ®ä¸­å¿ƒæ°´æ¸©è°ƒèŠ‚æ¡£ä½å†³ç­– | ä¸­ç­‰ | âŒ | - |
| 2025-09-03 | 6 | P3552 | äº‘å­˜å‚¨è®¾å¤‡æ•…éšœé¢„æµ‹ï¼ˆæ•°æ®æ¸…æ´—+é€»è¾‘å›å½’ï¼‰ | ä¸­ç­‰ | âŒ | - |

---

## ğŸ”§ é€šç”¨æ¨¡æ¿

### çº¿æ€§å›å½’æ¨¡æ¿ï¼ˆæ­£è§„æ–¹ç¨‹ + é«˜æ–¯æ¶ˆå…ƒï¼‰
```python
H1=X @ W1; H2=H1 @ W2; Y=H2.mean(0)
dY=Y - Y0; loss=(dY**2).mean()
gY=2/K*dY; gH2=np.ones((L,1)) * gY/L;
gW2 = H1.T @ gH2; gH1 =gH2 @ W2.T; gW1 = X.T @ gH1;
W2 -= eta * gW2; W1 -= eta * gW1;
# linear solve
X, X1 = list(map(lambda x: np.hstack([np.ones((len(x), 1)), x]), [X,X1])); X,Y = X[:,:4],X[:,-1]
W = np.linalg.solve(X.T @ X, X.T @ Y)   # (4,)
y_pred = np.rint(X1 @ W).astype(int)

def linear_regression(X, y):
    """
    æœ€å°äºŒä¹˜æ³•çº¿æ€§å›å½’
    X: n x d ç‰¹å¾çŸ©é˜µï¼ˆä¸å«åç½®åˆ—ï¼‰
    y: n x 1 æ ‡ç­¾
    è¿”å›: (d+1,) æƒé‡å‘é‡ [w0, w1, ..., wd]
    """
    n, d = len(X), len(X[0])
    # æ·»åŠ åç½®åˆ—
    X_aug = [[1.0] + list(X[i]) for i in range(n)]
    
    # è®¡ç®— X^T X å’Œ X^T y
    dim = d + 1
    XTX = [[0.0] * dim for _ in range(dim)]
    XTy = [0.0] * dim
    
    for i in range(n):
        for a in range(dim):
            XTy[a] += X_aug[i][a] * y[i]
            for b in range(dim):
                XTX[a][b] += X_aug[i][a] * X_aug[i][b]
    
    # é«˜æ–¯æ¶ˆå…ƒæ±‚è§£
    A = [XTX[i] + [XTy[i]] for i in range(dim)]
    for i in range(dim):
        pivot = A[i][i]
        for j in range(i, dim + 1):
            A[i][j] /= pivot
        for k in range(dim):
            if k != i:
                factor = A[k][i]
                for j in range(i, dim + 1):
                    A[k][j] -= factor * A[i][j]
    
    return [A[i][dim] for i in range(dim)]
```

### é€»è¾‘å›å½’æ¨¡æ¿
```python
import math

def sigmoid(z):
    """æ•°å€¼ç¨³å®šçš„ sigmoid"""
    if z >= 0:
        return 1.0 / (1.0 + math.exp(-z))
    else:
        ez = math.exp(z)
        return ez / (1.0 + ez)

def logistic_regression(X, y, lr=0.01, epochs=100, lam=0.0):
    """
    æ‰¹é‡æ¢¯åº¦ä¸‹é™è®­ç»ƒé€»è¾‘å›å½’
    X: n x d, y: n (0/1)
    è¿”å›: w (d,), b (scalar)
    """
    n, d = len(X), len(X[0])
    w = [0.0] * d
    b = 0.0
    
    for _ in range(epochs):
        grad_w = [0.0] * d
        grad_b = 0.0
        for i in range(n):
            z = b + sum(w[j] * X[i][j] for j in range(d))
            p = sigmoid(z)
            diff = p - y[i]
            for j in range(d):
                grad_w[j] += diff * X[i][j]
            grad_b += diff
        # åŠ  L2 æ­£åˆ™
        for j in range(d):
            grad_w[j] = grad_w[j] / n + (lam / n) * w[j]
        grad_b /= n
        # æ›´æ–°
        for j in range(d):
            w[j] -= lr * grad_w[j]
        b -= lr * grad_b
    
    return w, b
```

---

## é¢˜ç›®1: ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹æ‰‹æœºå”®ä»·ï¼ˆP4532ï¼‰â­

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: æ­£è§„æ–¹ç¨‹ + é«˜æ–¯æ¶ˆå…ƒ
- **æº**: [core46#ç¬¬2é¢˜-p4532](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- ç»™å®š K éƒ¨æ‰‹æœºçš„ä¸‰é¡¹è¯„åˆ†ï¼ˆç¡¬ä»¶èƒ½åŠ›ã€ç³»ç»Ÿæµç•…åº¦ã€AIèƒ½åŠ›ï¼‰å’Œå”®ä»·
- ä½¿ç”¨æœ€å°äºŒä¹˜æ³•å»ºç«‹çº¿æ€§å›å½’æ¨¡å‹ï¼š$y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3$
- é¢„æµ‹ N éƒ¨å¾…ä¸Šå¸‚æ‰‹æœºçš„å”®ä»·

### å…³é”®è§„åˆ™
1. ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ³•ï¼š$(X^T X) W = X^T Y$
2. é«˜æ–¯æ¶ˆå…ƒæ±‚è§£ 4Ã—4 çº¿æ€§æ–¹ç¨‹ç»„
3. **å››èˆäº”å…¥å–æ•´æ•°**

### æ ·ä¾‹
```
è¾“å…¥:
10
86 99 20 3595 175 171 90 6596 ...ï¼ˆK=10ä¸ªæ‰‹æœºï¼Œæ¯ä¸ª4ä¸ªæ•°å­—ï¼‰
2
159 135 173 120 144 59

è¾“å‡º:
7116 5120
```

### æ€è·¯
1. æ„é€ å¢å¹¿çŸ©é˜µ $X$ï¼ˆæ·»åŠ å…¨1åç½®åˆ—ï¼‰
2. è®¡ç®— $X^T X$ï¼ˆ4Ã—4ï¼‰å’Œ $X^T Y$ï¼ˆ4Ã—1ï¼‰
3. é«˜æ–¯æ¶ˆå…ƒæ±‚è§£æƒé‡ $W$
4. å¯¹æ–°æ•°æ®é¢„æµ‹å¹¶å››èˆäº”å…¥

### å¤æ‚åº¦
- æ—¶é—´: O(K + N)ï¼ˆçŸ©é˜µè¿ç®—ä¸ºå¸¸æ•°çº§ 4Ã—4ï¼‰
- ç©ºé—´: O(1)

### æˆ‘çš„ä»£ç 
```python
import sys
it = iter(sys.stdin.read().strip().split())
K = int(next(it)); X=[[int((next(it))) for _ in range(4)] for _ in range(K)]; M=int(next(it)); X1=[[int((next(it))) for _ in range(3)] for _ in range(M)];
import numpy as np
X, X1 = list(map(lambda x: np.hstack([np.ones((len(x), 1)), x]), [X,X1])); X,Y = X[:,:4],X[:,-1]
W = np.linalg.solve(X.T @ X, X.T @ Y)   # (4,)
y_pred = np.rint(X1 @ W).astype(int)
print(*y_pred)
```

---

## é¢˜ç›®2: åŒ»ç–—è¯Šæ–­æ¨¡å‹çš„è®­ç»ƒä¸æ›´æ–°ï¼ˆP4447ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: ä¸¤å±‚ MLP + åå‘ä¼ æ’­ + SGD
- **æº**: [core46#ç¬¬2é¢˜-p4447](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- è¾“å…¥ï¼šL ä¸ªæ—¶åˆ»çš„ç—‡çŠ¶åºåˆ—ï¼ˆæ¯æ—¶åˆ» D ç»´ç‰¹å¾ï¼‰
- æ¨¡å‹ï¼šä¸¤å±‚ MLPï¼ˆæ— åç½®ï¼Œæ— æ¿€æ´»å‡½æ•°ï¼‰
  - ç¬¬ä¸€å±‚ï¼š$h_t = x_t W_{mlp}$ï¼ˆDÃ—Dï¼‰
  - åˆ†ç±»å±‚ï¼š$p_t = h_t W_{cls}$ï¼ˆDÃ—Kï¼‰
- è¾“å‡ºå–åºåˆ—å¹³å‡ï¼š$\hat{y} = \frac{1}{L}\sum_t p_t$
- æŸå¤±ï¼šMSE = $\frac{1}{K}\sum_i (\hat{y}_i - y_i)^2$
- SGD æ›´æ–°æƒé‡

### å…³é”®è§„åˆ™
1. æ—  softmaxã€æ— åç½®ã€æ— æ¿€æ´»å‡½æ•°
2. è¾“å‡ºæ ¼å¼ï¼šé€—å·åˆ†éš”ï¼Œä¿ç•™ 2 ä½å°æ•°
3. æ¢¯åº¦ï¼šåˆ©ç”¨é“¾å¼æ³•åˆ™æ¨å¯¼

### æ ·ä¾‹
```
è¾“å…¥:
4,2,5,1.0
0.10,0.20,0.30,0.25,0.15
0.0,1.0,-1.5,2.5,3.0,-0.5,0.7,0.3
0.6,-0.4,0.2,0.9
0.5,0.1,-0.3,0.8,0.0,-0.2,0.4,0.6,-0.5,1.0

è¾“å‡º:
0.14,0.26,0.16,0.13,0.52
0.04
0.61,-0.48,0.21,0.78
0.49,0.09,-0.27,0.82,-0.07,-0.21,0.39,0.63,-0.48,0.92
```

### æ€è·¯
1. å‰å‘ï¼šè®¡ç®— $h_t$ã€$p_t$ã€$\hat{y}$
2. æŸå¤±ï¼šMSE
3. åå‘ï¼š$g = \frac{2}{K}(\hat{y} - y)$ï¼Œé“¾å¼æ±‚ $\nabla W_{cls}$ å’Œ $\nabla W_{mlp}$
4. SGD æ›´æ–°

### å¤æ‚åº¦
- æ—¶é—´: O(LÂ·DÂ² + LÂ·DÂ·K)
- ç©ºé—´: O(LÂ·D + DÂ² + DÂ·K)

### æˆ‘çš„ä»£ç 
```python
import sys 
for i, line in enumerate(sys.stdin):
    if i == 0: 
        it=iter(line.strip().split(','))
        L,D,K,eta=int(next(it)),int(next(it)),int(next(it)),float(next(it))
    if i == 1:
        Y0K = list(map(float, line.strip().split(',')))
    if i == 2:
        LD_all = list(map(float, line.strip().split(',')))
        LD = [LD_all[i*D : (i+1)*D] for i in range(L) ]
    if i == 3:
        Wdd_all = list(map(float, line.strip().split(',')))
        Wdd = [Wdd_all[i*D : (i+1)*D] for i in range(D) ]
    if i == 4:
        Wdk_all = list(map(float, line.strip().split(',')))
        Wdk = [Wdk_all[i*K : (i+1)*K] for i in range(D) ]
import numpy as np
Y0, X, W1, W2 = map(lambda a: np.asarray(a, float), [Y0K, LD, Wdd, Wdk])
H1=X @ W1; H2=H1 @ W2; Y=H2.mean(0)
dY=Y - Y0; loss=(dY**2).mean()
gY=2/K*dY; gH2=np.ones((L,1)) * gY/L;
gW2 = H1.T @ gH2; gH1 =gH2 @ W2.T; gW1 = X.T @ gH1;
W2 -= eta * gW2; W1 -= eta * gW1;

print(",".join(f"{y:.2f}" for y in Y))
print(f"{loss:.2f}")
print(",".join(f"{x:.2f}"  for x in W1.ravel()))
print(",".join(f"{x:.2f}"  for x in W2.ravel()))
```

---

## é¢˜ç›®3: å•†å“è´­ä¹°é¢„æµ‹ï¼ˆP4344ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: é€»è¾‘å›å½’ + L2æ­£åˆ™ + æ¢¯åº¦ä¸‹é™
- **æº**: [core46#ç¬¬3é¢˜-p4344](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- ç‰¹å¾ï¼šå¹´é¾„ã€æœˆæ”¶å…¥ã€æµè§ˆæ—¶é•¿ï¼ˆ3ç»´ï¼‰
- æ ‡ç­¾ï¼šæ˜¯å¦è´­ä¹°ï¼ˆ0/1ï¼‰
- æ¨¡å‹ï¼š$p = \sigma(w^T x + b)$
- æŸå¤±ï¼šäº¤å‰ç†µ + L2æ­£åˆ™ï¼š$\frac{\lambda}{2n}\|w\|_2^2$

### å…³é”®è§„åˆ™
1. æ‰¹é‡æ¢¯åº¦ä¸‹é™
2. ç»ˆæ­¢æ¡ä»¶ï¼šè¾¾åˆ° max_iter æˆ–æŸå¤±å˜åŒ– < tol
3. é˜ˆå€¼ 0.5 åˆ¤æ–­ç±»åˆ«
4. **è¾“å‡ºæ ¼å¼**ï¼š`ç±»åˆ« æ¦‚ç‡`ï¼Œæ¦‚ç‡ä¿ç•™ 4 ä½å°æ•°

### æ ·ä¾‹
```
è¾“å…¥:
10 1000 0.01 0.1 0.0001
25 8 5 0
30 15 15 1
...
3
32 18 12
48 33 22
62 48 10

è¾“å‡º:
1 0.7539
1 0.9966
0 0.0004
```

### æ€è·¯
1. åˆå§‹åŒ– w=0, b=0
2. æ¯æ¬¡è¿­ä»£è®¡ç®—æ¢¯åº¦ï¼ˆå« L2 æ­£åˆ™é¡¹ï¼‰
3. æ›´æ–°å‚æ•°
4. æ”¶æ•›æ£€æŸ¥

### å¤æ‚åº¦
- æ—¶é—´: O(max_iter Ã— n Ã— d)
- ç©ºé—´: O(n Ã— d)

### æˆ‘çš„ä»£ç 
```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## é¢˜ç›®4: åŸºäºé€»è¾‘å›å½’çš„æ„å›¾åˆ†ç±»å™¨ï¼ˆP3872ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: One-hot ç¼–ç  + é€»è¾‘å›å½’ + SGD
- **æº**: [core46#ç¬¬3é¢˜-p3872](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- è¾“å…¥ï¼šç”±å¤§å†™å­—æ¯ ABCDEFG ç»„æˆçš„å­—ç¬¦ä¸²
- One-hot ç¼–ç ï¼šé•¿åº¦ 7ï¼Œå­—æ¯å­˜åœ¨åˆ™ä¸º 1
- é€»è¾‘å›å½’ï¼š$p = \sigma(w \cdot x + b)$
- è®­ç»ƒï¼šå­¦ä¹ ç‡ 0.1ï¼Œè½®æ•° 20ï¼Œbatch=1ï¼ˆSGDï¼‰

### å…³é”®è§„åˆ™
1. å­—æ¯åªè®°å½•æ˜¯å¦å­˜åœ¨ï¼Œ**ä¸è®¡æ¬¡æ•°**
2. é˜ˆå€¼ 0.5ï¼šp > 0.5 è¾“å‡º 1ï¼Œå¦åˆ™ 0
3. åˆå§‹ wã€b å…¨ä¸º 0

### æ ·ä¾‹
```
è¾“å…¥:
10 2
CBG 0
AFE 0
FGD 1
...
DBA
DAD

è¾“å‡º:
0
0
```

### æ€è·¯
1. ç¼–ç ï¼šå°†å­—ç¬¦ä¸²è½¬ä¸º 7 ç»´ one-hot
2. SGD è®­ç»ƒ 20 è½®
3. é¢„æµ‹å¹¶è¾“å‡º

### å¤æ‚åº¦
- æ—¶é—´: O(20 Ã— N Ã— 7) = O(N)
- ç©ºé—´: O(7)

### æˆ‘çš„ä»£ç 
```python
import sys
it = iter(sys.stdin.read().strip().split())
N,M=int(next(it)),int(next(it));D=[[next(it), int(next(it))] for _ in range(N)];S= [next(it) for _ in range(M)]
Y =[0]*N; X=[[0]*7 for _ in range(N)];Z=[[0]*7 for _ in range(M)]
def encode(s,TT):
    for sj in s: TT[ord(sj)-ord('A')]=1
for i, (s, y) in enumerate(D): Y[i] = y; encode(s, X[i])
_=[encode(s, z) for s,z in zip(S,Z)]
import numpy as np
X, Y, Z = list(map(np.array, [X,Y,Z]))
sigmoid = lambda z: 1/(1+np.exp(-np.clip(z,-50,50)))
W=np.zeros(7);b=0;lr=0.1
for e in range(20):
    for x,y in zip(X,Y):
        loss = sigmoid(W @ x + b) - y
        W -= lr * (loss) * x; b -= lr * (loss)
out =[int(sigmoid(W @ z + b) > 0.5) for z in Z]
print(*out, sep='\n')
```

---

## é¢˜ç›®5: æ•°æ®ä¸­å¿ƒæ°´æ¸©è°ƒèŠ‚æ¡£ä½å†³ç­–ï¼ˆP3719ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: å›å½’/å†³ç­–é—®é¢˜ï¼ˆé DPï¼‰
- **æº**: [core46#ç¬¬3é¢˜-p3719](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
TODO

### æ€è·¯
TODO

### å¤æ‚åº¦
TODO

```
10 3 5 2 3 10
-41.53 13.54 -51.57 -0.90 -17.71 31.90 -24.43 88.34 74.12 -47.06 
23.09 -22.95 -28.74 21.31 -19.01 -31.90 -14.89 -78.25 -72.45 -15.58 
7.52 -94.70 -16.86 -13.02 -46.77 11.37 -38.38 -85.37 -52.83 -63.79 
-28.32 21.18 99.19 49.71 -34.97 -11.34 -37.89 47.18 9.20 -81.93 
-41.51 -32.35 -92.51 -32.15 -13.43 -23.66 20.44 -75.82 44.22 8.53 
47.37 9.90 -63.44 -59.53 -83.86 75.39 -25.82 65.88 -34.71 -19.44 
76.63 40.99 -59.21 68.59 -67.38 -42.16 -50.53 -97.76 -6.43 19.52 
-87.62 10.46 45.26 -25.53 -33.45 19.97 -10.22 18.23 -3.22 77.33 
-60.09 -59.57 1.14 38.16 11.20 72.59 67.72 1.23 20.46 10.16 
-91.39 89.93 -39.10 -46.39 -75.25 -60.65 -35.24 -42.82 -35.79 72.72 
87.64 -95.78 62.83 76.01 -64.21 -4.10 -57.60 -28.13 19.70 -53.57 
-44.83 -60.03 -21.49 8.07 92.63 -49.45 -92.33 82.55 66.42 88.41 
-14.72 -25.79 -61.13 -61.84 12.56 -9.34 0.41 12.45 -97.73 -28.63 
26.63 -53.13 -69.54 -36.86 -60.05 69.01 -40.20 14.10 4.46 -96.35 
-65.04 49.95 76.52 -59.36 -5.32 6.41 -19.52 70.04 -60.98 -69.90 
-12.80 -61.39 90.16 -24.62 30.56 -26.98 8.65 -80.11 -36.63 55.30 
-36.89 -77.94 -35.68 68.50 -82.66 -90.73 -58.08 21.55 -41.73 -46.05 
-84.69 -79.97 86.37 -41.67 -28.87 -69.67 6.72 73.15 -11.07 -38.84 
-89.53 -46.11 6.77 86.64 12.59 -81.60 -48.59 -99.16 73.70 -56.71 
-16.67 -86.89 -89.41 90.62 -57.18 78.30 -81.28 -76.13 40.99 43.49 


0
0
0
0
0
2
0
0
0
1
```
### æˆ‘çš„ä»£ç 
```python
# X, y, X1, n, k = read()
# y = y.astype(int)
# N = len(X)
# eps = 1e-8

# # æ ‡å‡†åŒ–ï¼ˆåªç”¨è®­ç»ƒé›†ç»Ÿè®¡ï¼‰
# mu = X.mean(0) if N else np.zeros(n)
# d = X - mu
# sig = np.sqrt((d*d).sum(0) / (N-1 if N > 1 else 1)) if N else np.ones(n)
# sig = np.where(sig < eps, 1.0, sig)
# X  = (X  - mu) / (sig + eps)
# X1 = (X1 - mu) / (sig + eps)

# def softmax(z):
#     z = z - z.max(1, keepdims=True)
#     e = np.exp(z)
#     return e / (e.sum(1, keepdims=True) + eps)

# W = np.zeros((n, k))
# b = np.zeros(k)
# lr, reg = 0.1, 1e-4

# for e in range(600 if N else 0):
#     P = softmax(X @ W + b)
#     P[np.arange(N), y] -= 1
#     dZ = P / N
#     W -= lr * (X.T @ dZ + reg * W)
#     b -= lr * dZ.sum(0)
#     if (e + 1) % 150 == 0: lr *= 0.9
# # print(W.round(),b)
# Y1 = softmax(X1 @ W + b).argmax(1)
# print(*Y1, sep="\n")
```

---

## é¢˜ç›®6: äº‘å­˜å‚¨è®¾å¤‡æ•…éšœé¢„æµ‹ï¼ˆP3552ï¼‰

- **éš¾åº¦**: ä¸­ç­‰
- **æ ¸å¿ƒ**: æ•°æ®æ¸…æ´— + é€»è¾‘å›å½’
- **æº**: [core46#ç¬¬3é¢˜-p3552](../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md)

### é¢˜ç›®æè¿°
- 5 ç»´ç‰¹å¾ï¼šå†™å…¥æ¬¡æ•°ã€è¯»å–æ¬¡æ•°ã€å†™å»¶è¿Ÿã€è¯»å»¶è¿Ÿã€ä½¿ç”¨å¹´é™
- æ•°æ®æ¸…æ´—ï¼š
  - ç¼ºå¤±å€¼ï¼ˆNaNï¼‰â†’ ç”¨å‡å€¼å¡«å……
  - å¼‚å¸¸å€¼ â†’ ç”¨ä¸­ä½æ•°æ›¿æ¢
- é€»è¾‘å›å½’ï¼š100 æ¬¡è¿­ä»£ï¼Œå­¦ä¹ ç‡ 0.01

### å¼‚å¸¸å€¼è§„åˆ™
| ç‰¹å¾ | å¼‚å¸¸æ¡ä»¶ |
|------|---------|
| å†™å…¥/è¯»å–æ¬¡æ•° | < 0 |
| å»¶è¿Ÿ | < 0 æˆ– > 1000 |
| å¹´é™ | < 0 æˆ– > 20 |

### æ ·ä¾‹
```
è¾“å…¥:
5
dev1,NaN,-50,NaN,-2.0,25,0
dev2,180,90,18.0,9.0,4,0
...
2
dev_predict1,80,40,NaN,2.0,2,0
dev_predict2,210,105,18.0,9.8,4,0

è¾“å‡º:
0
0
```

### æ€è·¯
1. æŒ‰åˆ—ç»Ÿè®¡æœ‰æ•ˆå€¼çš„å‡å€¼å’Œä¸­ä½æ•°
2. ç¼ºå¤±å€¼å¡«å‡å€¼ï¼Œå¼‚å¸¸å€¼å¡«ä¸­ä½æ•°
3. é€»è¾‘å›å½’è®­ç»ƒ 100 æ¬¡
4. é¢„æµ‹

### å¤æ‚åº¦
- æ—¶é—´: O(N log N)ï¼ˆæ’åºæ±‚ä¸­ä½æ•°ï¼‰+ O(100 Ã— N Ã— 5)
- ç©ºé—´: O(N)

### æˆ‘çš„ä»£ç 
```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## ğŸ“Œ æ˜“é”™ç‚¹æ€»ç»“

1. **æ­£è§„æ–¹ç¨‹çš„é«˜æ–¯æ¶ˆå…ƒ**ï¼šæ³¨æ„ä¸»å…ƒå½’ä¸€åŒ–å’Œæ¶ˆå…ƒé¡ºåº
2. **é€»è¾‘å›å½’çš„ sigmoid æº¢å‡º**ï¼šä½¿ç”¨æ•°å€¼ç¨³å®šå†™æ³•ï¼ˆåˆ† z â‰¥ 0 å’Œ z < 0ï¼‰
3. **L2 æ­£åˆ™çš„æ¢¯åº¦**ï¼š$\frac{\partial}{\partial w_j}(\frac{\lambda}{2n}\|w\|^2) = \frac{\lambda}{n}w_j$
4. **æ•°æ®æ¸…æ´—é¡ºåº**ï¼šå…ˆå¤„ç†ç¼ºå¤±å€¼ï¼Œå†å¤„ç†å¼‚å¸¸å€¼
5. **MLP åå‘ä¼ æ’­**ï¼šæ³¨æ„çŸ©é˜µä¹˜æ³•çš„è½¬ç½®æ–¹å‘
6. **One-hot ç¼–ç **ï¼šåªè®°å½•å­˜åœ¨æ€§ï¼Œä¸è®¡æ¬¡æ•°

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- æºæ–‡ä»¶ï¼š`../AI_ç¼–ç¨‹é¢˜_Pythonè§£ç­”_æ ¸å¿ƒ46é¢˜.md`
- ç´¢å¼•ï¼š`../ai_core46_index.md`

---

## ğŸ“ ä»£ç ç­”æ¡ˆ

### é¢˜ç›®1: P4532 çº¿æ€§å›å½’é¢„æµ‹æ‰‹æœºå”®ä»·
```python
import sys
import math

def linear_regression_predict(K, train_data, N, test_data):
    # X æ˜¯ KÃ—4 çŸ©é˜µï¼ŒY æ˜¯ KÃ—1 å‘é‡
    X = []
    Y = []
    idx = 0
    for _ in range(K):
        x1, x2, x3, y = train_data[idx:idx+4]
        idx += 4
        X.append([1.0, x1, x2, x3])
        Y.append(y)

    # è®¡ç®— X^T * X å’Œ X^T * Y
    XT_X = [[0.0]*4 for _ in range(4)]
    XT_Y = [0.0]*4
    for i in range(K):
        for a in range(4):
            XT_Y[a] += X[i][a] * Y[i]
            for b in range(4):
                XT_X[a][b] += X[i][a] * X[i][b]

    # é«˜æ–¯æ¶ˆå…ƒæ³•æ±‚è§£ (X^T X)W = X^T Y
    # æ„é€ å¢å¹¿çŸ©é˜µ
    A = [XT_X[i] + [XT_Y[i]] for i in range(4)]

    # æ¶ˆå…ƒ
    for i in range(4):
        pivot = A[i][i]
        for j in range(i, 5):
            A[i][j] /= pivot
        for k in range(4):
            if k != i:
                factor = A[k][i]
                for j in range(i, 5):
                    A[k][j] -= factor * A[i][j]

    W = [A[i][4] for i in range(4)]

    # é¢„æµ‹
    res = []
    idx = 0
    for _ in range(N):
        x1, x2, x3 = test_data[idx:idx+3]
        idx += 3
        y_pred = W[0] + W[1]*x1 + W[2]*x2 + W[3]*x3
        res.append(str(int(round(y_pred))))
    return res

def main():
    data = sys.stdin.read().strip().split()
    pos = 0
    K = int(data[pos]); pos += 1
    train_data = list(map(int, data[pos:pos+4*K]))
    pos += 4*K
    N = int(data[pos]); pos += 1
    test_data = list(map(int, data[pos:pos+3*N]))

    ans = linear_regression_predict(K, train_data, N, test_data)
    print(" ".join(ans))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®2: P4447 åŒ»ç–—è¯Šæ–­æ¨¡å‹çš„è®­ç»ƒä¸æ›´æ–°
```python
import sys
import ast

def parse_line(line: str):
    return list(ast.literal_eval("[" + line.strip() + "]"))

def solve_once(L, D, K, eta, y_true, seq_flat, Wmlp_flat, Wcls_flat):
    X = [seq_flat[i*D:(i+1)*D] for i in range(L)]
    Wmlp = [Wmlp_flat[i*D:(i+1)*D] for i in range(D)]
    Wcls = [Wcls_flat[i*K:(i+1)*K] for i in range(D)]

    # å‰å‘
    H_sum = [0.0]*D
    P_avg = [0.0]*K
    for t in range(L):
        x = X[t]
        h = [sum(x[d] * Wmlp[d][j] for d in range(D)) for j in range(D)]
        for j in range(D):
            H_sum[j] += h[j]
        p = [sum(h[j] * Wcls[j][k] for j in range(D)) for k in range(K)]
        for k in range(K):
            P_avg[k] += p[k]
    P_avg = [v / L for v in P_avg]

    # æŸå¤±
    loss = sum((P_avg[k] - y_true[k])**2 for k in range(K)) / K

    # åå‘
    g = [(2.0 / K) * (P_avg[k] - y_true[k]) for k in range(K)]
    H_bar = [v / L for v in H_sum]
    dWcls = [[H_bar[j] * g[k] for k in range(K)] for j in range(D)]
    v = [sum(Wcls[j][k] * g[k] for k in range(K)) for j in range(D)]
    X_sum = [sum(X[t][d] for t in range(L)) for d in range(D)]
    X_bar = [v_ / L for v_ in X_sum]
    dWmlp = [[X_bar[i] * v[j] for j in range(D)] for i in range(D)]

    # SGD æ›´æ–°
    for i in range(D):
        for j in range(D):
            Wmlp[i][j] -= eta * dWmlp[i][j]
    for j in range(D):
        for k in range(K):
            Wcls[j][k] -= eta * dWcls[j][k]

    Wmlp_new = [Wmlp[i][j] for i in range(D) for j in range(D)]
    Wcls_new = [Wcls[j][k] for j in range(D) for k in range(K)]
    return P_avg, loss, Wmlp_new, Wcls_new

def fmt_line(arr):
    return ",".join(f"{x:.2f}" for x in arr)

def main():
    lines = sys.stdin.read().strip().splitlines()
    L, D, K, eta = parse_line(lines[0])
    L, D, K = int(L), int(D), int(K)
    y_true = parse_line(lines[1])
    seq = parse_line(lines[2])
    Wmlp = parse_line(lines[3])
    Wcls = parse_line(lines[4])

    P_avg, loss, Wmlp_new, Wcls_new = solve_once(L, D, K, eta, y_true, seq, Wmlp, Wcls)
    print(fmt_line(P_avg))
    print(f"{loss:.2f}")
    print(fmt_line(Wmlp_new))
    print(fmt_line(Wcls_new))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®3: P4344 å•†å“è´­ä¹°é¢„æµ‹
```python
import sys
import math

def sigmoid(z):
    if z >= 0:
        return 1.0 / (1.0 + math.exp(-z))
    else:
        ez = math.exp(z)
        return ez / (1.0 + ez)

def compute_loss_and_grad(X, y, w, b, lam):
    n, d = len(X), len(w)
    eps = 1e-15
    loss, grad_w, grad_b = 0.0, [0.0]*d, 0.0

    for i in range(n):
        z = b + sum(w[j] * X[i][j] for j in range(d))
        p = sigmoid(z)
        loss += -(y[i] * math.log(max(p, eps)) + (1-y[i]) * math.log(max(1-p, eps)))
        diff = p - y[i]
        for j in range(d):
            grad_w[j] += diff * X[i][j]
        grad_b += diff

    loss /= n
    for j in range(d):
        grad_w[j] = grad_w[j] / n + (lam / n) * w[j]
    grad_b /= n
    l2 = sum(w[j]**2 for j in range(d))
    loss += (lam / (2*n)) * l2

    return loss, grad_w, grad_b

def train_logreg(X, y, max_iter, alpha, lam, tol):
    d = len(X[0])
    w, b = [0.0]*d, 0.0
    loss, _, _ = compute_loss_and_grad(X, y, w, b, lam)

    for _ in range(max_iter):
        _, grad_w, grad_b = compute_loss_and_grad(X, y, w, b, lam)
        for j in range(d):
            w[j] -= alpha * grad_w[j]
        b -= alpha * grad_b
        new_loss, _, _ = compute_loss_and_grad(X, y, w, b, lam)
        if abs(loss - new_loss) < tol:
            break
        loss = new_loss
    return w, b

def predict_one(x, w, b):
    z = b + sum(w[j] * x[j] for j in range(len(w)))
    p = sigmoid(z)
    return (1 if p >= 0.5 else 0), p

def main():
    data = sys.stdin.read().strip().split()
    it = iter(data)
    n, max_iter = int(next(it)), int(next(it))
    alpha, lam, tol = float(next(it)), float(next(it)), float(next(it))

    X, y = [], []
    for _ in range(n):
        X.append([float(next(it)), float(next(it)), float(next(it))])
        y.append(int(next(it)))

    m = int(next(it))
    test = [[float(next(it)), float(next(it)), float(next(it))] for _ in range(m)]

    w, b = train_logreg(X, y, max_iter, alpha, lam, tol)
    for x in test:
        lab, p = predict_one(x, w, b)
        print(f"{lab} {p:.4f}")

if __name__ == "__main__":
    main()
```

### é¢˜ç›®4: P3872 åŸºäºé€»è¾‘å›å½’çš„æ„å›¾åˆ†ç±»å™¨
```python
import sys, math

def encode(seq):
    x = [0.0] * 7
    for ch in set(seq.strip()):
        idx = ord(ch) - ord('A')
        if 0 <= idx < 7:
            x[idx] = 1.0
    return x

def train(X, y, lr=0.1, epochs=20):
    w, b = [0.0]*7, 0.0
    for _ in range(epochs):
        for xi, yi in zip(X, y):
            z = sum(w[j] * xi[j] for j in range(7)) + b
            p = 1.0 / (1.0 + math.exp(-z))
            dz = p - yi
            for j in range(7):
                w[j] -= lr * dz * xi[j]
            b -= lr * dz
    return w, b

def predict(w, b, xi):
    z = sum(w[j] * xi[j] for j in range(7)) + b
    p = 1.0 / (1.0 + math.exp(-z))
    return 1 if p > 0.5 else 0

def main():
    data = sys.stdin.read().strip().split()
    it = iter(data)
    N, M = int(next(it)), int(next(it))

    X, y = [], []
    for _ in range(N):
        X.append(encode(next(it)))
        y.append(int(next(it)))

    w, b = train(X, y)

    for _ in range(M):
        print(predict(w, b, encode(next(it))))

if __name__ == "__main__":
    main()
```

### é¢˜ç›®5: P3552 äº‘å­˜å‚¨è®¾å¤‡æ•…éšœé¢„æµ‹
```python
import sys, math

def parse_line(line):
    parts = [p.strip() for p in line.strip().split(',')]
    if len(parts) < 7: return None
    feats = parts[1:6]
    y = int(float(parts[-1]))
    def to_num(s):
        return None if s == "NaN" else float(s)
    return [to_num(v) for v in feats], y

def valid(col, v):
    if v is None: return False
    if col in (0,1): return v >= 0
    if col in (2,3): return 0 <= v <= 1000
    if col == 4: return 0 <= v <= 20
    return True

def median(vals):
    if not vals: return 0.0
    vals = sorted(vals)
    n = len(vals)
    return vals[n//2] if n % 2 else 0.5*(vals[n//2-1]+vals[n//2])

def sigmoid(z):
    z = max(-30, min(30, z))
    return 1.0 / (1.0 + math.exp(-z))

def main():
    lines = sys.stdin.read().strip().splitlines()
    n = int(lines[0])
    train = [parse_line(lines[i+1]) for i in range(n)]
    m = int(lines[n+1])
    test = [parse_line(lines[n+2+i]) for i in range(m)]

    # ç»Ÿè®¡å‡å€¼å’Œä¸­ä½æ•°
    means, meds = [0.0]*5, [0.0]*5
    for j in range(5):
        valid_vals = [t[0][j] for t in train if t[0][j] is not None and valid(j, t[0][j])]
        means[j] = sum(valid_vals)/len(valid_vals) if valid_vals else 0.0
        meds[j] = median(valid_vals)

    # æ¸…æ´—
    def clean(row):
        x = []
        for j in range(5):
            v = row[0][j]
            if v is None: v = means[j]
            elif not valid(j, v): v = meds[j]
            x.append(v)
        return x

    X_train = [clean(t) for t in train]
    y_train = [t[1] for t in train]
    X_test = [clean(t) for t in test]

    # è®­ç»ƒé€»è¾‘å›å½’
    w, b = [0.0]*5, 0.0
    for _ in range(100):
        grad_w, grad_b = [0.0]*5, 0.0
        for i in range(n):
            z = b + sum(w[j]*X_train[i][j] for j in range(5))
            p = sigmoid(z)
            diff = p - y_train[i]
            for j in range(5):
                grad_w[j] += diff * X_train[i][j]
            grad_b += diff
        for j in range(5):
            w[j] -= 0.01 * grad_w[j] / n
        b -= 0.01 * grad_b / n

    # é¢„æµ‹
    for x in X_test:
        z = b + sum(w[j]*x[j] for j in range(5))
        p = sigmoid(z)
        print(1 if p >= 0.5 else 0)

if __name__ == "__main__":
    main()
```

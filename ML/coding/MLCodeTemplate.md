# MLE Interview Prepï¼ˆDeep-MLï¼‰[0/41 å®Œæˆ]

> ğŸ“Š **è¿›åº¦**: 0/41 å®Œæˆ (0%)  
> ğŸ”„ **æœ€åæ›´æ–°**: 2026-01-11  
> ğŸ“ **æ¥æº**: [Deep-ML MLE Interview Prep](https://www.deep-ml.com/)  
> ğŸ’¡ **è¯´æ˜**: 41 essential problems for MLE interviews

---

## ğŸ“‹ é¢˜ç›®æ€»è§ˆ

| åˆ†ç±» | é¢˜ç›®æ•° | å®Œæˆ | è¿›åº¦ |
|------|--------|------|------|
| 1. Core ML Algorithms | 5 | 0 | 0% |
| 2. Loss & Regularization | 4 | 0 | 0% |
| 3. Model Evaluation | 6 | 0 | 0% |
| 4. Neural Networks | 6 | 0 | 0% |
| 5. Optimizers | 3 | 0 | 0% |
| 6. CNNs | 2 | 0 | 0% |
| 7. Sequences | 2 | 0 | 0% |
| 8. Transformers | 3 | 0 | 0% |
| 9. Production & MLOps | 10 | 0 | 0% |

---

## ğŸ”§ é€šç”¨æ¨¡æ¿é€ŸæŸ¥

> ğŸ’¡ æŒ‰ç±»åˆ«ç»„ç»‡ï¼Œè·³è½¬ï¼š[1.å›å½’/åˆ†ç±»](#t1-å›å½’åˆ†ç±») | [2.èšç±»/é™ç»´](#t2-èšç±»é™ç»´) | [3.æŸå¤±å‡½æ•°](#t3-æŸå¤±å‡½æ•°) | [4.è¯„ä¼°æŒ‡æ ‡](#t4-è¯„ä¼°æŒ‡æ ‡) | [5.æ¿€æ´»å‡½æ•°](#t5-æ¿€æ´»å‡½æ•°) | [6.ä¼˜åŒ–å™¨](#t6-ä¼˜åŒ–å™¨) | [7.NNå±‚](#t7-ç¥ç»ç½‘ç»œå±‚) | [8.CNN](#t8-cnn) | [9.Transformer](#t9-transformer)

---

### T1. å›å½’/åˆ†ç±»

#### Linear Regression (æ¢¯åº¦ä¸‹é™ + æ­£è§„æ–¹ç¨‹)
```python
def linear_regression_gd(X, y, lr=0.01, epochs=1000):
    n, d = X.shape
    X = np.hstack([np.ones((n, 1)), X])  # æ·»åŠ  bias
    W = np.zeros(d + 1)
    for _ in range(epochs):
        pred = X @ W
        grad = 2 / n * X.T @ (pred - y)
        W -= lr * grad
    return W

# æ­£è§„æ–¹ç¨‹è§£
W = np.linalg.solve(X.T @ X, X.T @ y)
```

#### Logistic Regression
```python
def sigmoid(z):
    return 1 / (1 + np.exp(np.clip(-z, -500, 500)))
def sigmoid_stable(z):
    out = np.empty_like(z); 
    pos = z >= 0
    out[pos]  = 1.0/(1.0 + np.exp(-z[pos])) #z[pos] >=0 --> e^{-z} ä¸ä¼šçˆ†ç‚¸
    ez = np.exp(z[~pos])           # z<0 åŒºé—´æ›´ç¨³å®š
    out[~pos] = ez/(1.0 + ez)  # è®¡ç®— Ïƒ(z) ç”¨ç¨³å®šå†™æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥ 1/(1+exp(-z)) å› ä¸ºz <0å¯èƒ½ä¼šçˆ†ç‚¸
    return out

def logistic_regression(X, y, lr=0.01, epochs=100, lam=0.0):
    n, d = X.shape
    W = np.zeros(d); b = 0
    for _ in range(epochs):
        z = X @ W + b; p = sigmoid(z); dy = p - y; dg = dy / n
        dW = X.T @ dg + lam * W; db = dg
        W -= lr * dW; b -= lr * db
    return W, b
```

#### Ridge/Lasso (æ­£åˆ™åŒ–æ¢¯åº¦)
```python
# Ridge (L2)
loss = 1/2 * (dy**2) /n +  lam/2 * W ** 2
dW = X.T @ dg + lam * W

# Lasso (L1)  
loss = 1/2 * (dy**2) /n +  lam * np.abs(W)
dW = X.T @ dg + lam * np.sign(W)
```

#### Decision Tree (ID3/C4.5)
```python
def entropy(y):
    _, counts = np.unique(y, return_counts=True)
    p = counts / len(y)
    return -np.sum(p * np.log2(p + 1e-10))

def info_gain(X, y, feature):
    H = entropy(y); x = X[:, feature]; 
    H_cond_v = [(x == v).mean() * entropy(y[x == v]) for v in np.unique(x)]
    return H - sum(H_cond_v)

def majority(y): 
    c = Counter(y)
    return c.most_common(1)[0][0] #ä¸è€ƒè™‘å¹³ç¥¨ã€è¯„æµ‹æ•°æ®ä¹Ÿä¸å‡º tie
    # å¹³ç¥¨æ—¶é€‰æœ€æ—©å‡ºç°çš„:
    # return max(c, key=lambda k: (c[k], -y.index(k)))

def tree(X, y, feature):
    b = max(feature, key=lambda k: gain(X,y,feature))
```

---

### T2. èšç±»/é™ç»´

#### KMeans
```python
def kmeans(X, K, iters=100):
    N, D = X.shape
    C = X[np.random.choice(N, K, replace=False)]   # åˆå§‹åŒ–ä¸­å¿ƒ
    for _ in range(iters):
        # E-stepï¼šåˆ†é…ç°‡ï¼ˆå¹¿æ’­ç®—è·ç¦»ï¼‰
        dist = ((X[:, None, :] - C[None, :, :])**2).sum(axis=2)
        labels = dist.argmin(axis=1)
        # M-stepï¼šæ›´æ–°ä¸­å¿ƒ
        C = np.array([X[labels == k].mean(axis=0) for k in range(K)])
    return C, labels
```

#### PCA
```python
# æ–¹æ³•1: SVD
X = (data - data.mean(0)) / data.std(0)  # æ ‡å‡†åŒ–
_, _, Vt = np.linalg.svd(X, full_matrices=False)
P = Vt[:k].T  # (d, k) ä¸»æˆåˆ†æ–¹å‘

# æ–¹æ³•2: åæ–¹å·®çŸ©é˜µç‰¹å¾åˆ†è§£
C = np.cov(X, rowvar=False)
w, V = np.linalg.eigh(C)
I = np.argsort(w)[::-1]
P = V[:, I[:k]]
```

---

### T3. æŸå¤±å‡½æ•°

#### Cross-Entropy Loss (Binary + Multi-class)
```python
def binary_cross_entropy(y_true, y_pred):
    eps = 1e-15; y_pred = np.clip(y_pred, eps, 1 - eps)
    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean(0)

def bce_loss_from_logits(z, y):
    """æ•°å€¼ç¨³å®šç‰ˆ BCE (ä» logits ç›´æ¥è®¡ç®—)"""
    absz = np.abs(z)
    # softplus(z) = log(1 + exp(-|z|)) + max(z, 0) é˜²æ­¢æº¢å‡º
    softplus = np.log1p(np.exp(-absz)) + np.maximum(z, 0) 
    BCEloss_total = softplus - y * z
    return BCEloss_total.mean()    

def logistic_reg_loss(X, W, y, lam=0.0):
    z = X @ W; p = sigmoid_stable(z)
    loss = bce_loss_from_logits(z, y) + 0.5*lam*np.sum(W*W)
    return loss

def multi_class_ce(y_true, y_pred):
    """y_true: one-hot, y_pred: softmax output"""
    eps = 1e-15
    loss_total = -np.sum(y_true * np.log(y_pred + eps), axis=1)
    return loss_total.mean(0)
```

---

### T4. è¯„ä¼°æŒ‡æ ‡

#### K-fold Cross Validation
```python
def k_fold_cross_validation(X, y, k=5, shuffle=True):
    n = len(X); idx = np.arange(n)
    if shuffle: np.random.shuffle(idx)
    folds = np.array_split(idx, k)  # è‡ªåŠ¨æŠŠ n åˆ†æˆ k æ®µï¼ˆå°½é‡å‡åŒ€ï¼‰
    return [(np.concatenate([*folds[:i], *folds[i+1:]]), folds[i]) for i in range(k)]
```

#### Confusion Matrix + Precision/Recall/F1/AUC
```python
def 
    cm = np.bincount(true*K + pred, minlength=K*K).reshape(K, K)
    TP = np.diag(cm)
    FP = cm.sum(0) - TP
    FN = cm.sum(1) - TP
    TN = cm.sum() - TP - FP - FN


def confusion_matrix(y_true, y_pred):
    TP = ((y_pred == 1) & (y_true == 1)).sum()
    TN = ((y_pred == 0) & (y_true == 0)).sum()
    FP = ((y_pred == 1) & (y_true == 0)).sum()
    FN = ((y_pred == 0) & (y_true == 1)).sum()
    return TP, TN, FP, FN

def confusion_matrix(data): # binary only
    counts = Counter(tuple(pair) for pair in data) 
    TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]
    return [[TP, FN], [FP, TN]]

def multi_cat_confusion(Y, Y0):
    TP = np.bincount(Y[Y == Y0], minlength=K)
    FP = np.bincount(Y[Y != Y0], minlength=K) # FP[c]+=1 when é¢„æµ‹æˆY[i] = cï¼Œä½†çœŸå®Y0[i]!= c
    FN = np.bincount(Y0[Y != Y0], minlength=K) #FN[c]+=1 when çœŸå®Y0[i]== c, ä½†é¢„æµ‹æˆåˆ«çš„Y[i]!=c

    P = np.divide(TP, TP + FP, out=np.zeros(K), where=(TP + FP) != 0)
    R = np.divide(TP, TP + FN, out=np.zeros(K), where=(TP + FN) != 0)
    F1 = np.divide(2 * P * R, P + R, out=np.zeros(K), where=(P + R) != 0)

def auc(y, p):
    P, N = y.sum(), len(y) - y.sum()
    if P == 0 or N == 0: return 0.0
    y = y[np.argsort(-p)]
    tpr = np.r_[0, np.cumsum(y) / P] # np.r_ æŒ‰è¡Œæ‹¼æ¥
    fpr = np.r_[0, np.cumsum(1 - y) / N]
    return np.trapz(tpr, fpr)

def precision(y_true, y_pred):
    TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
    return TP / (TP + FP + 1e-10)

def recall(y_true, y_pred):
    TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
    return TP / (TP + FN + 1e-10)

def f1_score(y_true, y_pred):
    p, r = precision(y_true, y_pred), recall(y_true, y_pred)
    return 2 * p * r / (p + r + 1e-10)

def auc_roc(y_true, y_scores):
    """ç®€æ˜“å®ç°ï¼šæ¢¯å½¢æ³•åˆ™"""
    thresholds = np.sort(np.unique(y_scores))[::-1]
    tpr, fpr = [0], [0]
    for t in thresholds:
        y_pred = (y_scores >= t).astype(int)
        TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
        tpr.append(TP / (TP + FN + 1e-10))
        fpr.append(FP / (FP + TN + 1e-10))
    return np.trapz(tpr, fpr)
```

---

### T5. æ¿€æ´»å‡½æ•°

#### Softmax
```python
def softmax(x, axis=-1):
    x = x - x.max(axis=axis, keepdims=True)  # æ•°å€¼ç¨³å®š
    e = np.exp(x)
    return e / e.sum(axis=axis, keepdims=True)
```

#### ReLU
```python
def relu(x):
    return np.maximum(0, x)

def relu_grad(x):
    return (x > 0).astype(float)
```

---

### T6. ä¼˜åŒ–å™¨

### sigmoid activation
```python
z = X @ W + b; p = sigmoid(z)
dz = 2/n * (p - y) * p * (1 - p)   # MSE çš„ dz
dW = X.T @ dz
db = dz.sum()
```

#### Gradient Descent (BGD/SGD/Mini-batch)
```python
def reg(W, X, Y, lr):
    n = len(X)
    p = X @ W; dy = p - Y; dg = 2/n * dy
    dW = X.T @ dg; W -= lr * dW
    return W

def gradient_descent(X, y, lr, epoch, Bs, method):
    n = len(X)
    W = np.zeros(X.shape[1])
    for _ in range(epoch):
        if method == 'batch':
            W = reg(W, X, y, lr)
        elif method == 'stochastic':
            for i in range(n): 
                W = reg(W, X[i:i+1], y[i:i+1], lr)
        elif method == 'mini_batch':
            for i in range(0, n, Bs):
                W = reg(W, X[i:i+Bs], y[i:i+Bs], lr)
    return W
```

#### Adam Optimizer
```python
def adam(params, grads, m, v, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
    for i in range(len(params)):
        m[i] = beta1 * m[i] + (1 - beta1) * grads[i]
        v[i] = beta2 * v[i] + (1 - beta2) * grads[i]**2
        m_hat = m[i] / (1 - beta1**t)  # bias correction
        v_hat = v[i] / (1 - beta2**t)
        params[i] -= lr * m_hat / (np.sqrt(v_hat) + eps)
    return params, m, v
```

---

### T7. ç¥ç»ç½‘ç»œå±‚

#### Batch Normalization (BCHW)
BN çš„æ ¸å¿ƒæ˜¯ï¼šå¯¹æ¯ä¸ªâ€œé€šé“/ç‰¹å¾ç»´â€å•ç‹¬ç®—ç»Ÿè®¡é‡ï¼Œç»Ÿè®¡é‡æ¥è‡ªbatch ä»¥åŠå…¶å®ƒéé€šé“ç»´ã€‚
```python
def batch_norm(x, gamma, beta, eps=1e-5):
    # x: (N, C, H, W) for BCHW
    mean = x.mean(axis=(0, 2, 3), keepdims=True)
    var = x.var(axis=(0, 2, 3), keepdims=True)
    x_norm = (x - mean) / np.sqrt(var + eps)
    return gamma * x_norm + beta
```
#### Layer Normalization (BLD)
def layer_norm_ch(x, gamma, beta, eps=1e-5):
    # x: (N, C, H, W)
    mean = x.mean(axis=1, keepdims=True)          # (N, 1, H, W)
    var  = x.var(axis=1, keepdims=True)           # (N, 1, H, W)
    x_norm = (x - mean) / np.sqrt(var + eps)
    return gamma * x_norm + beta  


#### Dropout (Inverted)
```python
def dropout(x, p=0.5, training=True):
    if not training or p == 0:
        return x
    mask = np.random.binomial(1, 1-p, x.shape) / (1-p)
    return x * mask
```

---

### T8. CNN

#### Conv2D
```python
def conv2d(X, K, stride=1, padding=0):
    # X: (C, H, W), K: (OC, IC, KH, KW)
    X = np.pad(X, ((0,0), (padding, padding), (padding, padding)))
    _, H, W = X.shape
    OC, IC, KH, KW = K.shape
    Ho = (H - KH) // stride + 1
    Wo = (W - KW) // stride + 1
    out = np.zeros((OC, Ho, Wo))
    for oc in range(OC):
        for h in range(Ho):
            for w in range(Wo):
                out[oc, h, w] = np.sum(
                    X[:, h*stride:h*stride+KH, w*stride:w*stride+KW] * K[oc])
    return out
```

#### Global Average Pooling
```python
def global_avg_pool(x):
    # x: (N, C, H, W) â†’ (N, C)
    return x.mean(axis=(2, 3))
```

---

### T9. Transformer

#### Self-Attention
```python
def self_attention(Q, K, V, d_k):
    scores = Q @ K.T / np.sqrt(d_k)
    weights = softmax(scores, axis=-1)
    return weights @ V
```

#### Positional Encoding
```python
def positional_encoding(max_len, d_model):
    pe = np.zeros((max_len, d_model))
    pos = np.arange(max_len)[:, np.newaxis]
    div = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    pe[:, 0::2] = np.sin(pos * div)
    pe[:, 1::2] = np.cos(pos * div)
    return pe
```

---

## 1. Core ML Algorithmsï¼ˆ0/5ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 1.1 | Linear Regression Using Gradient Descent | Easy | âŒ | [P15](https://www.deep-ml.com/problems/15) |
| 1.2 | Train Logistic Regression with Gradient Descent | Medium | âŒ | [P106](https://www.deep-ml.com/problems/106) |
| 1.3 | K-Means Clustering | Medium | âŒ | [P17](https://www.deep-ml.com/problems/17) |
| 1.4 | Principal Component Analysis (PCA) Implementation | Medium | âŒ | [P19](https://www.deep-ml.com/problems/19) |
| 1.5 | Decision Tree Learning | Medium | âŒ | [P20](https://www.deep-ml.com/problems/20) |

### 1.1 Linear Regression Using Gradient Descent (P15)

**é¢˜ç›®æè¿°**: ä½¿ç”¨æ¢¯åº¦ä¸‹é™å®ç°çº¿æ€§å›å½’

**å…³é”®å…¬å¼**:
- é¢„æµ‹: $\hat{y} = X \cdot W$
- æŸå¤±: $L = \frac{1}{n}\sum(y - \hat{y})^2$
- æ¢¯åº¦: $\nabla W = -\frac{2}{n}X^T(y - \hat{y})$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 1.2 Train Logistic Regression with Gradient Descent (P106)

**é¢˜ç›®æè¿°**: ä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒé€»è¾‘å›å½’

**å…³é”®å…¬å¼**:
- Sigmoid: $\sigma(z) = \frac{1}{1+e^{-z}}$
- æŸå¤±: $L = -\frac{1}{n}\sum[y\log(p) + (1-y)\log(1-p)]$
- æ¢¯åº¦: $\nabla W = \frac{1}{n}X^T(p - y)$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 1.3 K-Means Clustering (P17)

**é¢˜ç›®æè¿°**: å®ç° K-Means èšç±»ç®—æ³•

**æ ¸å¿ƒæ­¥éª¤**:
1. åˆå§‹åŒ– K ä¸ªä¸­å¿ƒ
2. E-step: åˆ†é…æ¯ä¸ªç‚¹åˆ°æœ€è¿‘ä¸­å¿ƒ
3. M-step: æ›´æ–°ä¸­å¿ƒä¸ºç°‡å†…å‡å€¼
4. é‡å¤ç›´åˆ°æ”¶æ•›

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 1.4 Principal Component Analysis (PCA) Implementation (P19)

**é¢˜ç›®æè¿°**: å®ç° PCA é™ç»´

**æ ¸å¿ƒæ­¥éª¤**:
1. æ•°æ®æ ‡å‡†åŒ–
2. è®¡ç®—åæ–¹å·®çŸ©é˜µ æˆ– SVD åˆ†è§£
3. å–å‰ k ä¸ªä¸»æˆåˆ†

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 1.5 Decision Tree Learning (P20)

**é¢˜ç›®æè¿°**: å®ç°å†³ç­–æ ‘å­¦ä¹ ç®—æ³•

**æ ¸å¿ƒæ¦‚å¿µ**:
- ç†µ: $H(Y) = -\sum p_i \log_2 p_i$
- ä¿¡æ¯å¢ç›Š: $IG = H(Y) - H(Y|X)$

```python
import numpy as np
from collections import Counter

def majority(y):
    y = list(y)
    c = Counter(y)
    return max(c, key=lambda k: (c[k], -y.index(k)))

def entropy(y):
    _,c=np.unique(y, return_counts=True)
    p=c/c.sum()
    return -(p*np.log2(p)).sum()

def gain(x,y):
    H=entropy(y); U=np.unique(x)
    return H - sum((x==u).mean()*entropy(y[x==u]) for u in U)

def g(x,k,t):
    y=np.array([r[t] for r in x], object)
    z=np.array([r[k] for r in x], object)
    return gain(z,y)

def d(x,a,t):
    if not x: return 'No examples'
    y=np.array([r[t] for r in x], object)
    if (y==y[0]).all(): return x[0][t]
    if not a: return majority(y)
    b=max(a, key=lambda k:g(x,k,t))
    return {b:{v:d([q for q in x if q[b]==v],[k for k in a if k!=b],t)
               for v in np.unique([q[b] for q in x])}}

def learn_decision_tree(examples, attributes, target_attr):
    return d(examples, attributes, target_attr)

```

---

## 2. Loss & Regularizationï¼ˆ0/4ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 2.1 | Implement Gradient Descent Variants with MSE Loss | Medium | âŒ | [P47](https://www.deep-ml.com/problems/47) |
| 2.2 | Compute Multi-class Cross-Entropy Loss | Easy | âŒ | [P134](https://www.deep-ml.com/problems/134) |
| 2.3 | Implement Ridge Regression Loss Function | Easy | âŒ | [P43](https://www.deep-ml.com/problems/43) |
| 2.4 | Implement Lasso Regression using Gradient Descent | Medium | âŒ | [P50](https://www.deep-ml.com/problems/50) |

### 2.1 Gradient Descent Variants with MSE Loss (P47)

**é¢˜ç›®æè¿°**: å®ç°ä¸åŒå˜ç§çš„æ¢¯åº¦ä¸‹é™ï¼ˆBGD, SGD, Mini-batchï¼‰

**å…³é”®å…¬å¼**:
- MSE: $L = \frac{1}{n}\sum(y - \hat{y})^2$

```python
def reg(W,X,Y,lr):
    n = len(X)
    p = X @ W; dy=p-Y; dg=2/n * dy
    dW = X.T @ dg; W -= lr * dW
    return W
def gradient_descent(X, y, lr, epoch, Bs_batch_size, method):
    for _ in range(epoch):
        if method == 'batch':
            W = reg(W,X,y,lr)
        elif method == 'stochastic':
            for i in range(n): W = reg(W,X[i:i+1],y[i:i+1],lr)
        elif method == 'mini_batch':
            for i in range(0, n, Bs): # Bs = batchsize
                W = reg(W, X[i:i+Bs],y[i:i+Bs],lr)
    return W
```

---

### 2.2 Multi-class Cross-Entropy Loss (P134)

**é¢˜ç›®æè¿°**: è®¡ç®—å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±

**å…³é”®å…¬å¼**:
$$L = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C} y_{ic} \log(p_{ic})$$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 2.3 Ridge Regression Loss Function (P43)

**é¢˜ç›®æè¿°**: å®ç°å¸¦ L2 æ­£åˆ™çš„æŸå¤±å‡½æ•°

**å…³é”®å…¬å¼**:
$$L = \frac{1}{n}\|y - Xw\|^2 + \lambda\|w\|^2$$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 2.4 Lasso Regression using Gradient Descent (P50)

**é¢˜ç›®æè¿°**: ä½¿ç”¨æ¢¯åº¦ä¸‹é™å®ç° Lasso å›å½’

**å…³é”®å…¬å¼**:
$$L = \frac{1}{n}\|y - Xw\|^2 + \lambda\|w\|_1$$

**æ³¨æ„**: L1 èŒƒæ•°ä¸å¯å¯¼ï¼Œéœ€ç”¨ subgradient

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## 3. Model Evaluationï¼ˆ0/6ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 3.1 | Implement K-Fold Cross-Validation | Medium | âŒ | [P18](https://www.deep-ml.com/problems/18) |
| 3.2 | Generate a Confusion Matrix for Binary Classification | Easy | âŒ | [P75](https://www.deep-ml.com/problems/75) |
| 3.3 | Implement Precision Metric | Easy | âŒ | [P46](https://www.deep-ml.com/problems/46) |
| 3.4 | Implement Recall Metric in Binary Classification | Easy | âŒ | [P52](https://www.deep-ml.com/problems/52) |
| 3.5 | Implement F-Score Calculation for Binary Classification | Easy | âŒ | [P61](https://www.deep-ml.com/problems/61) |
| 3.6 | Calculate AUC (Area Under ROC Curve) | Medium | âŒ | [P277](https://www.deep-ml.com/problems/277) |

### 3.1 K-Fold Cross-Validation (P18)

**é¢˜ç›®æè¿°**: å®ç° K æŠ˜äº¤å‰éªŒè¯

**æ ¸å¿ƒæ­¥éª¤**:
1. å°†æ•°æ®åˆ†æˆ K ä»½
2. æ¯æ¬¡ç”¨ 1 ä»½åšéªŒè¯ï¼ŒK-1 ä»½åšè®­ç»ƒ
3. è¿”å› K æ¬¡éªŒè¯çš„å¹³å‡ç»“æœ

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 3.2 Confusion Matrix (P75)

**é¢˜ç›®æè¿°**: ç”ŸæˆäºŒåˆ†ç±»çš„æ··æ·†çŸ©é˜µ

**å…¬å¼**:
|  | Pred=1 | Pred=0 |
|--|--------|--------|
| True=1 | TP | FN |
| True=0 | FP | TN |

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 3.3 Precision Metric (P46)

**é¢˜ç›®æè¿°**: å®ç°ç²¾ç¡®ç‡

**å…¬å¼**: $Precision = \frac{TP}{TP + FP}$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 3.4 Recall Metric (P52)

**é¢˜ç›®æè¿°**: å®ç°å¬å›ç‡

**å…¬å¼**: $Recall = \frac{TP}{TP + FN}$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 3.5 F-Score Calculation (P61)

**é¢˜ç›®æè¿°**: å®ç° F1 åˆ†æ•°

**å…¬å¼**: $F_1 = \frac{2 \cdot P \cdot R}{P + R}$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 3.6 AUC (Area Under ROC Curve) (P277)

**é¢˜ç›®æè¿°**: è®¡ç®— ROC æ›²çº¿ä¸‹é¢ç§¯

**æ ¸å¿ƒæ€æƒ³**:
1. è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ TPR å’Œ FPR
2. ç”¨æ¢¯å½¢æ³•åˆ™ç§¯åˆ†

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## 4. Neural Networksï¼ˆ0/6ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 4.1 | Single Neuron with Backpropagation | Easy | âŒ | [P25](https://www.deep-ml.com/problems/25) |
| 4.2 | Implementing a Custom Dense Layer in Python | Medium | âŒ | [P40](https://www.deep-ml.com/problems/40) |
| 4.3 | Implement Batch Normalization for BCHW Input | Medium | âŒ | [P115](https://www.deep-ml.com/problems/115) |
| 4.4 | Dropout Layer | Easy | âŒ | [P151](https://www.deep-ml.com/problems/151) |
| 4.5 | Implement ReLU Activation Function | Easy | âŒ | [P42](https://www.deep-ml.com/problems/42) |
| 4.6 | Softmax Activation Function Implementation | Easy | âŒ | [P23](https://www.deep-ml.com/problems/23) |

### 4.1 Single Neuron with Backpropagation (P25)

**é¢˜ç›®æè¿°**: å®ç°å•ç¥ç»å…ƒçš„å‰å‘å’Œåå‘ä¼ æ’­

**å…¬å¼**:
- å‰å‘: $y = \sigma(w \cdot x + b)$
- åå‘: $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \sigma'(z) \cdot x$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 4.2 Custom Dense Layer (P40)

**é¢˜ç›®æè¿°**: å®ç°å…¨è¿æ¥å±‚

**å…¬å¼**:
- å‰å‘: $Y = XW + b$
- åå‘: $\nabla W = X^T \nabla Y$, $\nabla X = \nabla Y \cdot W^T$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 4.3 Batch Normalization for BCHW (P115)

**é¢˜ç›®æè¿°**: å®ç° BCHW æ ¼å¼çš„ BatchNorm

**å…¬å¼**:
$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta$$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 4.4 Dropout Layer (P151)

**é¢˜ç›®æè¿°**: å®ç° Dropout å±‚

**å…¬å¼**: $y = x \cdot \text{mask} / (1 - p)$ (inverted dropout)

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 4.5 ReLU Activation Function (P42)

**é¢˜ç›®æè¿°**: å®ç° ReLU æ¿€æ´»å‡½æ•°

**å…¬å¼**: $\text{ReLU}(x) = \max(0, x)$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 4.6 Softmax Activation Function (P23)

**é¢˜ç›®æè¿°**: å®ç° Softmax å‡½æ•°

**å…¬å¼**: $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$

**æŠ€å·§**: å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## 5. Optimizersï¼ˆ0/3ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 5.1 | Implement Adam Optimization Algorithm | Medium | âŒ | [P49](https://www.deep-ml.com/problems/49) |
| 5.2 | Momentum Optimizer | Easy | âŒ | [P146](https://www.deep-ml.com/problems/146) |
| 5.3 | Gradient Clipping by Global Norm | Easy | âŒ | [P197](https://www.deep-ml.com/problems/197) |

### 5.1 Adam Optimization Algorithm (P49)

**é¢˜ç›®æè¿°**: å®ç° Adam ä¼˜åŒ–å™¨

**å…¬å¼**:
- $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$
- $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$
- $\hat{m}_t = m_t / (1 - \beta_1^t)$
- $\hat{v}_t = v_t / (1 - \beta_2^t)$
- $\theta_t = \theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 5.2 Momentum Optimizer (P146)

**é¢˜ç›®æè¿°**: å®ç°åŠ¨é‡ä¼˜åŒ–å™¨

**å…¬å¼**:
- $v_t = \gamma v_{t-1} + \alpha \nabla L$
- $\theta_t = \theta_{t-1} - v_t$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 5.3 Gradient Clipping by Global Norm (P197)

**é¢˜ç›®æè¿°**: æŒ‰å…¨å±€èŒƒæ•°è£å‰ªæ¢¯åº¦

**å…¬å¼**: è‹¥ $\|g\| > \text{max\_norm}$ï¼Œåˆ™ $g = g \cdot \frac{\text{max\_norm}}{\|g\|}$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## 6. CNNsï¼ˆ0/2ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 6.1 | Simple Convolutional 2D Layer | Medium | âŒ | [P41](https://www.deep-ml.com/problems/41) |
| 6.2 | Implement Global Average Pooling | Easy | âŒ | [P114](https://www.deep-ml.com/problems/114) |

### 6.1 Simple Convolutional 2D Layer (P41)

**é¢˜ç›®æè¿°**: å®ç° 2D å·ç§¯å±‚

**è¾“å‡ºå°ºå¯¸**: $H_{out} = (H + 2P - K) / S + 1$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 6.2 Global Average Pooling (P114)

**é¢˜ç›®æè¿°**: å®ç°å…¨å±€å¹³å‡æ± åŒ–

**å…¬å¼**: å¯¹æ¯ä¸ªé€šé“å–ç©ºé—´ç»´åº¦çš„å¹³å‡å€¼

```python
def global_avg_pool(x):
    # x: (N, C, H, W) â†’ (N, C)
    return x.mean(axis=(2, 3))
```

---

## 7. Sequencesï¼ˆ0/2ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 7.1 | Implement LSTM Network | Hard | âŒ | [P59](https://www.deep-ml.com/problems/59) |
| 7.2 | Implement GRU Cell | Medium | âŒ | [P287](https://www.deep-ml.com/problems/287) |

### 7.1 LSTM Network (P59)

**é¢˜ç›®æè¿°**: å®ç° LSTM ç½‘ç»œ

**é—¨æ§å…¬å¼**:
- é—å¿˜é—¨: $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
- è¾“å…¥é—¨: $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
- å€™é€‰å€¼: $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
- ç»†èƒçŠ¶æ€: $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
- è¾“å‡ºé—¨: $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
- éšçŠ¶æ€: $h_t = o_t * \tanh(C_t)$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 7.2 GRU Cell (P287)

**é¢˜ç›®æè¿°**: å®ç° GRU å•å…ƒ

**é—¨æ§å…¬å¼**:
- æ›´æ–°é—¨: $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
- é‡ç½®é—¨: $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$
- å€™é€‰éšçŠ¶æ€: $\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t])$
- éšçŠ¶æ€: $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## 8. Transformersï¼ˆ0/3ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 8.1 | Implement Self-Attention Mechanism | Medium | âŒ | [P53](https://www.deep-ml.com/problems/53) |
| 8.2 | Implement Multi-Head Attention | Hard | âŒ | [P94](https://www.deep-ml.com/problems/94) |
| 8.3 | Positional Encoding Calculator | Easy | âŒ | [P85](https://www.deep-ml.com/problems/85) |

### 8.1 Self-Attention Mechanism (P53)

**é¢˜ç›®æè¿°**: å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶

**å…¬å¼**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 8.2 Multi-Head Attention (P94)

**é¢˜ç›®æè¿°**: å®ç°å¤šå¤´æ³¨æ„åŠ›

**å…¬å¼**:
- $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
- $\text{MultiHead} = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 8.3 Positional Encoding Calculator (P85)

**é¢˜ç›®æè¿°**: è®¡ç®—ä½ç½®ç¼–ç 

**å…¬å¼**:
- $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$
- $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$

```python
def positional_encoding(max_len, d_model):
    pe = np.zeros((max_len, d_model))
    pos = np.arange(max_len)[:, np.newaxis]
    div = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    pe[:, 0::2] = np.sin(pos * div)
    pe[:, 1::2] = np.cos(pos * div)
    return pe
```

---

## 9. Production & MLOpsï¼ˆ0/10ï¼‰

| # | é¢˜ç›® | éš¾åº¦ | çŠ¶æ€ | é“¾æ¥ |
|---|------|------|------|------|
| 9.1 | Implement Stratified Train-Test Split | Easy | âŒ | [P275](https://www.deep-ml.com/problems/275) |
| 9.2 | Implement Grid Search | Medium | âŒ | [P288](https://www.deep-ml.com/problems/288) |
| 9.3 | Implement Early Stopping Based on Validation Loss | Easy | âŒ | [P135](https://www.deep-ml.com/problems/135) |
| 9.4 | Feature Drift Detection using PSI | Medium | âŒ | [P253](https://www.deep-ml.com/problems/253) |
| 9.5 | A/B Test Statistical Analysis | Medium | âŒ | [P269](https://www.deep-ml.com/problems/269) |
| 9.6 | Calculate P50/P95/P99 Latency Percentiles | Easy | âŒ | [P293](https://www.deep-ml.com/problems/293) |
| 9.7 | Implement INT8 Quantization | Medium | âŒ | [P294](https://www.deep-ml.com/problems/294) |
| 9.8 | Implement Prediction Distribution Monitoring | Medium | âŒ | [P295](https://www.deep-ml.com/problems/295) |
| 9.9 | Calculate Statistical Power for Experiment Design | Medium | âŒ | [P296](https://www.deep-ml.com/problems/296) |
| 9.10 | Implement Request Batching for Inference | Medium | âŒ | [P297](https://www.deep-ml.com/problems/297) |

### 9.1 Stratified Train-Test Split (P275)

**é¢˜ç›®æè¿°**: å®ç°åˆ†å±‚åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†

**æ ¸å¿ƒ**: ä¿æŒå„ç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ä¸€è‡´

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.2 Grid Search (P288)

**é¢˜ç›®æè¿°**: å®ç°ç½‘æ ¼æœç´¢è¶…å‚è°ƒä¼˜

**æ ¸å¿ƒ**: éå†æ‰€æœ‰è¶…å‚ç»„åˆï¼Œç”¨äº¤å‰éªŒè¯è¯„ä¼°

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.3 Early Stopping (P135)

**é¢˜ç›®æè¿°**: åŸºäºéªŒè¯æŸå¤±å®ç°æ—©åœ

**æ ¸å¿ƒ**: å½“éªŒè¯æŸå¤±è¿ç»­ patience è½®ä¸ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ

```python
class EarlyStopping:
    def __init__(self, patience=5, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience
```

---

### 9.4 Feature Drift Detection using PSI (P253)

**é¢˜ç›®æè¿°**: ä½¿ç”¨ PSI æ£€æµ‹ç‰¹å¾æ¼‚ç§»

**å…¬å¼**:
$$PSI = \sum_i (A_i - E_i) \cdot \ln\frac{A_i}{E_i}$$

å…¶ä¸­ $A_i$ æ˜¯å®é™…åˆ†å¸ƒï¼Œ$E_i$ æ˜¯æœŸæœ›åˆ†å¸ƒ

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.5 A/B Test Statistical Analysis (P269)

**é¢˜ç›®æè¿°**: æ¨¡å‹æ¯”è¾ƒçš„ A/B æµ‹è¯•ç»Ÿè®¡åˆ†æ

**æ ¸å¿ƒ**: è®¡ç®— p-valueï¼Œåˆ¤æ–­æ˜¾è‘—æ€§

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.6 Calculate P50/P95/P99 Latency Percentiles (P293)

**é¢˜ç›®æè¿°**: è®¡ç®—å»¶è¿Ÿç™¾åˆ†ä½æ•°

```python
def percentiles(data):
    data = sorted(data)
    n = len(data)
    p50 = data[int(n * 0.50)]
    p95 = data[int(n * 0.95)]
    p99 = data[int(n * 0.99)]
    return p50, p95, p99
```

---

### 9.7 INT8 Quantization (P294)

**é¢˜ç›®æè¿°**: å®ç° INT8 é‡åŒ–

**å…¬å¼**: $x_{int8} = \text{round}(x / scale) + zero\_point$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.8 Prediction Distribution Monitoring (P295)

**é¢˜ç›®æè¿°**: ç›‘æ§é¢„æµ‹åˆ†å¸ƒå˜åŒ–

**æ ¸å¿ƒ**: æ¯”è¾ƒè®­ç»ƒæ—¶å’Œæ¨ç†æ—¶çš„é¢„æµ‹åˆ†å¸ƒ

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.9 Statistical Power for Experiment Design (P296)

**é¢˜ç›®æè¿°**: è®¡ç®—å®éªŒè®¾è®¡çš„ç»Ÿè®¡åŠŸæ•ˆ

**å…¬å¼**: $n = \frac{(z_\alpha + z_\beta)^2 \cdot 2\sigma^2}{\delta^2}$

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

### 9.10 Request Batching for Inference (P297)

**é¢˜ç›®æè¿°**: å®ç°æ¨ç†è¯·æ±‚æ‰¹å¤„ç†

**æ ¸å¿ƒ**: å°†å¤šä¸ªè¯·æ±‚åˆå¹¶æˆ batch ä»¥æé«˜åå

```python
# TODO: å¡«å†™ä½ çš„ä»£ç 
```

---

## ğŸ“Œ æ˜“é”™ç‚¹æ€»ç»“

1. **æ•°å€¼ç¨³å®šæ€§**: sigmoid/softmax è¦å‡æœ€å¤§å€¼é˜²æº¢å‡º
2. **æ­£åˆ™åŒ–æ¢¯åº¦**: L2 æ¢¯åº¦æ˜¯ $\lambda w$ï¼ŒL1 æ˜¯ $\lambda \cdot \text{sign}(w)$
3. **BatchNorm ç»´åº¦**: BCHW æ ¼å¼æ²¿ (N, H, W) ç»´åº¦æ±‚å‡å€¼
4. **Dropout ç¼©æ”¾**: è®­ç»ƒæ—¶é™¤ä»¥ $(1-p)$ (inverted dropout)
5. **Adam bias correction**: åˆå§‹é˜¶æ®µ $1-\beta^t$ æ¥è¿‘ 0ï¼Œå¿…é¡»åšä¿®æ­£
6. **å·ç§¯å¡«å……**: è¾“å‡ºå°ºå¯¸å…¬å¼åˆ«å¿˜äº† padding
7. **Attention ç¼©æ”¾**: é™¤ä»¥ $\sqrt{d_k}$ é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
8. **äº¤å‰ç†µ log(0)**: åŠ  eps é˜²æ­¢å–å¯¹æ•°æ—¶å‡ºç° -inf

---

## ğŸ”— ç›¸å…³èµ„æº

- [Deep-ML MLE Interview Prep](https://www.deep-ml.com/)
- æœ¬åœ°é¢˜åº“: `ML/coding/` ä¸‹å„åˆ†ç±»æ–‡ä»¶
